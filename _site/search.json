[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Zachary Mueller, and I’m a Machine Learning Software Engineer at 🤗. I majored in Software Design and Development and I have minors in both Computer Science and Environmental Science.\nI have a heavy passion for Deep Learning and Open Source libraries. As a result, below you will find some notable articles I’ve written, a few courses I’ve made, some of software libraries I’ve written, interesting projects, and the open source libraries I have tried to contribute the most to."
  },
  {
    "objectID": "index.html#my-blogs",
    "href": "index.html#my-blogs",
    "title": "About",
    "section": "My blogs",
    "text": "My blogs\nExplore this website to learn more! Generally most of my larger articles are in /blog."
  },
  {
    "objectID": "index.html#outside-articles-and-posts",
    "href": "index.html#outside-articles-and-posts",
    "title": "About",
    "section": "Outside Articles and Posts",
    "text": "Outside Articles and Posts\n\n\n\n\n\n\n\nFrom PyTorch DDP to Accelerate to Trainer, mastery of distributed training with ease (October 2022)\nAn overview of training deep learning models in PyTorch, particularly focusing on Distributed Data Parallelism (DDP) for distributed training across multiple GPUs or machines. It discusses the setup and usage of DDP, as well as introduces 🤗 Accelerate, a library that simplifies distributed training with minimal code changes. Additionally, the article briefly covers the use of the 🤗 Trainer API, which abstracts training details for even easier distributed training.\n\n\nMethods for Automating Learning Rate Finders (March 2021)\nAddressing the challenges of manually selecting learning rates for deep learning models and presents automated methods for finding optimal learning rates. The methods, including Leslie Smith’s approach and those developed by Novetta and ESRI, are compared across various datasets and domains, such as computer vision classification, semantic segmentation, tabular classification, and natural language processing.\n\n\nlib2nbdev (June 2021)\nNbdev, originating from fast.ai, simplifies the software engineering process by centralizing code, documentation, and tests in Jupyter Notebooks, enhancing project readability and testing while streamlining development-to-production pipelines. It offers auto-generated documentation, release generators, multiprocessing for testing, and GitHub Actions integration, and Novetta’s open-source tool, lib2nbdev, facilitates the conversion of existing Python projects into nbdev libraries, saving substantial time and effort."
  },
  {
    "objectID": "index.html#scientific-publications",
    "href": "index.html#scientific-publications",
    "title": "About",
    "section": "Scientific Publications",
    "text": "Scientific Publications\n\n\n\n\n\n\n\nA Neural Network Model to Identify Relative Movements from Wearable Devices\nPresents a neural network model for analyzing movement data from wearable devices, utilizing various base variables and extensive feature engineering to generate 171 unique features. The model achieves a remarkable 95% average accuracy on test datasets, outperforming a previous similar work by 20%.\n\n\nAffordably Assessing and Comparing Trends in Particulate Matter Concentrations Around a University Campus\nAddresses the importance of air quality for human health by creating affordable IoT devices using Raspberry Pi Zeros and Honeywell PM sensors to monitor PM 10 and PM 2.5 air pollution around the University of West Florida campus. The goal was to track air quality trends during periods of high building usage and develop a neural network analysis for future predictions."
  },
  {
    "objectID": "index.html#courses",
    "href": "index.html#courses",
    "title": "About",
    "section": "Courses",
    "text": "Courses\n\n\n\n\n\n\n\nWalk with fastai: Revisited\nA full revamp of the original Walk with fastai course made for the modern version of the library and focusing on different aspects than the prior iterations.\n\n\nWalk with fastai\nWalk with fastai was the second iteration of my exploration with courses. In it I taught over 10 lessons on the new fastai API, exploring new avenues that the API could preform and displaying unique situations often not covered within the orignal course. So far it has been wildly successful, with many of the lectures garnishing over a thousand views on YouTube\n\n\nPractical Deep Learning for Coders 2.0\nThis was a rehash of the fastai course of the same name that I taught at the University of West Florida through the Artificial Intellegence Research Group in 2019"
  },
  {
    "objectID": "index.html#open-source-software-libraries",
    "href": "index.html#open-source-software-libraries",
    "title": "About",
    "section": "Open Source Software Libraries",
    "text": "Open Source Software Libraries\n\n\n\n\n\n\n\nAccelerate\nWhile at Hugging Face, I have been a major developer on the Accelerate library; it is a framework designed to ensure PyTorch code can be ran on any system without code changes.\n\n\nAdaptNLP\nWhile at Novetta, I was the lead developer for the AdaptNLP library. This library wraps the HuggingFace Transformers and fastai libraries into an easy-to-use interface for both fine-tuning and performing inference with transformer-based models. It was featured in the Weights and Biases HuggingFace Study Group\n\n\nlib2nbdev\nSeeing a need to ease integration of the nbdev framework into more projects, I built lib2nbdev as a single-use library that will convert existing python projects into a compatible nbdev library.\n\n\nwwf\nAlong with being a course, the Walk with fastai library (wwf) is also a conglomerate of useful documentation and code for fastai applications centralized in a single location\n\n\nnbverbose\nnbverbose aims at combining both the one-line documentation style that fastai incentivizes without losing best practices nor documentation accessability. By combining the fastcore library with nbdev, this library provides an efficient way to document your code while minimizing the vertical space needed to get there\n\n\nfastinference\nThe fastinference library was designed around making inference with the fastai library simpler, faster, and more approachable. This entailed providing ONNX exportability as well as in-house interpretability modules\n\n\nfastdebug\nAimed at easing the annoyance of common bugs, this library helps users of both the PyTorch and fastai ecosystems provide clear debugging logs when it comes to extremely common exceptions that get thrown that could use a little touch-up\n\n\nfastshap\nThe fastshap library brought in SHAP into the fastai framework. It has since been merged with the fastinference module, though one compatible with fastai version 1 is available on pypi"
  },
  {
    "objectID": "index.html#open-source-contributions",
    "href": "index.html#open-source-contributions",
    "title": "About",
    "section": "Open Source Contributions",
    "text": "Open Source Contributions\n\n\n\nHugging Face\nfastai\n\n\n\n\nAccelerate\nfastai\n\n\nHub\nfastcore\n\n\ntransformers\nnbdev"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "My Blogs",
    "section": "",
    "text": "Announcing Walk with fastai, the missing pieces for success\n\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSpeeding up fastai Tabular with NumPy\n\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\nThe Idea of a Transform\n\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\nSummer Smackdown - Week 1\n\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/Week-1.html",
    "href": "blog/Week-1.html",
    "title": "Summer Smackdown - Week 1",
    "section": "",
    "text": "These posts will most likely wind up being a bit of an odd bunch in terms of formatting until I figure out a style I like and enjoy, as the goal is to merge all Four of the lessons into one big post.\nGiven I wanted to update all of these blogs on Sundays, I decided I would include the first ‘day’ of work as well.\nOverall how the schedule I plan to follow looks is as such:\nAs I started this goal on a Saturday, this week there will not be much in my recap, but I’ll be as inclusive as I can into the small lessons I learned."
  },
  {
    "objectID": "blog/Week-1.html#computational-linear-algebra",
    "href": "blog/Week-1.html#computational-linear-algebra",
    "title": "Summer Smackdown - Week 1",
    "section": "Computational Linear Algebra",
    "text": "Computational Linear Algebra\nWe start off learning about the Markov Chain, a way to describe a sequence of events where the probably of each event depends on the state of the previous event. Also known as the next event is determined by the previous event. The course utilizes the Numpy library to hold matrix multiplications to solve the various problems. My notes go through and futher explain what each answer means in context.\nFor example, problem 1 is about using a stochastic matrix, which is a square probablity matrix, in order to predict how health-related incidents will be in the following year.\nWe start off knowing that the current year had 85% asymtpmatic, 10% symptomatic, 5% AIDS, and 0% death. Next, we were given the following probability table:\n\n\n\nalt text\n\n\nNow that we’re here, we use matrix multiplication to get our answer:\nimport numpy as np\n\ni = np.array([[.85,.1,.05,0]])\nmat = np.array([[.9,.07,.02,.01],\n                [0,.93,.05,.02],\n                [0,0,.85,.15],\n                [0,0,0,1]])\n\nres = mat.T @ i.T\n\nThe @ symbol is used when doing matrix multiplication, where we multiply each row by the column, and then sum them together.\n\nOne thing Jeremy points out is another way to write the above:\n(i @ mat).T) which saves us a few seconds of code, and looks cleaner.\nThe answer winds up being:\narray([[0.765 ],\n       [0.1525],\n       [0.0645],\n       [0.018 ]])\nHowever, what does the answer mean? Well, it means that within the next year:\n\n76.5% of people will be asymptomatic\n15.25% of people will be symptomatic\n6.45% of people will have AIDS\n1.8% of people will die as a result of their illnesses\n\nWe’ve started using some matrix multiplication to get solutions, but can we get a bit more advanced with it?\nTake problem 2:\n\n\n\nalt text\n\n\nGiven the above table, figure out which store is best for what individual. This is a straight matrix by matrix multiplication problem where we will have ‘dem’ represent a matrix of the demand per individual, and ‘p’ be the prices for each item in two particular shops.\ndem = np.array([[6, 5, 3, 1],\n       [3,6,2,2],\n       [3,4,3,1]])\n\np = np.array([[1.5, 1],\n       [2., 2.5],\n       [5., 4.5],\n       [16., 17.]])\nWe yet again solve this by doing dem@p, which gives us a table that looks like the following:\narray([[50. , 49. ],\n       [58.5, 61. ],\n       [43.5, 43.5]])\nThe above table is now described as having the rows be an individual, and the columns being a particular store with the content as the price they would pay for the items they need. We can see that for Person 1 shop 2 would be the best, for Person 2 shop 1 would be the best, and for Person 3 they could go to either one.\nThen Rachel goes further to describe images a little bit and convolutions, which I was already familar with from the Practical Deep Learning for Coders course, however this Medium article she mentions I found especially helpful: CNNs from Different Viewpoints\nWhat this helped show for me was how matrix multiplication is actually applied within these Neural Networks we are generating through the Fast.AI library, especially the following image:\n\n\n\nalt text\n\n\nHere we have a 2x2 matrix (filter) being applied on a single-channel image (3x3), to get our four results: P,W,R,S. I enjoy this view of how our layers are working as I can see each product mapped with corresponding coordinates, versus a Neural Network viewpoint: \nWhere alpha, beta, gamma, etc are the connections or lines from each node to result.\nThis is as far as I got yesterday, so next week lesson 1 should be fully completed."
  },
  {
    "objectID": "blog/Week-1.html#matrix-calculus",
    "href": "blog/Week-1.html#matrix-calculus",
    "title": "Summer Smackdown - Week 1",
    "section": "Matrix Calculus",
    "text": "Matrix Calculus\nOne thing Jeremy suggests us to do during the Foundations course is turn paper to code, so I wanted to apply that to this course, despite it being pure-math heavy. The goal of doing this was just to know how to apply various scary-looking math into code easier, as my experience before this was none.\nThis week I went over the Introduction and Review sections of the paper, as I last took AP Calculus senior year of high school… It’s been a few years.\nSo! The introduction segment. Any activation of a single unit inside a nerual network is done using the “dot product of an edge weight vector, w, with an input vector x, plus a scalar bias b.” Okay. That was a lot thrown out at me. Let’s make that a bit easier. The above can also be written as y=mx+b, a basic linear function where m and x are both matrix’s. The better way to right that would be like so:\n\n\n\nalt text\n\n\nWhere n and i are how many layers or activation uits we have. This could then also be written as z = w * x + b where z, the ‘affine function’ (linear function), is derived from a linear unit that clips negative values to zero from the bias.\nAnother way to visualize a neuron is like so:\n\nNow, when we are training our models, all we are doing is choosing a w and b so we can get our desired output for all of our inputs. We can help choose and navigate what are our best options by using a loss function to grade the final activations to the target for all of our inputs. To help minimize, a variation of gradient decent is used where we take the partial derivitive (gradient) of an activation with respect to w and b.\nIn laymans terms? Gradually tweaking w and b in order to make some loss function as close to zero as we can.\nThe next example shown in the paper is taking a function we’re familair with, Mean Squared Error, and showing us its derivitive (gradient): \nAt first glance that looks absolutely disgustingly terrifying. But let’s try to break it down into code instead and see if we can try to understand it better.\nSo first, the original where N is the number of inputs\ndef loss(N):\n  y = 0\n  for x in range(N):\n    y += (targ(x) - activ(x)) ** 2\n  return y/N\nOkay, doesn’t look too bad now. For all inputs, we take the square of our target minus our activation (or our answer). Let’s look at that derivitive now. I made two functions, actf and grad as we have that interior summation.\ndef grad(N, w, b):\n  y = 0\n  for x in range(N):\n    y += (targ(x) - actf(x)) ** 2\n  return y/N\n\ndef actf(x, w, b):\n  y = 0\n  for i in range(abs(x)):\n    y += (w[i] * x[i] + b)\n  return max(0, y)\nThat looks a bit better, we can see that w and x are both going to be matrix’s, weight and input respectivly, and b is our bias.\nAlright, not as scary anymore. The last bit I did was a review on the Scalar derivative rules, and attempting to recreate this in code. For this I found the sympy library a huge help, as we can visualize functions and their derivitives.\nFor example, say I have the equation \nWe can write this in code as y = 3*x**2. Well, if we want the derivitive all we have to do is first declare ‘x’ as a ‘Symbol’, then use the .diff function to get the derivitive!\n\nfrom sympy import *\nx = Symbol('x')\ny = 3*x**2\nyprime = y.diff(x)\nThe result will give us 6*x, what we were expecting."
  },
  {
    "objectID": "blog/Week-1.html#natural-language-processing",
    "href": "blog/Week-1.html#natural-language-processing",
    "title": "Summer Smackdown - Week 1",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\nAs I stated before, this course is not quite done yet, so as a result I’m going through the notebooks carefully to learn what I can and once the videos are released I will be rewatching them to make sure I did not miss anything and I understand all the concepts. But for now, here’s what I’ve learned:\nThere are many ethical issues that are brought upon by NLP’s, such as Google Translate:\n\nHere we can see the same translation does not keep the proper pronouns as it should, and is bias towards men being doctors, not very 21st Century if you ask me!\nBut, onwards we must go. The topics I went into this week were on using Non-Negative Matrix Factorization (NMF) and Single Value Decomposition (SVD) for Topic Modeling. Topic modeling beings with something called a term-document matrix, which is a matrix of words by material. In the example used, we have the amount of times particular names showed up in a few classic books:\n\nThis is called a bags of words approach as we don’t care for the sentence structure or order words come in, just how often they appear.\nFor this lesson, Rachel used the Newsgroups dataset which consists of 18,000 blog posts that follow 20 topics on a forum, this was popular in the 80’s and 90’s apparently as the internet did not exist really then.\nFrom here, we went into a few topics: stop words, stemming, and lemmatization\nStop Words:\nStop words are ‘extremely common words which are of little value in helping’. There is no single universal list of stop words, and each program uses a slightly different one. For example,the sklearn library has the following first twenty as theirs:\nfrom sklearn.feature_extraction import stop_words\nsorted(list(stop_words.ENGLISH_STOP_WORDS))[:20]\n['a',\n 'about',\n 'above',\n 'across',\n 'after',\n 'afterwards',\n 'again',\n 'against',\n 'all',\n 'almost',\n 'alone',\n 'along',\n 'already',\n 'also',\n 'although',\n 'always',\n 'am',\n 'among',\n 'amongst',\n 'amoungst']\nThese words are usually ignored and dropped, however for Machine Learning its been found recently that we (ML algorithms) may benefit more from their inclusion than exclusion.\nStemming and Lemmatization:\nStemming and lemmatization are used to generate the root forms of words. Lemmatization uses the rules of the original language, thus the tokens are all actually words. In contrast, stemming just chops the ends off of the words. These results won’t be actual words, but it is faster. “Stemming is the poor-man’s lemmatization” (Noah Smith, 2011).\nTo visualize this, we can use the nltk library. Say we have a list of words: organize, organizes, organizing. We know that they all stem from organize in some degree or another. Let’s compare the two together. We can do this with the following code:\n import nltk\n from nltk import stem\n wl = ['organize', 'organizes', 'organizing']\n [wnl.lemmatize(word) for word in wl]\n [porter.stem(word) for word in wl]\nThe output of this code is radically different:\nLemmatization: [‘organize’, ‘organizes’, ‘organizing’]\nStemming: [‘organ’, ‘organ’, ‘organ’]\nLemmatization will allow us to have more context within the words we tokenize and is almost always better than using Stemming, especially with languages that have more compex morphologies.\nThe last topic I got through in the lesson was Spacy. Spacy is a modern and fast NLP library, which Fast.AI uses. Spacy always uses Lemmatization for it’s tokenizations, as it is considered better. This is the first example of an opinionated choice in a library.\nI did not get into Foundational course yet this week, so I won’t have an update on that until the following post, but thank you all for reading! The goals for the upcoming week for each course are:\n\nNLP: Finish topic modeling\nMatrix Calculus: Introduction to vector calculus and partial derivatives\nLinear Algebra: Finish the ‘Why are we here’ notebook and the NMF SVD Notebooks\nFoundations: Get through notebooks 01 and 02\n\nSee you next week!\nZach Mueller"
  },
  {
    "objectID": "blog/Walkwithfastai.html",
    "href": "blog/Walkwithfastai.html",
    "title": "Announcing Walk with fastai, the missing pieces for success",
    "section": "",
    "text": "Details about my newest course, available at https://thezachmueller.gumroad.com/l/walkwithfastai"
  },
  {
    "objectID": "blog/Walkwithfastai.html#why-do-this-whats-different",
    "href": "blog/Walkwithfastai.html#why-do-this-whats-different",
    "title": "Announcing Walk with fastai, the missing pieces for success",
    "section": "Why do this? What’s different?",
    "text": "Why do this? What’s different?\nFirst let’s talk about what’s new. This course will still take you from someone who either has completed Jeremy Howards course or who hasn’t even touched fastai at all and take you to a level above most practitioners. We still start at an understanding of some Python, and maybe some PyTorch and take you further beyond.\nHowever it’s been three years since the first came out and much has changed. New techiques have been released, changes to the framework have occured, and in these times concepts like distributed computing mean much more! As a result over 50% of the course material is completely new.\nBut wait, what about the old material?\nThe old material is also completely redone and added verbose content thanks to the power of Quarto and nbdev. Just look at the before and after:\n\nBefore:\n\n\n\nAfter:\n\nNo longer is this course just a passing resource, but instead it could be read entirely on its own without the accompaning lecture content and be infinitely more valuable than its predecessor."
  },
  {
    "objectID": "blog/Walkwithfastai.html#what-about-the-original-course",
    "href": "blog/Walkwithfastai.html#what-about-the-original-course",
    "title": "Announcing Walk with fastai, the missing pieces for success",
    "section": "What about the original course?",
    "text": "What about the original course?\nThe original course will always be free and a readily-available resource to all, however its content will not be updated for latest iterations of the framework and so forth. The YouTube videos will not be going anywhere, and the code repository wil still be publically available. Always.\nThat being said, this new version of the course will go much futher beyond than the original did, and has enough material to have it stand out as its own and not just be a complete repeat of what was presented three years ago."
  },
  {
    "objectID": "blog/Walkwithfastai.html#not-convinced-heres-what-others-have-to-say-about-the-original-course",
    "href": "blog/Walkwithfastai.html#not-convinced-heres-what-others-have-to-say-about-the-original-course",
    "title": "Announcing Walk with fastai, the missing pieces for success",
    "section": "Not convinced? Here’s what others have to say about the original course:",
    "text": "Not convinced? Here’s what others have to say about the original course:\n\nZach Mueller is probably the most active member of the fastai community. He is also very actively involved with fastai development. If there is one person who understands the fastai library through and through (apart from Jeremy Howard and Sylvain Gugger), that is certainly Zach! He went out of his way to create a fabulous learning resource for us, Walk With Fastai. Numerous notebooks explaining how to work with the library to achieve wonderful results. A must read! - Radek Osmulski, Senior Data Scientist at NVIDIA AI\n\n\nWalk with Fastai is the best source available for utilizing fastai and goes above and beyond what is available in the documentation. Zach somehow extracted all of the knowledge I wanted to know about fastai and combined it into blog posts, videos, and source code that are easy to follow and build on. If you want to use fastai, this is a great resource to reference. I’ve had to go back and forth a couple of times between the best source (between it and fastbook) and I think I’ve gotten more out of Walk with fastai. - Kevin Bird\n\n\nI really enjoyed the vision section Zach’s Walk with fastai series. His extension of the fastai keypoint example from a single point to multipoint was really great and importantly included covering the basics of doing ML well such as cleaning the data, ensuring the transforms make sense and so forth before starting any modelling - super critical to teaching deep learning. I loved that he then goes deeper and shows off how to customize a UNET for key point regression (because why not!) and really get the most out of the fastai API. Highly recommended as an accompaniment to the fastai course! - Morgan McGuire, Growth ML Engineer at Weights and Biases\n\n\nYour addendum course videos in Walk with fastai is so super useful. I’ve learnt so much more about using fastai effectively in new problem spaces by combining Jeremy’s lessons with your own, and am still learning as I am still progressing through them. - Nissan Dookeran"
  },
  {
    "objectID": "blog/Walkwithfastai.html#how-do-i-sign-up",
    "href": "blog/Walkwithfastai.html#how-do-i-sign-up",
    "title": "Announcing Walk with fastai, the missing pieces for success",
    "section": "How do I sign up?",
    "text": "How do I sign up?\nPreorders are now live, which you can find here.\nI value your trust in me as a teacher and a creator so if you preorder before the course begins you will be able to not only attend the lectures live and directly as me questions, but you will also get weekly updates from me detailing progress, teases, and overall how the course is coming along.\nIf you do not preorder you will receive access to the material as a lesson finishes, within a day for the unedited lecture and within a week for the edited ones.\nAlso starting today preorders are now 50% off, so be sure to get them while they’re available!"
  },
  {
    "objectID": "blog/TabularNumpy.html",
    "href": "blog/TabularNumpy.html",
    "title": "Speeding up fastai Tabular with NumPy",
    "section": "",
    "text": "Speeding up fastai tabular training by 40%"
  },
  {
    "objectID": "blog/TabularNumpy.html#what-is-fastai-tabular-a-tldr",
    "href": "blog/TabularNumpy.html#what-is-fastai-tabular-a-tldr",
    "title": "Speeding up fastai Tabular with NumPy",
    "section": "What is fastai Tabular? A TL;DR",
    "text": "What is fastai Tabular? A TL;DR\nWhen working with tabular data, fastai has introduced a powerful tool to help with prerocessing your data: TabularPandas. It’s super helpful and useful as you can have everything in one place, encode and decode all of your tables at once, and the memory usage on top of your Pandas dataframe can be very minimal. Let’s look at an example of it.\nFirst let’s import the tabular module:\nfrom fastai2.tabular.all import *\nFor our particular tests today, we’ll be using the ADULT_SAMPLE dataset, where we need to identify if a particular individual makes above or below $50,000. Let’s grab the data:\npath = untar_data(URLs.ADULT_SAMPLE)\nAnd now we can open it in Pandas:\ndf = pd.read_csv(path/'adult.csv')\ndf.head()\nNow that we have our DataFrame, let’s fit it into a TabularPandas object for preprocessing. To do so, we need to decalre the following:\n\nprocs (pre-processing our data, such as normalization and converting categorical values to numbers)\ncat_names (categorical variables)\ncont_names (continuous variables)\ny_names (our y columns)\n\nFor our case, these look like so:\ncat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\ncont_names = ['age', 'fnlwgt', 'education-num']\nprocs = [Categorify, FillMissing, Normalize]\ny_names = 'salary'\nWe’ll also need to tell TabularPandas how we want to split our data. We’ll use a random 20% subsample:\nsplits = RandomSplitter()(range_of(df))\nNow let’s make a TabularPandas!\nto = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,\n                   y_names=y_names, splits=splits)\nNow all of our data is pre-processed here and we can grab all of the raw values if we wanted to say use it with XGBoost like so:\nto.train.xs.iloc[:3]\nAndi it’s fully encoded! Now that we’re a bit familiar with TabularPandas, let’s do some speed tests!"
  },
  {
    "objectID": "blog/TabularNumpy.html#the-baseline",
    "href": "blog/TabularNumpy.html#the-baseline",
    "title": "Speeding up fastai Tabular with NumPy",
    "section": "The Baseline",
    "text": "The Baseline\nFor our tests, we’ll run 4 different tests: 1. One batch of the training data 2. Iterating over the entire training dataset 3. Iterating over the entire validation set 4. Fitting for 10 epochs (GPU only)\nAnd for each of these we will compare the times on the CPU and the GPU.\n\nCPU:\nFirst let’s grab the first batch. The reason this is important is each time we iterate over the training DataLoader, we actually shuffle our data, which can add some time:\ndls = to.dataloaders(bs=128, device='cpu')\nTo test our times, we’ll use %%timeit. It measures the execution time of a Python function for a certain amount of loops, and reports back the fastest one. For iterating over the entire DataLoader we’ll look at the time per batch as well.\nFirst, a batch from training:\n%%timeit\n_ = next(iter(dls.train))\nNow the validation:\n%%timeit\n_ = next(iter(dls.valid))\nAlright, so first we can see that our shuffling function is adding almost 15 milliseconds on our time, something we can improve on! Let’s then go through the entire DataLoader:\n%%timeit\nfor _ in dls.train:\n    _\nNow let’s get an average time per batch:\nprint(661/len(dls.train))\nAbout 3.25 milliseconds per batch on the training dataset, let’s look at the validation:\n%%timeit\nfor _ in dls.valid:\n    _\nprint(159/len(dls.valid))\nAnd about 3.11 milliseconds per batch on the validation, so we can see that it’s about the same after shuffling. Now let’s compare some GPU times:\n\n\nGPU\ndls = to.dataloaders(bs=128, device='cuda')\n%%timeit\n_ = next(iter(dls.train))\n%%timeit\n_ = next(iter(dls.valid))\nSo first, grabbing just one batch we can see it added about a half a millisecond on the training and .2 milliseconds on the validation, so we’re not utilizing the GPU for this process much (which makes sense, TabularPandas is CPU bound). And now let’s iterate:\n%%timeit\nfor _ in dls.train:\n    _\nprint(693/len(dls.train))\n%%timeit\nfor _ in dls.valid:\n    _\nprint(163/len(dls.valid))\nAnd here we can see a little bit more being added here as well. Now that we have those baselines, let’s fit for ten epochs real quick:\nlearn = tabular_learner(dls, layers=[200,100], metrics=accuracy)\n%%time\nlearn.fit(10, 1e-2)\nAfter fitting, we got about 22.9 seconds in total and ~2.29 seconds per epoch! Now that we have our baselines, let’s try to speed that up!"
  },
  {
    "objectID": "blog/TabularNumpy.html#bringing-in-numpy",
    "href": "blog/TabularNumpy.html#bringing-in-numpy",
    "title": "Speeding up fastai Tabular with NumPy",
    "section": "Bringing in NumPy",
    "text": "Bringing in NumPy\n\nThe Dataset\nWith speeding everything up, I wanted to keep TabularPandas as it is, as it’s a great way to pre-process your data! So instead we’ll create a new Dataset class where we will convert our TabularPandas object into a NumPy array. Why is that important? NumPy is a super-fast library that has been hyper-optimized by using as much C code as it possibly can which is leagues faster than Python. Let’s build our Dataset!\nWe’ll want it to maintain the cats, conts, and ys from our TabularPandas object seperate. We can call to_numpy() on all of them because they are simply stored as a DataFrame! Finally, to deal with categorical versus continuous variables, we’ll assign our cats as np.long and our conts as np.float32 (we also have our ys as np.int8, but this is because we’re doing classification):\nclass TabDataset():\n    \"A `NumPy` dataset from a `TabularPandas` object\"\n    def __init__(self, to):\n        self.cats = to.cats.to_numpy().astype(np.long)\n        self.conts = to.conts.to_numpy().astype(np.float32)\n        self.ys = to.ys.to_numpy()\nGreat! Now we need a few more bits for everything to work! For our Dataset to function, we need to be able to gather the values from it each time we call from it. We use the __getitem__ function to do so! For our particular problem, we need it to return some cats, conts, and our ys. And to save on more time we’ll return a whole batch of values:\nclass TabDataset():\n    \"A `NumPy` dataset from a `TabularPandas` object\"\n    def __init__(self, to):\n        self.cats = to.cats.to_numpy().astype(np.long)\n        self.conts = to.conts.to_numpy().astype(np.float32)\n        self.ys = to.ys.to_numpy()\n\n    def __getitem__(self, idx):\n        idx = idx[0]\n        return self.cats[idx:idx+self.bs], self.conts[idx:idx+self.bs], self.ys[idx:idx+self.bs]\nYou’ll notice we don’t explicitly pass in a batch size, so where is that coming from? This is added when we build our DataLoader, as we’ll see later. Let’s finish up our Dataset class by adding in an option to get the length of the dataset (we’ll do the length of our categorical table in this case).\nclass TabDataset():\n    \"A `NumPy` dataset from a `TabularPandas` object\"\n    def __init__(self, to):\n        self.cats = to.cats.to_numpy().astype(np.long)\n        self.conts = to.conts.to_numpy().astype(np.float32)\n        self.ys = to.ys.to_numpy()\n\n    def __getitem__(self, idx):\n        idx = idx[0]\n        return self.cats[idx:idx+self.bs], self.conts[idx:idx+self.bs], self.ys[idx:idx+self.bs]\n\n    def __len__(self): return len(self.cats)\nAnd now we can make some Datasets!\ntrain_ds = TabDataset(to.train)\nvalid_ds = TabDataset(to.valid)\nWe can look at some data real quick if we want to as well! First we need to assign a batch size:\ntrain_ds.bs = 3\nAnd now let’s look at some data:\ntrain_ds[[3]]\nWe can see that we output what could be considered a batch of data! The only thing missing is to make it into a tensor! Fantastic! Now let’s build the DataLoader, as there’s some pieces in it that we need, so simply having this Dataset won’t be enough\n\n\nThe DataLoader\nNow to build our DataLoader, we’re going to want to modify 4 particular functions:\n\ncreate_item\ncreate_batch\nget_idxs\nshuffle_ds\n\nEach of these play a particular role. First let’s look at our template:\nclass TabDataLoader(DataLoader):\n    def __init__(self, dataset, bs=1, num_workers=0, device='cuda', shuffle=False, **kwargs):\n        \"A `DataLoader` based on a `TabDataset`\"\n        super().__init__(dataset, bs=bs, num_workers=num_workers, shuffle=shuffle, \n                         device=device, drop_last=shuffle, **kwargs)\n        self.dataset.bs=bs\nAs you can see, our __init__ will build a DataLoader, and we keep track of our Dataset and set the Datasets batch size here as well\ndl = TabDataLoader(train_ds, bs=3)\ndl.dataset.bs\ndl.dataset[[0]]\nAnd we can see that we grab everything as normal in the Dataset! Great! Now let’s work on create_item and create_batch. create_item is very simple as we already do so when we make our call to the dataset, so we just pass it on. create_batch is also very simplistic. We’ll take some index’s from our Dataset and convert them all to Tensors!\nclass TabDataLoader(DataLoader):\n    def __init__(self, dataset, bs=1, num_workers=0, device='cuda', shuffle=False, **kwargs):\n        \"A `DataLoader` based on a `TabDataset`\"\n        super().__init__(dataset, bs=bs, num_workers=num_workers, shuffle=shuffle, \n                         device=device, drop_last=shuffle, **kwargs)\n        self.dataset.bs=bs\n    \n    def create_item(self, s): return s\n\n    def create_batch(self, b):\n        cat, cont, y = self.dataset[b]\n        return tensor(cat).to(self.device), tensor(cont).to(self.device), tensor(y).to(self.device)\nNow we’re almost done. The last two pieces missing is get_idxs and shuffle_fn. These are needed as after each epoch we actually shuffle the dataset and we need to get a list of index’s for our DataLoader to use! To save on time (as we’re using array indexing), we can shuffle the interior dataset instead! A major benefit is slicing (consecutive idxs) instead of indexing (non-consecutive idxs). Let’s look at what that looks like:\nclass TabDataLoader(DataLoader):\n    def __init__(self, dataset, bs=1, num_workers=0, device='cuda', shuffle=False, **kwargs):\n        \"A `DataLoader` based on a `TabDataset`\"\n        super().__init__(dataset, bs=bs, num_workers=num_workers, shuffle=shuffle, \n                         device=device, drop_last=shuffle, **kwargs)\n        self.dataset.bs=bs\n    \n    def create_item(self, s): return s\n\n    def create_batch(self, b):\n        \"Create a batch of data\"\n        cat, cont, y = self.dataset[b]\n        return tensor(cat).to(self.device), tensor(cont).to(self.device), tensor(y).to(self.device)\n\n    def get_idxs(self):\n        \"Get index's to select\"\n        idxs = Inf.count if self.indexed else Inf.nones\n        if self.n is not None: idxs = list(range(len(self.dataset)))\n        return idxs\n\n    def shuffle_fn(self):\n        \"Shuffle the interior dataset\"\n        rng = np.random.permutation(len(self.dataset))\n        self.dataset.cats = self.dataset.cats[rng]\n        self.dataset.conts = self.dataset.conts[rng]\n        self.dataset.ys = self.dataset.ys[rng]\nAnd now we have all the pieces we need to build a DataLoader with NumPy! We’ll examine it’s speed now and then we’ll build some convience functions later. First let’s build the Datasets:\ntrain_ds = TabDataset(to.train)\nvalid_ds = TabDataset(to.valid)\nAnd then the DataLoader:\ntrain_dl = TabDataLoader(train_ds, device='cpu', shuffle=True, bs=128)\nvalid_dl = TabDataLoader(valid_ds, device='cpu', bs=128)\nAnd now let’s grab some CPU timings similar to what we did before:\n%%timeit\n_ = next(iter(train_dl))\n%%timeit\n_ = next(iter(valid_dl))\nRight away we can see that we are leagues faster than the previous version. Shuffling only added ~370 microseconds, which means we used 4% of the time! Now let’s iterate over the entire DataLoader:\n%%timeit\nfor _ in train_dl:\n    _\nprint(31.8/len(train_dl))\n%%timeit\nfor _ in valid_dl:\n    _\nprint(8.07/len(valid_dl))\nAnd as we can see, each individual batch of data is about 0.158 milliseconds! Yet again, about 6% of time time, quite a decrease! So we have sucessfully decreased the time! Let’s look at the GPU now:\ntrain_dl = TabDataLoader(train_ds, device='cuda', shuffle=True, bs=128)\nvalid_dl = TabDataLoader(valid_ds, device='cuda', bs=128)\n%%timeit\n_ = next(iter(train_dl))\n%%timeit\n_ = next(iter(valid_dl))\n%%timeit\nfor _ in train_dl:\n    _\nprint(51.5/len(train_dl))\n%%timeit\nfor _ in valid_dl:\n    _\nprint(12.8/len(valid_dl))\nWhich as we can see, it adds a little bit of time from converting the tensors over to cuda. You could save a little bit more by converting first, but as this should be seperate from the dataset I decided to just keep it here. Now that we have all the steps, finally we can take a look at training! First let’s build a quick helper function to make DataLoaders similar to what fastai’s tabular_learner would be expecting:\nclass TabDataLoaders(DataLoaders):\n    def __init__(self, to, bs=64, val_bs=None, shuffle_train=True, device='cpu', **kwargs):\n        train_ds = TabDataset(to.train)\n        valid_ds = TabDataset(to.valid)\n        val_bs = bs if val_bs is None else val_bs\n        train = TabDataLoader(train_ds, bs=bs, shuffle=shuffle_train, device=device, **kwargs)\n        valid = TabDataLoader(valid_ds, bs=val_bs, shuffle=False, device=device, **kwargs)\n        super().__init__(train, valid, device=device, **kwargs)\ndls = TabDataLoaders(to, bs=128, device='cuda')\nAnd now we can build our model and train! We need to build our own TabularModel here, so we’ll need to grab the size of our embeddings and build a Learner. For simplicity we’ll still use TabularPandas to get those sizes:\nemb_szs = get_emb_sz(to)\nnet = TabularModel(emb_szs, n_cont=3, out_sz=2, layers=[200,100]).cuda()\nlearn = Learner(dls, net, metrics=accuracy, loss_func=CrossEntropyLossFlat())\nAnd now let’s train!\n%%time\nlearn.fit(10, 1e-2)\nAs you can see, we cut the speed down 60%! So we saw a tremendous speed up! Let’s quickly revisit all of the times and results in a pretty table."
  },
  {
    "objectID": "blog/TabularNumpy.html#results",
    "href": "blog/TabularNumpy.html#results",
    "title": "Speeding up fastai Tabular with NumPy",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\n\n\n\n\n\nCPU?\nFirst Batch\nPer Batch\nPer Epoch\nTen Epochs\n\n\n\n\nfastai2\nYes\n18.3ms (train) 3.37ms (valid)\n3.25ms (train) 3.11ms (valid)\n\n\n\n\n\nNo\n18.8ms (train) 3.49ms (valid)\n3.41ms (train) 3.19ms (valid)\n2.29s\n22.9s\n\n\nNumPy\nYes\n0.669ms (train) 0.3ms (valid\n0.15ms (train) 0.15ms (valid)\n\n\n\n\n\nNo\n0.835ms (train) 0.451ms (valid)\n0.25ms (train) 0.25ms (valid)\n1.38s\n13.8s\n\n\n\nSo in summary, we first sped up the time to grab a single batch of data by converting everything from Pandas to NumPy. Afterwards we made a custom DataLoader that could handle these NumPy arrays and induce the speedup we saw! I hope this article helps you better understand how the interior DataLoader can be integrated in with NumPy, and that it helps you speed up your tabular training!\n\nSmall note: show_batch() etc will not work with this particular code base, this is simply a proof of concept"
  },
  {
    "objectID": "blog/TransformFunctions.html",
    "href": "blog/TransformFunctions.html",
    "title": "The Idea of a Transform",
    "section": "",
    "text": "Utilizing basic functions inside the DataBlock"
  },
  {
    "objectID": "blog/TransformFunctions.html#the-datablock-api-continued",
    "href": "blog/TransformFunctions.html#the-datablock-api-continued",
    "title": "The Idea of a Transform",
    "section": "The DataBlock API Continued",
    "text": "The DataBlock API Continued\nThis is part two of my series exploring the DataBlock API. If you have not read part one, fastai and the New DataBlock API, see here In it, we discussed the ideas of the DataBlock as a whole and how each of the lego-bricks can fit together to help solve some interesting problems. In this next blog, we’ll be slowly diving into more complex ideas and uses with it, such as adjusting our y values inside of our get_y, dealing with classification data seperated by folders (and the splitters we can use)\nAlso, as a little note, this blog is not explaining the Transform class, this will come later\nFrom here on we’ll be focusing solely on generating the DataLoaders. Seperate blogs will be made about training the various models. Now, onto the code! As we’re still Vision based, we’ll use the vision sub-library:\nfrom fastai2.vision.all import *\n\nImageWoof\nImageWoof is a subset of 10 dogs from ImageNet. The idea is that these 10 species of dogs are extremely similar, and so they’re hard to classify from scratch. We won’t care about that part today, let’s go through and see how the data is formatted and apply the DataBlock. First let’s grab the data:\npath = untar_data(URLs.IMAGEWOOF)\nNow if we take a look at the path first, we’ll notice that we have train and val folders. The two ideas I’ll be introducing with this dataset for splitting and labelling are GrandparentSplitter and parent_label\npath.ls()\nWhat do each of these do? I’ll go into heavy detail on fastai’s splitters and labellers but GrandparentSplitter operates with the assumption our data is split like ImageNet, where we have training data in a training folder and validation data into a validation folder such as here. Let’s make a splitter now by passing in the name of the training folder and the validation folder:\nsplitter = GrandparentSplitter(train_name='train', valid_name='val')\nLet’s look at some splits. First we’ll grab our list of images then use our GrandparentSplitter to seperate out two indicies for us, which we’ll then look at to make sure it’s working properly\nitems = get_image_files(path)\nsplits = splitter(items)\nsplits[0][0], splits[1][0]\nNow let’s look at images 0 and 9025:\nitems[0], items[9025]\nAnd we can see that the folders line up!\nNow that we have the splitter out of the way, we need a way to get our classes! But what do they look like? We’ll look inside the train folder at some of the images for some examples:\ntrain_p = path/'train'\ntrain_p.ls()[:3]\nitems = get_image_files(train_p)[:5]; items\nWe can visualize this folder setup like so:\n\nWhat this tells us is that our labels are in the folder one level above the actual image, or in the parent folder (if we consider it like a tree). As such, we can use the parent_label function to extract it! Let’s look:\nlabeller = parent_label\nlabeller(items[0])\nFrom here we can simply build our DataBlock similar to the last post:\nblocks = (ImageBlock, CategoryBlock)\nitem_tfms=[Resize(224)]\nbatch_tfms=[Normalize.from_stats(*imagenet_stats)]\nblock = DataBlock(blocks=blocks,\n                  get_items=get_image_files,\n                  get_y=parent_label,\n                  item_tfms=item_tfms,\n                  batch_tfms=batch_tfms)\nAnd make our DataLoaders:\ndls = block.dataloaders(path, bs=64)\nTo make sure it all worked out, let’s look at a batch:\ndls.show_batch(max_n=3)"
  },
  {
    "objectID": "blog/TransformFunctions.html#the-idea-of-a-transform",
    "href": "blog/TransformFunctions.html#the-idea-of-a-transform",
    "title": "The Idea of a Transform",
    "section": "The idea of a transform",
    "text": "The idea of a transform\nNow we’re still going to use the ImageWoof dataset here, but I want to introduce you to the concept of a transform. From an outside perspective and what we’ve seen so far, this is normally limited to what we would call “augmentation.” With the new fastai this is no longer the case. Instead, let’s think of a transform as “any modification we can apply to our data at any point in time.”\nBut what does that really mean? What is a transform? A function! Any transform can be written out as a simple function that we pass in at any moment.\nWhat do I mean by this though? Let’s take a look at those labels again. If we notice, we see bits like:\nlabeller(items[0]), labeller(items[1200])\nBut that has no actual meaning to us (or anyone else reading to what we are doing). Let’s use a transform that will change this into something readable.\nFirst we’ll build a dictionary that keeps track of what each original class name means:\nlbl_dict = dict(n02086240= 'Shih-Tzu',\n  n02087394= 'Rhodesian ridgeback',\n  n02088364= 'Beagle',\n  n02089973= 'English foxhound',\n  n02093754= 'Australian terrier',\n  n02096294= 'Border terrier',\n  n02099601= 'Golden retriever',\n  n02105641= 'Old English sheepdog',\n  n02111889= 'Samoyed',\n  n02115641= 'Dingo'\n)\nNow to use this as a function, we need a way to look into the dictionary with any raw input and return back our string. This can be done via the __getitem__ function:\nlbl_dict.__getitem__(labeller(items[0]))\nBut what is __getitem__? It’s a generic python function in classes that will look up objects via a key. In our case, our object is a dictionary and so we can pass in a key value to use (such as n02105641) and it will know to return back “Old English sheepdog” when called\nLooks readable enough now, right? So where do I put this into the API. We can stack these mini-transforms anywhere we’d like them applied. For instance here, we want it done on our get_y, but after parent_label has been applied. Let’s do that:\nblock = DataBlock(blocks=blocks,\n                  get_items=get_image_files,\n                  get_y=[parent_label, lbl_dict.__getitem__],\n                  item_tfms=item_tfms,\n                  batch_tfms=batch_tfms)\ndls = block.dataloaders(path, bs=64)\ndls.show_batch(max_n=3)\nAwesome! It worked, and that was super simple. Does the order matter here though? Let’s try reversing it:\nblock = DataBlock(blocks=blocks,\n                  get_items=get_image_files,\n                  get_y=[lbl_dict.__getitem__, parent_label],\n                  item_tfms=item_tfms,\n                  batch_tfms=batch_tfms)\ndls = block.dataloaders(path, bs=64)\n\nOh no, I got an error! What is it telling me? That I was passing in the full image path to the dictionary before we extracted the parent_label, so order does matter in how you place these functions! Further, these functions can go in any of the building blocks for the DataBlock except during data augmentation (as these require special modifications we’ll look at later)."
  }
]