[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Zachary Mueller, and I‚Äôm a Machine Learning Software Engineer at ü§ó. I majored in Software Design and Development and I have minors in both Computer Science and Environmental Science.\nI have a heavy passion for Deep Learning and Open Source libraries. As a result, below you will find some notable articles I‚Äôve written, a few courses I‚Äôve made, some of software libraries I‚Äôve written, interesting projects, and the open source libraries I have tried to contribute the most to."
  },
  {
    "objectID": "index.html#my-blogs",
    "href": "index.html#my-blogs",
    "title": "About",
    "section": "My blogs",
    "text": "My blogs\nExplore this website to learn more! Generally most of my larger articles are in /blog."
  },
  {
    "objectID": "index.html#outside-articles-and-posts",
    "href": "index.html#outside-articles-and-posts",
    "title": "About",
    "section": "Outside Articles and Posts",
    "text": "Outside Articles and Posts\n\n\n\n\n\n\n\nFrom PyTorch DDP to Accelerate to Trainer, mastery of distributed training with ease (October 2022)\nAn overview of training deep learning models in PyTorch, particularly focusing on Distributed Data Parallelism (DDP) for distributed training across multiple GPUs or machines. It discusses the setup and usage of DDP, as well as introduces ü§ó Accelerate, a library that simplifies distributed training with minimal code changes. Additionally, the article briefly covers the use of the ü§ó Trainer API, which abstracts training details for even easier distributed training.\n\n\nMethods for Automating Learning Rate Finders (March 2021)\nAddressing the challenges of manually selecting learning rates for deep learning models and presents automated methods for finding optimal learning rates. The methods, including Leslie Smith‚Äôs approach and those developed by Novetta and ESRI, are compared across various datasets and domains, such as computer vision classification, semantic segmentation, tabular classification, and natural language processing.\n\n\nlib2nbdev (June 2021)\nNbdev, originating from fast.ai, simplifies the software engineering process by centralizing code, documentation, and tests in Jupyter Notebooks, enhancing project readability and testing while streamlining development-to-production pipelines. It offers auto-generated documentation, release generators, multiprocessing for testing, and GitHub Actions integration, and Novetta‚Äôs open-source tool, lib2nbdev, facilitates the conversion of existing Python projects into nbdev libraries, saving substantial time and effort."
  },
  {
    "objectID": "index.html#scientific-publications",
    "href": "index.html#scientific-publications",
    "title": "About",
    "section": "Scientific Publications",
    "text": "Scientific Publications\n\n\n\n\n\n\n\nA Neural Network Model to Identify Relative Movements from Wearable Devices\nPresents a neural network model for analyzing movement data from wearable devices, utilizing various base variables and extensive feature engineering to generate 171 unique features. The model achieves a remarkable 95% average accuracy on test datasets, outperforming a previous similar work by 20%.\n\n\nAffordably Assessing and Comparing Trends in Particulate Matter Concentrations Around a University Campus\nAddresses the importance of air quality for human health by creating affordable IoT devices using Raspberry Pi Zeros and Honeywell PM sensors to monitor PM 10 and PM 2.5 air pollution around the University of West Florida campus. The goal was to track air quality trends during periods of high building usage and develop a neural network analysis for future predictions."
  },
  {
    "objectID": "index.html#courses",
    "href": "index.html#courses",
    "title": "About",
    "section": "Courses",
    "text": "Courses\n\n\n\n\n\n\n\nWalk with fastai: Revisited\nA full revamp of the original Walk with fastai course made for the modern version of the library and focusing on different aspects than the prior iterations.\n\n\nWalk with fastai\nWalk with fastai was the second iteration of my exploration with courses. In it I taught over 10 lessons on the new fastai API, exploring new avenues that the API could preform and displaying unique situations often not covered within the orignal course. So far it has been wildly successful, with many of the lectures garnishing over a thousand views on YouTube\n\n\nPractical Deep Learning for Coders 2.0\nThis was a rehash of the fastai course of the same name that I taught at the University of West Florida through the Artificial Intellegence Research Group in 2019"
  },
  {
    "objectID": "index.html#open-source-software-libraries",
    "href": "index.html#open-source-software-libraries",
    "title": "About",
    "section": "Open Source Software Libraries",
    "text": "Open Source Software Libraries\n\n\n\n\n\n\n\nAccelerate\nWhile at Hugging Face, I have been a major developer on the Accelerate library; it is a framework designed to ensure PyTorch code can be ran on any system without code changes.\n\n\nAdaptNLP\nWhile at Novetta, I was the lead developer for the AdaptNLP library. This library wraps the HuggingFace Transformers and fastai libraries into an easy-to-use interface for both fine-tuning and performing inference with transformer-based models. It was featured in the Weights and Biases HuggingFace Study Group\n\n\nlib2nbdev\nSeeing a need to ease integration of the nbdev framework into more projects, I built lib2nbdev as a single-use library that will convert existing python projects into a compatible nbdev library.\n\n\nwwf\nAlong with being a course, the Walk with fastai library (wwf) is also a conglomerate of useful documentation and code for fastai applications centralized in a single location\n\n\nnbverbose\nnbverbose aims at combining both the one-line documentation style that fastai incentivizes without losing best practices nor documentation accessability. By combining the fastcore library with nbdev, this library provides an efficient way to document your code while minimizing the vertical space needed to get there\n\n\nfastinference\nThe fastinference library was designed around making inference with the fastai library simpler, faster, and more approachable. This entailed providing ONNX exportability as well as in-house interpretability modules\n\n\nfastdebug\nAimed at easing the annoyance of common bugs, this library helps users of both the PyTorch and fastai ecosystems provide clear debugging logs when it comes to extremely common exceptions that get thrown that could use a little touch-up\n\n\nfastshap\nThe fastshap library brought in SHAP into the fastai framework. It has since been merged with the fastinference module, though one compatible with fastai version 1 is available on pypi"
  },
  {
    "objectID": "index.html#open-source-contributions",
    "href": "index.html#open-source-contributions",
    "title": "About",
    "section": "Open Source Contributions",
    "text": "Open Source Contributions\n\n\n\nHugging Face\nfastai\n\n\n\n\nAccelerate\nfastai\n\n\nHub\nfastcore\n\n\ntransformers\nnbdev"
  },
  {
    "objectID": "blog/A-New-Year.html",
    "href": "blog/A-New-Year.html",
    "title": "New Year, New Adventures",
    "section": "",
    "text": "I realize I haven‚Äôt been updating this as much as I would have liked to, so this year I intend on publishing one new blog a month. These blogs will be on whatever topic I focused on that month (or previous months), along with any major guides or updates I might have had. Let‚Äôs start with the New Year:\nfastai2, a new version of the fastai library is currently in development and I have plunged deep into the library learning all of its secrets. This was at first mostly due to just seeing what was new, but as I learned more about the new API, the framework, and how it all fits together I realized I liked it much more than fastaiv1 (not saying there was anything wrong with the first version). From October of last year until now I‚Äôve been doing this exploration which culminated into running a Study Group / Course: A walk with fastai2. The goal was to keep the library and teaching simplistic enough that most people could follow and join without too much back and forth. This will be going on from now until the end of the school year, when I begin my internship! (More to come on that).\nWhile we are in the first few weeks of A walk with fastai2, I‚Äôve received many questions asking about how to explore it‚Äôs source code and documentation (versus the first version, which could struggle with code-understanding at times). The largest difference there is that the library is built with a different library, nbdev. nbdev is designed to help make the process of building a library and documentation all at once while never leaving your Jupyter environment. As a result, these fastai2 notebooks come with working examples and tests that you can play around with to learn just what the code is really doing! One very great improvement. The notebooks follow the same names as the .py files, so tracking is very easy.\nAnother benefit with the new API is many of the struggles with the old library were solved: * Easier to make labeled test sets * Medium-level API is much easier to work with * Low-level API can be even easier than the medium level! * GPU transforms * Bring in more State-of-the-Art techniques\nBut what does this all mean? How do I implement xyz task? I‚Äôm hoping in the study group to touch on most any problem that people would face. I‚Äôve been away because I‚Äôve been building numerous notebooks (planning total of 37) dealing with tasks from as simple as telling your model ‚ÄúI don‚Äôt know‚Äù to as complex as building a new addition to the API from scratch!"
  },
  {
    "objectID": "blog/A-New-Year.html#from-there",
    "href": "blog/A-New-Year.html#from-there",
    "title": "New Year, New Adventures",
    "section": "From there:",
    "text": "From there:\nWhile that is a lot, that is not the only things I will have been up to. Along with the new library, Sanyam Bhutani was kind enough to invite me onto his Chai-Time podcast series, and the episode will go live here in the next few weeks. This year I also want to get into Kaggling as I have been very intimidated to do so. While the course is running I will most likely not, as there is just too much on my plate. But once it is over I intend on joining one competition and seeing it start to finish (something I am notirous not to do!) Also just before winter break I was able to visit San Francisco and grab lunch with Jeremy Howard, which was an amazing experience. Finally, in some last news, this summer I will be interning at Novetta with an amazing team of interns looking to work with fastai and fastai2 on a number of tasks! Thank you for reading, this one was rather brief due to the craziness that is getting this lecture together. The other blogs will be much more balanced."
  },
  {
    "objectID": "blog/HackingTheEnum.html",
    "href": "blog/HackingTheEnum.html",
    "title": "Hacking the Enum",
    "section": "",
    "text": "A tale of how to work around some of the limitations of the Enum class and expanding it to behave in ways that seem user-intuitive as an API"
  },
  {
    "objectID": "blog/HackingTheEnum.html#what-will-we-talk-about-in-this-blog",
    "href": "blog/HackingTheEnum.html#what-will-we-talk-about-in-this-blog",
    "title": "Hacking the Enum",
    "section": "What will we talk about in this blog?",
    "text": "What will we talk about in this blog?\nThis will be a straightforward story of how I went down a rabbit-hole to get Enum working how I wanted it to, why this came in the first place, and it‚Äôs very specific use-case.\nHopefully what you, the reader, get out of this is learning a bit more about the Enum class, how metaclasses work, and potentially finding capabilities to use this class in your own user-centric API‚Äôs."
  },
  {
    "objectID": "blog/HackingTheEnum.html#okay-but-what-is-an-enum-and-why-do-you-need-to-hack-it",
    "href": "blog/HackingTheEnum.html#okay-but-what-is-an-enum-and-why-do-you-need-to-hack-it",
    "title": "Hacking the Enum",
    "section": "Okay‚Ä¶ but what is an Enum? And why do you need to hack it?",
    "text": "Okay‚Ä¶ but what is an Enum? And why do you need to hack it?\nSo what is the Enum?\nEnum‚Äôs are a way to write namespace classes in a very dataclass-style API and is part of the python standard library.\nBelow is a literate example using the days of the week:\nimport enum\n\nclass DayOfWeek(enum.Enum):\n    \"An enum containing values of days of the week\"\n    MONDAY = 1\n    TUESDAY = 2\n    WEDNESDAY = 3\n    THURDAY = 4\n    FRIDAY = 5\n    SATURDAY = 6\n    SUNDAY = 7\nNow when we do DaysOfTheWeek.{DAY}, we return back that number we assigned as its property:\nDayOfWeek.MONDAY\nBut wait, that‚Äôs not 1? That‚Äôs some weird thing!\nCorrect. Enum‚Äôs return their member values as a struct-like object. So we can get the value by doing:\nDayOfWeek.MONDAY.value\nAnd the name of that member:\nDayOfWeek.MONDAY.name\nAs you can see it‚Äôs a very quick way for us to write out some structure that contains some values in an object-oriented sense. But what are some of its limitations?\nEven though we know that DayOfTheWeek.MONDAY was set to 1, we can‚Äôt do a raw comparison in python to check this, e.g.:\nDayOfWeek.MONDAY == 1\nBecause it expects to check that .value attribute instead:\nDayOfWeek.MONDAY.value == 1\nThis can get quite annoyting when you want to use the Enum as a convience user-interface, but still want your user to be able to put in the raw values in as well, since adding one should not limit the base API.\nThere‚Äôs also the off-chance that you may wish to have your Enum options be documentable.\nIdeally this shouldn‚Äôt be the case too often, as typically the values in an enum should be quick representations of a generic item (such as SUNDAY and 1, or TORCH and \"torch\").\nBut there may be an API where this could benefit from having explicit documentation for what each special Enum convention means.\nIn the next two sections, we‚Äôll try and figure out how this can be done"
  },
  {
    "objectID": "blog/HackingTheEnum.html#having-enum-return-the-raw-values-and-the-idea-of-metaclasses",
    "href": "blog/HackingTheEnum.html#having-enum-return-the-raw-values-and-the-idea-of-metaclasses",
    "title": "Hacking the Enum",
    "section": "Having Enum return the raw values, and the idea of metaclasses",
    "text": "Having Enum return the raw values, and the idea of metaclasses\nWhat is a metaclass? Metaclasses can generally be thought of as inner classes, that change some innate behavior in the superclass before that superclass is compiled.\nUhhh‚Ä¶ what?\nA better way to think about it, is there are certain functionalities every class does in Python in a factory-way, that you don‚Äôt need to think about or implement when writing a class, such as a class‚Äôs __new__ method, which is automatically called when you run __init__, and it handles creating of a new object for you.\nOkay so‚Ä¶ back to the Enum then please? How does this all connect?\nThe Enum class has one of these core functionalities called __getattribute__. It‚Äôs what the Enum uses to get our MONDAY enum we saw earlier:\nDayOfWeek.__getattribute__(DayOfWeek, \"MONDAY\")\nBut we don‚Äôt ever have to write that to get the value of MONDAY, we just call the attribute directly:\nDayOfWeek.MONDAY\nTo override this, we must do so in a metaclass, as this __getattribute__ must be set and used before a class is compiled.\nBut what happens if we don‚Äôt?\nLet‚Äôs first write what we would think our potential ‚Äúraw value return‚Äù __getattribute__ function should look like.\nIt should take in a name, and then get the attribute in the same way we did just above, before finally getting the value.value.\n\nNote: When Enum‚Äôs are returned, they are of type Enum themselves.\n\ndef __getattribute__(cls, name):\n    value = cls.__getattribute__(name)\n    if isinstance(value, cls):\n        value = value.value\n    return value\nThat seems like that should work right? First we grab the value from cls, then check if it‚Äôs an instance of our Enum, and if so then go dig and get it‚Äôs .value.\nSo, what happens if we throw this into our DayOfWeek?\nclass DayOfWeek(enum.Enum):\n    \"An enum containing values of days of the week\"\n    def __getattribute__(cls, name):\n        value = cls.__getattribute__(name)\n        if isinstance(value, cls):\n            value = value.value\n        return value\n    MONDAY = 1\n    TUESDAY = 2\n    WEDNESDAY = 3\n    THURDAY = 4\n    FRIDAY = 5\n    SATURDAY = 6\n    SUNDAY = 7\nUh oh! A recursion error!\nThis is because it‚Äôs fighting with its original implementation when first compiling, and as a result hit a continuous loop.\nThis goes back to that note I made earlier, about how there are certain functions loaded before a class is compiled.\nSo, how do we bypass this? Through a metaclass.\nEnum has a specific metaclass, designed to be utilized for overriding these functions, the EnumMeta:\nclass DirectValueMeta(enum.EnumMeta):\n    \"Metaclass that allows for directly getting an enum attribute\"\n    def __getattribute__(cls, name):\n        value = super().__getattribute__(name)\n        if isinstance(value, cls):\n            value = value.value\n        return value\nSince we‚Äôre using a metaclass now, we can replace cls with super(), and now the two are seperated.\nFrom here the class will then know to return value, and all will be as it should.\nTo specify metaclasses in python, during any class declaration use the metaclass= notation and it will be applied\nclass DayOfWeek(enum.Enum, metaclass=DirectValueMeta):\n    \"An enum containing values of days of the week\"\n    MONDAY = 1\n    TUESDAY = 2\n    WEDNESDAY = 3\n    THURDAY = 4\n    FRIDAY = 5\n    SATURDAY = 6\n    SUNDAY = 7\nWhat winds up happening however is now the type of DayOfWeek has changed, since it‚Äôs underlying implementation is one of DirectValueMeta:\ntype(DayOfWeek)\nSo, something to keep in mind when testing out and watching for typings (such as isinstance)\nisinstance(DayOfWeek, enum.Enum)\n\nNote: You can get the original class types of an object with a metadata implementation by looking inside of class.__bases__\n\nDayOfWeek.__bases__\nAlright so‚Ä¶ did it work?\nDayOfWeek.MONDAY\nYes it did! And since we overrode how __getattribute__ functions, we can also directly do comparisons between the enum and a non-enum, as though they are the same:\nDayOfWeek.MONDAY == 1\nAwesome! Now let‚Äôs talk about that second idea"
  },
  {
    "objectID": "blog/HackingTheEnum.html#documenting-members-inside-of-an-enum",
    "href": "blog/HackingTheEnum.html#documenting-members-inside-of-an-enum",
    "title": "Hacking the Enum",
    "section": "Documenting members inside of an Enum",
    "text": "Documenting members inside of an Enum\nThis idea came to me during a feverdream at 2am, when I was considering how fastai‚Äôs Callback events are documented.\nSpecifically, they live inside of a namespace-like object that lists every single possible event inside of their training loop, such as before_epoch (before an epoch begins) and after_train (after the training phase was completed).\nThis namespace object was created by using the mk_class function and passing in a list of possible events. From there it stores it as a namespace class that can have a docstring in just a few short lines of code:\n_attrs = L([\"attrA\", \"attrB\", \"attrC\"])\n\nmk_class(\"SomeClass\", **_attrs.map_dict(), \n  doc=\"All possible attrs as attributes to get tab-completion and typo-proofing\")\nThis then creates SomeClass that let‚Äôs us perform SomeClass.attrA, and it will return \"attrA\"\nI wanted to complete three goals with my namespace-hacking endevor here:\n\nIt should be as simple to create as the original implementation was, meaning I should only have to write \"attrA\" once\nIt should be assumed that every attribute should be documented, but they can contain a value instead of just \"attrA\" (such as in our DayOfWeek example where SUNDAY has a value of 7)\nThe repr should then be changed to include the added docstring, if present.\n\nAs a result, I should be able to perform DayOfWeek.SUNDAY? or help(DayOfWeek.SUNDAY) and get back our documentation.\n\nNote: A defined limitation is we assume that every enum must either have a value of itself in lowercase form, or a specified value if it is joined with a docstring. We cannot just have an undocumented value that is special\n\nBefore we begin, let‚Äôs write a clear example of how this API should look, though it cannot be ran of course:\nclass Weekday(SomeInheritedEnumClass):\n    \"The days of the week\"\n    MONDAY = 1, \"The first day of the week\"\n    TUESDAY = \"The second day of the week\"\n    WEDNESDAY = _\n    THURSDAY = 4, \"The fourth day of the week\"\n    FRIDAY = 5, \"The fifth day of the week\"\n    SATURDAY = 6, \"The sixth day of the week\"\n    SUNDAY = \"The seventh day of the week\"\nWhat are the special cases here?\n\nWhen checking MONDAY, it should have a value of 1 and a docstring of \"The first day of the week\"\nWhen checking TUESDAY it should have a value of \"tuesday\" and a docstring of `‚ÄúThe second day of the week‚Äù\nWhen checking WEDNESDAY, it should have no documentation, and a value of \"wednesday\"\n\nLet‚Äôs break this down step-by-step. First, let‚Äôs try and write an implementation that sets Enum.value to be the first value we passed in, as by default it will currently set both items as the value:\nclass SmallEnum(enum.Enum, metaclass=DirectValueMeta):\n    \"An enum containing values of days of the week\"\n    SOMEVALUE = 1, \"Some docstring\"\nSmallEnum.SOMEVALUE\nThe Enum documentation says that when we want to override the returned value of an enum, we should override the __new__ function, create a new object, and then set the _value_, which is where our .value attribute really gets set.\nTheir example is extremely straightforward, so we can adapt it easily:\n&gt;&gt;&gt; class Coordinate(bytes, Enum):\n...     \"\"\"\n...     Coordinate with binary codes that can be indexed by the int code.\n...     \"\"\"\n...     def __new__(cls, value, label, unit):\n...         obj = bytes.__new__(cls, [value])\n...         obj._value_ = value\n...         obj.label = label\n...         obj.unit = unit\n...         return obj\n...     PX = (0, 'P.X', 'km')\n...     PY = (1, 'P.Y', 'km')\n...     VX = (2, 'V.X', 'km/s')\n...     VY = (3, 'V.Y', 'km/s')\n\nNote: I am choosing to use inheritence and a new Enum type rather than keeping the __new__ implementation in the same class, due to the fact I want this to be a reusable capability\n\nFor naming conventions, we‚Äôll call this a DocumentedEnum, or an enum with documentation!\nAnd to keep our first iteration simple, we should assume that we will always pass in tuples of (value, docstring). While this won‚Äôt be what the final product will look like, it‚Äôs a great starting point\n\nNote: We won‚Äôt be using our metaclass here for a myriad of reasons. The main reason is the interaction between the docstring, the metaclass, and the value we want isn‚Äôt meant to be toyed with. So as a result we‚Äôll keep this as a base Enum\n\nclass DocumentedEnum(enum.Enum):\n    \"\"\"\n    An `Enum` capabile of having its members have docstrings\n\n    Should be passed in the form of:\n      value, docstring\n\n    Based on https://stackoverflow.com/questions/19330460/how-do-i-put-docstrings-on-enums\n    \"\"\"\n\n    def __new__(cls, *args):\n        obj = object.__new__(cls)\n        obj._value_ = args[0] # Assign `_value_` to the first argument\n        return obj\n\n    def __init__(self, *args):\n        \"\"\"\n        Creates a generic enumeration with potential assigning of a member docstring\n\n        Should be passed in the form of:\n          value, docstring\n        Or:\n          docstring\n        \"\"\"\n        if len(args) == 2 and isinstance(args[-1], str):\n            self.__doc__ = args[-1]\nLet‚Äôs break down what we‚Äôve done here.\nFirst, __new__:\n\nSimilar to the example, we create a new generic object. Then we assign the first value in args to be the value.\nWe keep the parameter unspecified (*args) so we can pass in as many parameters as needed, be it either one (for just the docstring) or two (value and a docstring). Documenting this behavior should be done in __init__, as we‚Äôve done here.\n\nNext that __init__: * init right now is very simple, we check if the last argument is a string, and if so set that to the docstring. This type-check is just to make sure we can set our docstring as a valid string. * We could likely change this implementation to better guarded, such as checking if it‚Äôs a string and the length of args matches what we should expect. The final implementaiton will reflect i|t.\nNow let‚Äôs see it in action, with our Weekday proposal earlier:\nclass Weekday(DocumentedEnum):\n    \"The days of the week\"\n    MONDAY = 1, \"The first day of the week\"\n    THURSDAY = 4, \"The fourth day of the week\"\n    FRIDAY = 5, \"The fifth day of the week\"\n    SATURDAY = 6, \"The sixth day of the week\"\nLet‚Äôs see how this looks so far, by checking Monday, Tuesday, and Wednesday:\nWeekday.MONDAY, Weekday.MONDAY.__doc__\nThis looks good! That‚Äôs what we expected.\nBut wait, I can‚Äôt do our comparisons anymore, can I?\nThat is indeed correct. So, what can we do?\nWe can instead implement our own custom __eq__. It should be very simple, checking first if we can perform the regular eq (e.g.¬†they‚Äôre of the same type), and if not then we check their .value:\nclass DocumentedEnum(enum.Enum):\n    \"\"\"\n    An `Enum` capabile of having its members have docstrings\n\n    Should be passed in the form of:\n      value, docstring\n\n    Based on https://stackoverflow.com/questions/19330460/how-do-i-put-docstrings-on-enums\n    \"\"\"\n\n    def __new__(cls, *args):\n        obj = object.__new__(cls)\n        obj._value_ = args[0] # Assign `_value_` to the first argument\n        return obj\n\n    def __eq__(self, obj):\n        if type(self) == type(obj): \n            return super().__eq__(obj)\n        return self.value == obj\n\n    def __ne__(self, obj):\n        if type(self) == type(obj):\n            return super().__ne__(obj)\n        return self.value != obj\n\n    def __init__(self, *args):\n        \"\"\"\n        Creates a generic enumeration with potential assigning of a member docstring\n\n        Should be passed in the form of:\n          value, docstring\n        Or:\n          docstring\n        \"\"\"\n        if len(args) == 2 and isinstance(args[-1], str):\n            self.__doc__ = args[-1]\nclass Weekday(DocumentedEnum):\n    \"The days of the week\"\n    MONDAY = 1, \"The first day of the week\"\n    THURSDAY = 4, \"The fourth day of the week\"\n    FRIDAY = 5, \"The fifth day of the week\"\n    SATURDAY = 6, \"The sixth day of the week\"\nWeekday.MONDAY == Weekday.MONDAY\nWeekday.MONDAY == 1\nWeekday.MONDAY != 1\nGreat! While we may have lost the ability to just pull 1 when doing Weekday.MONDAY, we can still use it logically when doing == or !=, so we haven‚Äôt lost that functionality"
  },
  {
    "objectID": "blog/HackingTheEnum.html#reflection",
    "href": "blog/HackingTheEnum.html#reflection",
    "title": "Hacking the Enum",
    "section": "Reflection",
    "text": "Reflection\nAlright, so we just implemented a lot. Is it worth it?\nLet‚Äôs analyze each implementation, and it‚Äôs costs, benefits, and potential use case:\n\nGetting direct values from Enums\nIf you are writing a user-centric API, where the Enum acts as a convience towards some parameter, while also still wanting to accept the string, this should absolutely be implemented. It keeps the API readable and allows for tab-completion for your user (very important!)\nAs we saw later on, the cost of doing so is you limit yourself to a base Enum class and you cannot apply any metaclasses you might want without a large chunk of frustration.\nRecommendation: 7/10\n\nNote: For a more abstract example of something similar, see fastcore‚Äôs AttrDict\n\n\n\nDocumented Enums\nIf you‚Äôre creating your namespace classes well, you shouldn‚Äôt need to document them as their meanings should be straightforward to understand.\nBut: If there is ever a case where this is not possible, then you absolutely should. Not having special acronyms or meanings documented directly with the code in a reachable manner hurts code usability to the user, and can potentially frustrate them.\nRecommendation: 9/10\n\n\nAdding custom comparators to your Enum\nSimilarly to the direct values, if you are writing a user API it is better for you (the writer!) to be able to have a simple interface to perform logic that shouldn‚Äôt need heavy refactoring just to allow for a particular interface.\nRecommendation: 9/10"
  },
  {
    "objectID": "blog/HackingTheEnum.html#conclusion",
    "href": "blog/HackingTheEnum.html#conclusion",
    "title": "Hacking the Enum",
    "section": "Conclusion",
    "text": "Conclusion\nI hope you all enjoyed this little exploration into Enums and subclassing. If you liked this content please make sure to give me a follow on twitter as that‚Äôs where I‚Äôll post new articles and relevent tips or tricks!\nThanks for reading!"
  },
  {
    "objectID": "blog/HuggingFaceLesson1.html",
    "href": "blog/HuggingFaceLesson1.html",
    "title": "HuggingFace Course Notes, Chapter 1 (And Zero), Part 1",
    "section": "",
    "text": "This notebook covers all of Chapter 0, and Chapter 1 up to ‚ÄúHow do Transformers Work?‚Äù"
  },
  {
    "objectID": "blog/HuggingFaceLesson1.html#chapter-0-setup",
    "href": "blog/HuggingFaceLesson1.html#chapter-0-setup",
    "title": "HuggingFace Course Notes, Chapter 1 (And Zero), Part 1",
    "section": "Chapter 0 (Setup):",
    "text": "Chapter 0 (Setup):\nSince HF in of itself has no dependancy requirements, they recommend us installing transformers[dev] so it gets all the dev requirements for ‚Äúany imaginable use case‚Äù.\nA full list of what it installs is below:\ndeps = {\n    \"Pillow\": \"Pillow\",\n    \"black\": \"black==21.4b0\",\n    \"cookiecutter\": \"cookiecutter==1.7.2\",\n    \"dataclasses\": \"dataclasses\",\n    \"datasets\": \"datasets\",\n    \"deepspeed\": \"deepspeed&gt;=0.4.0\",\n    \"docutils\": \"docutils==0.16.0\",\n    \"fairscale\": \"fairscale&gt;0.3\",\n    \"faiss-cpu\": \"faiss-cpu\",\n    \"fastapi\": \"fastapi\",\n    \"filelock\": \"filelock\",\n    \"flake8\": \"flake8&gt;=3.8.3\",\n    \"flax\": \"flax&gt;=0.3.4\",\n    \"fugashi\": \"fugashi&gt;=1.0\",\n    \"huggingface-hub\": \"huggingface-hub==0.0.8\",\n    \"importlib_metadata\": \"importlib_metadata\",\n    \"ipadic\": \"ipadic&gt;=1.0.0,&lt;2.0\",\n    \"isort\": \"isort&gt;=5.5.4\",\n    \"jax\": \"jax&gt;=0.2.8\",\n    \"jaxlib\": \"jaxlib&gt;=0.1.65\",\n    \"jieba\": \"jieba\",\n    \"keras2onnx\": \"keras2onnx\",\n    \"nltk\": \"nltk\",\n    \"numpy\": \"numpy&gt;=1.17\",\n    \"onnxconverter-common\": \"onnxconverter-common\",\n    \"onnxruntime-tools\": \"onnxruntime-tools&gt;=1.4.2\",\n    \"onnxruntime\": \"onnxruntime&gt;=1.4.0\",\n    \"optuna\": \"optuna\",\n    \"packaging\": \"packaging\",\n    \"parameterized\": \"parameterized\",\n    \"protobuf\": \"protobuf\",\n    \"psutil\": \"psutil\",\n    \"pydantic\": \"pydantic\",\n    \"pytest\": \"pytest\",\n    \"pytest-sugar\": \"pytest-sugar\",\n    \"pytest-xdist\": \"pytest-xdist\",\n    \"python\": \"python&gt;=3.6.0\",\n    \"ray\": \"ray\",\n    \"recommonmark\": \"recommonmark\",\n    \"regex\": \"regex!=2019.12.17\",\n    \"requests\": \"requests\",\n    \"rouge-score\": \"rouge-score\",\n    \"sacrebleu\": \"sacrebleu&gt;=1.4.12\",\n    \"sacremoses\": \"sacremoses\",\n    \"sagemaker\": \"sagemaker&gt;=2.31.0\",\n    \"scikit-learn\": \"scikit-learn\",\n    \"sentencepiece\": \"sentencepiece==0.1.91\",\n    \"soundfile\": \"soundfile\",\n    \"sphinx-copybutton\": \"sphinx-copybutton\",\n    \"sphinx-markdown-tables\": \"sphinx-markdown-tables\",\n    \"sphinx-rtd-theme\": \"sphinx-rtd-theme==0.4.3\",\n    \"sphinx\": \"sphinx==3.2.1\",\n    \"sphinxext-opengraph\": \"sphinxext-opengraph==0.4.1\",\n    \"starlette\": \"starlette\",\n    \"tensorflow-cpu\": \"tensorflow-cpu&gt;=2.3\",\n    \"tensorflow\": \"tensorflow&gt;=2.3\",\n    \"timeout-decorator\": \"timeout-decorator\",\n    \"timm\": \"timm\",\n    \"tokenizers\": \"tokenizers&gt;=0.10.1,&lt;0.11\",\n    \"torch\": \"torch&gt;=1.0\",\n    \"torchaudio\": \"torchaudio\",\n    \"tqdm\": \"tqdm&gt;=4.27\",\n    \"unidic\": \"unidic&gt;=1.0.2\",\n    \"unidic_lite\": \"unidic_lite&gt;=1.0.7\",\n    \"uvicorn\": \"uvicorn\",\n}\n\nNote: after exploring a bit I found their requirements are located here\n\n!pip install transformers[dev] -U &gt;&gt; /dev/null # Ensure we upgrade and clean the output\nThis should take a bit to run. I noticed four incompatibility errors in Colab, we‚Äôll see if it has any issues.\n!pip show transformers\nAlright! We can move onto Chapter 1! ü§ó"
  },
  {
    "objectID": "blog/HuggingFaceLesson1.html#chapter-1",
    "href": "blog/HuggingFaceLesson1.html#chapter-1",
    "title": "HuggingFace Course Notes, Chapter 1 (And Zero), Part 1",
    "section": "Chapter 1",
    "text": "Chapter 1\n\nIntroduction\nLooks as though it‚Äôs split into three main chunks:\n\nIntroduction\nDiving in\nAdvanced\n\nIntroduction will show a very surface level with Transformers models and HF Transformers, fine-tuning a basic model, and sharing models and tokenizers.\nDiving in will go further into the HF datasets and tokenizers library, basic NLP tasks, and how to ask for help (presumably on the forums or on Twitter?)\nAdvanced looks to be covering specialized architecture, speeding up training, custom training loops (yay!) and contributing to HF itself.\n\nNote: This is better taken after an intro course such as Practical Deep Learning for Coders or any course developed by deeplearning.ai.\n\nIt also mentions that they don‚Äôt expect any prior PyTorch or Tensorflow knowledge, but some familiarity will help. (fastai likely helps here too some)\nThe wonderful authors:\n\nMatthew Carrigan - MLE @ HF\nLysandre Debut - MLE @ HF, worked with Transformers library from the very beginning\nSylvain Gugger - Research Engineer @ HF, core maintainer of Transformers. And one of our favorite former fastai folk\n\nWhat we will learn:\n\nThe pipeline function\nThe Transformer architecture\nEncoder, decoder, and encoder/decoder architectures and when to use each\n\n\n\nNatural Language Processing\n\nWhat is it?\n\nClassifying whole sentences or each word in a sentence, generating text content, question answering, and generating a new sentence from an input text\n\nWhy is it challenging?\n\nFor a human, given ‚ÄúI am hungry‚Äù and ‚ÄúI am sad‚Äù we can know how similar thye are. That‚Äôs hard for ML models.\n\n\nTransformers, what can they do?\nWe get to look at pipeline now!\nThe Model Hub is a super valuable resource because it contains thousands of pretrained models for you to use, and you can upload your own. The language model zoo.\n\n\nWorking with Pipelines, with Sylvain\n\nOffhand note, I like that the videos are broken up into ~4-5 minute chunks\n\nGeneral approach to how I will take these notes:\n\nWatch video without notes\nRead the website and take notes\nGo back to the video and catch anything I missed\n\nThe pipeline is a very quick and powerful way to grab inference with any HF model.\nLet‚Äôs break down one example below they showed:\nfrom transformers import pipeline\n\nclassifier = pipeline(\"sentiment-analysis\")\nclassifier(\"I've been waiting for a HuggingFace course all my life!\")\nWhat did this do here?\n\nDownloaded a model (judging by the download bar). Don‚Äôt know which model yet is the default\nI think we downloaded a pretrained tokenizer too?\nSaid model was the default for a sentiment-analysis task\nWe asked it to classify the sentiment in our sentence. Labels are positive and negative, and it gave us back an array of dictionaries with those values\n\nWe can also pass in multiple inputs/texts:\nclassifier([\n    \"I've been waiting for a HuggingFace course my whole life.\", \n    \"I hate this so much!\"\n])\nThe default model for this task is a pretrained model fine-uned for sentient analysis in english. Let‚Äôs see if I can‚Äôt find it\ndir(classifier)\nclassifier.framework\ntype(classifier.model)\nSo it‚Äôs a DistilBertForSequenceClassification, likely using the default which would be en-sentiment\nCurrent available pipeline classes: * feature-extraction (vector representation of a text) * fill-mask * ner (Named-Entity-Recognition) * question-answering * sentiment-analysis * text-generation * translation * zero-shot-classification\n\n\nZero-Shot Classification\n\nClassifying unlabelled tasks.\n\nzero-shot-classification pipeline let‚Äôs us specify which labels to use for classification, even if they may differ from the pretrained models.\n\nSide Note: I‚Äôm going to write a quick namespace class via mk_class in fastcore to hold all of these tasks, so I can get tab-completion\n\npip install fastcore &gt;&gt; /dev/null\nfrom fastcore.basics import mk_class\ncls_dict = {'FeatureExtraction':'feature-extraction',\n 'FillMask':'fill-mask',\n 'NER':'ner',\n 'QuestionAnswering':'question-answering',\n 'SentimentAnalysis':'sentiment-analysis',\n 'Summarization':'summarization',\n 'TextGeneration':'text-generation',\n 'Translation':'translation',\n 'ZeroShotClassification':'zero-shot-classification'\n }\n\nmk_class('Task', **cls_dict)\nTask.FeatureExtraction\nAs you can see all I‚Äôve done is load a fancy namespace-like object from fastcore that holds my dictionary values as attributes instead.\nBack to the HF stuff. Let‚Äôs load in a pipeline:\nclassifier = pipeline(Task.ZeroShotClassification)\nSeems this model took quite a bit longer to download, but our Task object is working great!\nclassifier(\n    \"This is a course about the Transformers library\",\n    candidate_labels=['education','politics','business']\n)\nVery interesting, so we can see right away it could tell this was educational! (Or fit the closest to that label.) I wonder how it works under the hood, something I may peruse later.\n\n\nText Generation\nGenerate some fancy text given a prompt.\nSimilar to predictive text feature on my iPhone.\nHas some randomness, so we won‚Äôt 100% get the smae thing each time\ngenerator = pipeline(Task.TextGeneration)\ngenerator(\"In this course we will teach you how to\")\nTheres a few args we can control and pass to it, such as num_return_sequences and max_length.\nThe homework is to try and generate two sentences of 15 words each. Let‚Äôs try that:\ngenerator(\n    \"In Marine Biology,\",\n    num_return_sequences=2,\n    max_length=15\n)\nCool! Easy to use\nA headache I ran into is it‚Äôs num_return_sequences, not num_returned_sequences.\n\n\nUse any model from the Hub in a pipeline\nI love the HuggingFace hub, so very happy to see this in here\nModels can be found on the ModelHub. In this example we use distilgpt2\ngenerator = pipeline(Task.TextGeneration, model='distilgpt2')\n\ngenerator(\n    \"In this course, we will teach you how to\",\n    max_length=30,\n    num_return_sequences=2\n)\n\n\nMask Filling\nFill in the blanks of a given text\nunmasker = pipeline(Task.FillMask)\nunmasker('This course will teach you all about &lt;mask&gt; models.', top_k=2)\nSo here it thought the best word to fill that with was mathematical, followed by computational (and showed the filled in sentence)\ntop_k is how many possibilities are displayed\n\nNote: Model fills &lt;mask&gt;, and different models will have different things it will try and fill that with. One way to check this is by looking at the mask word used in the widget (on HF ModelHub)\n\n\n\nNamed Entity Recognition (NER)\nFind parts of an input text that correspond to entities such as persons, locations, or organizations.\nner = pipeline(Task.NER, grouped_entities=True)\nner(\"My name is Zach Mueller and I go to school in Pensacola\")\nWhat does having it not grouped do?\nner = pipeline(Task.NER, grouped_entities=False)\nner(\"My name is Zach Mueller and I go to school in Pensacola\")\nSo we can see that the first grouped ‚ÄúZach‚Äù and ‚ÄúMueller‚Äù together as a single item, and Pen, Sa, Cola together too (likely split with the subword tokenizer). Having grouped=True sounds like a good default in this case\nMost models that you want to have aligned with this task have some form of POS abbriviation in the name or tag\n\n\nQuestion Answering (QA)\nThis is very straightforward, query a question and then receive an answer given some context.\nqa = pipeline(Task.QuestionAnswering)\nqa(\n    question=\"Where do I work?\",\n    context=\"My name is Zach Mueller and I go to school in Pensacola\"\n)\n\nNote: this is an extraction method, not text generation. So it just extracted Pensacola from the question.\n\n\n\nSummarization\nReduce a text to a shorter one, while keeping most of the important aspects referenced in the text\nsummarizer = pipeline(Task.Summarization)\nsummarizer(\"\"\"\n    America has changed dramatically during recent years. Not only has the number of \n    graduates in traditional engineering disciplines such as mechanical, civil, \n    electrical, chemical, and aeronautical engineering declined, but in most of \n    the premier American universities engineering curricula now concentrate on \n    and encourage largely the study of engineering science. As a result, there \n    are declining offerings in engineering subjects dealing with infrastructure, \n    the environment, and related issues, and greater concentration on high \n    technology subjects, largely supporting increasingly complex scientific \n    developments. While the latter is important, it should not be at the expense \n    of more traditional engineering.\n\n    Rapidly developing economies such as China and India, as well as other \n    industrial countries in Europe and Asia, continue to encourage and advance \n    the teaching of engineering. Both China and India, respectively, graduate \n    six and eight times as many traditional engineers as does the United States. \n    Other industrial countries at minimum maintain their output, while America \n    suffers an increasingly serious decline in the number of engineering graduates \n    and a lack of well-educated engineers.\n\"\"\")\n\n\nTranslation\nThe last task in the tutorial/lesson is machine translation. Usually the model name will have some lang1_to_lang2 naming convention in the title. The easiest way to pick one is to search on the model hub. In this example we‚Äôll translate French to english (let‚Äôs see how much I remember from my French classes in high school!)\ntranslator = pipeline(Task.Translation, model='Helsinki-NLP/opus-mt-fr-en')\ntranslator(\"Je m'apelle Zach, comment-vous est appelez-vous?\")\nWe can also specify a max_lenght or min_length for the generated result\nIn the next chapter, we‚Äôll learn what is inside a pipeline and customizing its behavior"
  },
  {
    "objectID": "blog/TransformFunctions.html",
    "href": "blog/TransformFunctions.html",
    "title": "The Idea of a Transform",
    "section": "",
    "text": "Utilizing basic functions inside the DataBlock"
  },
  {
    "objectID": "blog/TransformFunctions.html#the-datablock-api-continued",
    "href": "blog/TransformFunctions.html#the-datablock-api-continued",
    "title": "The Idea of a Transform",
    "section": "The DataBlock API Continued",
    "text": "The DataBlock API Continued\nThis is part two of my series exploring the DataBlock API. If you have not read part one, fastai and the New DataBlock API, see here In it, we discussed the ideas of the DataBlock as a whole and how each of the lego-bricks can fit together to help solve some interesting problems. In this next blog, we‚Äôll be slowly diving into more complex ideas and uses with it, such as adjusting our y values inside of our get_y, dealing with classification data seperated by folders (and the splitters we can use)\nAlso, as a little note, this blog is not explaining the Transform class, this will come later\nFrom here on we‚Äôll be focusing solely on generating the DataLoaders. Seperate blogs will be made about training the various models. Now, onto the code! As we‚Äôre still Vision based, we‚Äôll use the vision sub-library:\nfrom fastai2.vision.all import *\n\nImageWoof\nImageWoof is a subset of 10 dogs from ImageNet. The idea is that these 10 species of dogs are extremely similar, and so they‚Äôre hard to classify from scratch. We won‚Äôt care about that part today, let‚Äôs go through and see how the data is formatted and apply the DataBlock. First let‚Äôs grab the data:\npath = untar_data(URLs.IMAGEWOOF)\nNow if we take a look at the path first, we‚Äôll notice that we have train and val folders. The two ideas I‚Äôll be introducing with this dataset for splitting and labelling are GrandparentSplitter and parent_label\npath.ls()\nWhat do each of these do? I‚Äôll go into heavy detail on fastai‚Äôs splitters and labellers but GrandparentSplitter operates with the assumption our data is split like ImageNet, where we have training data in a training folder and validation data into a validation folder such as here. Let‚Äôs make a splitter now by passing in the name of the training folder and the validation folder:\nsplitter = GrandparentSplitter(train_name='train', valid_name='val')\nLet‚Äôs look at some splits. First we‚Äôll grab our list of images then use our GrandparentSplitter to seperate out two indicies for us, which we‚Äôll then look at to make sure it‚Äôs working properly\nitems = get_image_files(path)\nsplits = splitter(items)\nsplits[0][0], splits[1][0]\nNow let‚Äôs look at images 0 and 9025:\nitems[0], items[9025]\nAnd we can see that the folders line up!\nNow that we have the splitter out of the way, we need a way to get our classes! But what do they look like? We‚Äôll look inside the train folder at some of the images for some examples:\ntrain_p = path/'train'\ntrain_p.ls()[:3]\nitems = get_image_files(train_p)[:5]; items\nWe can visualize this folder setup like so:\n\nWhat this tells us is that our labels are in the folder one level above the actual image, or in the parent folder (if we consider it like a tree). As such, we can use the parent_label function to extract it! Let‚Äôs look:\nlabeller = parent_label\nlabeller(items[0])\nFrom here we can simply build our DataBlock similar to the last post:\nblocks = (ImageBlock, CategoryBlock)\nitem_tfms=[Resize(224)]\nbatch_tfms=[Normalize.from_stats(*imagenet_stats)]\nblock = DataBlock(blocks=blocks,\n                  get_items=get_image_files,\n                  get_y=parent_label,\n                  item_tfms=item_tfms,\n                  batch_tfms=batch_tfms)\nAnd make our DataLoaders:\ndls = block.dataloaders(path, bs=64)\nTo make sure it all worked out, let‚Äôs look at a batch:\ndls.show_batch(max_n=3)"
  },
  {
    "objectID": "blog/TransformFunctions.html#the-idea-of-a-transform",
    "href": "blog/TransformFunctions.html#the-idea-of-a-transform",
    "title": "The Idea of a Transform",
    "section": "The idea of a transform",
    "text": "The idea of a transform\nNow we‚Äôre still going to use the ImageWoof dataset here, but I want to introduce you to the concept of a transform. From an outside perspective and what we‚Äôve seen so far, this is normally limited to what we would call ‚Äúaugmentation.‚Äù With the new fastai this is no longer the case. Instead, let‚Äôs think of a transform as ‚Äúany modification we can apply to our data at any point in time.‚Äù\nBut what does that really mean? What is a transform? A function! Any transform can be written out as a simple function that we pass in at any moment.\nWhat do I mean by this though? Let‚Äôs take a look at those labels again. If we notice, we see bits like:\nlabeller(items[0]), labeller(items[1200])\nBut that has no actual meaning to us (or anyone else reading to what we are doing). Let‚Äôs use a transform that will change this into something readable.\nFirst we‚Äôll build a dictionary that keeps track of what each original class name means:\nlbl_dict = dict(n02086240= 'Shih-Tzu',\n  n02087394= 'Rhodesian ridgeback',\n  n02088364= 'Beagle',\n  n02089973= 'English foxhound',\n  n02093754= 'Australian terrier',\n  n02096294= 'Border terrier',\n  n02099601= 'Golden retriever',\n  n02105641= 'Old English sheepdog',\n  n02111889= 'Samoyed',\n  n02115641= 'Dingo'\n)\nNow to use this as a function, we need a way to look into the dictionary with any raw input and return back our string. This can be done via the __getitem__ function:\nlbl_dict.__getitem__(labeller(items[0]))\nBut what is __getitem__? It‚Äôs a generic python function in classes that will look up objects via a key. In our case, our object is a dictionary and so we can pass in a key value to use (such as n02105641) and it will know to return back ‚ÄúOld English sheepdog‚Äù when called\nLooks readable enough now, right? So where do I put this into the API. We can stack these mini-transforms anywhere we‚Äôd like them applied. For instance here, we want it done on our get_y, but after parent_label has been applied. Let‚Äôs do that:\nblock = DataBlock(blocks=blocks,\n                  get_items=get_image_files,\n                  get_y=[parent_label, lbl_dict.__getitem__],\n                  item_tfms=item_tfms,\n                  batch_tfms=batch_tfms)\ndls = block.dataloaders(path, bs=64)\ndls.show_batch(max_n=3)\nAwesome! It worked, and that was super simple. Does the order matter here though? Let‚Äôs try reversing it:\nblock = DataBlock(blocks=blocks,\n                  get_items=get_image_files,\n                  get_y=[lbl_dict.__getitem__, parent_label],\n                  item_tfms=item_tfms,\n                  batch_tfms=batch_tfms)\ndls = block.dataloaders(path, bs=64)\n\nOh no, I got an error! What is it telling me? That I was passing in the full image path to the dictionary before we extracted the parent_label, so order does matter in how you place these functions! Further, these functions can go in any of the building blocks for the DataBlock except during data augmentation (as these require special modifications we‚Äôll look at later)."
  },
  {
    "objectID": "blog/Week-1.html",
    "href": "blog/Week-1.html",
    "title": "Summer Smackdown - Week 1",
    "section": "",
    "text": "These posts will most likely wind up being a bit of an odd bunch in terms of formatting until I figure out a style I like and enjoy, as the goal is to merge all Four of the lessons into one big post.\nGiven I wanted to update all of these blogs on Sundays, I decided I would include the first ‚Äòday‚Äô of work as well.\nOverall how the schedule I plan to follow looks is as such:\nAs I started this goal on a Saturday, this week there will not be much in my recap, but I‚Äôll be as inclusive as I can into the small lessons I learned."
  },
  {
    "objectID": "blog/Week-1.html#computational-linear-algebra",
    "href": "blog/Week-1.html#computational-linear-algebra",
    "title": "Summer Smackdown - Week 1",
    "section": "Computational Linear Algebra",
    "text": "Computational Linear Algebra\nWe start off learning about the Markov Chain, a way to describe a sequence of events where the probably of each event depends on the state of the previous event. Also known as the next event is determined by the previous event. The course utilizes the Numpy library to hold matrix multiplications to solve the various problems. My notes go through and futher explain what each answer means in context.\nFor example, problem 1 is about using a stochastic matrix, which is a square probablity matrix, in order to predict how health-related incidents will be in the following year.\nWe start off knowing that the current year had 85% asymtpmatic, 10% symptomatic, 5% AIDS, and 0% death. Next, we were given the following probability table:\n\n\n\nalt text\n\n\nNow that we‚Äôre here, we use matrix multiplication to get our answer:\nimport numpy as np\n\ni = np.array([[.85,.1,.05,0]])\nmat = np.array([[.9,.07,.02,.01],\n                [0,.93,.05,.02],\n                [0,0,.85,.15],\n                [0,0,0,1]])\n\nres = mat.T @ i.T\n\nThe @ symbol is used when doing matrix multiplication, where we multiply each row by the column, and then sum them together.\n\nOne thing Jeremy points out is another way to write the above:\n(i @ mat).T) which saves us a few seconds of code, and looks cleaner.\nThe answer winds up being:\narray([[0.765 ],\n       [0.1525],\n       [0.0645],\n       [0.018 ]])\nHowever, what does the answer mean? Well, it means that within the next year:\n\n76.5% of people will be asymptomatic\n15.25% of people will be symptomatic\n6.45% of people will have AIDS\n1.8% of people will die as a result of their illnesses\n\nWe‚Äôve started using some matrix multiplication to get solutions, but can we get a bit more advanced with it?\nTake problem 2:\n\n\n\nalt text\n\n\nGiven the above table, figure out which store is best for what individual. This is a straight matrix by matrix multiplication problem where we will have ‚Äòdem‚Äô represent a matrix of the demand per individual, and ‚Äòp‚Äô be the prices for each item in two particular shops.\ndem = np.array([[6, 5, 3, 1],\n       [3,6,2,2],\n       [3,4,3,1]])\n\np = np.array([[1.5, 1],\n       [2., 2.5],\n       [5., 4.5],\n       [16., 17.]])\nWe yet again solve this by doing dem@p, which gives us a table that looks like the following:\narray([[50. , 49. ],\n       [58.5, 61. ],\n       [43.5, 43.5]])\nThe above table is now described as having the rows be an individual, and the columns being a particular store with the content as the price they would pay for the items they need. We can see that for Person 1 shop 2 would be the best, for Person 2 shop 1 would be the best, and for Person 3 they could go to either one.\nThen Rachel goes further to describe images a little bit and convolutions, which I was already familar with from the Practical Deep Learning for Coders course, however this Medium article she mentions I found especially helpful: CNNs from Different Viewpoints\nWhat this helped show for me was how matrix multiplication is actually applied within these Neural Networks we are generating through the Fast.AI library, especially the following image:\n\n\n\nalt text\n\n\nHere we have a 2x2 matrix (filter) being applied on a single-channel image (3x3), to get our four results: P,W,R,S. I enjoy this view of how our layers are working as I can see each product mapped with corresponding coordinates, versus a Neural Network viewpoint: \nWhere alpha, beta, gamma, etc are the connections or lines from each node to result.\nThis is as far as I got yesterday, so next week lesson 1 should be fully completed."
  },
  {
    "objectID": "blog/Week-1.html#matrix-calculus",
    "href": "blog/Week-1.html#matrix-calculus",
    "title": "Summer Smackdown - Week 1",
    "section": "Matrix Calculus",
    "text": "Matrix Calculus\nOne thing Jeremy suggests us to do during the Foundations course is turn paper to code, so I wanted to apply that to this course, despite it being pure-math heavy. The goal of doing this was just to know how to apply various scary-looking math into code easier, as my experience before this was none.\nThis week I went over the Introduction and Review sections of the paper, as I last took AP Calculus senior year of high school‚Ä¶ It‚Äôs been a few years.\nSo! The introduction segment. Any activation of a single unit inside a nerual network is done using the ‚Äúdot product of an edge weight vector, w, with an input vector x, plus a scalar bias b.‚Äù Okay. That was a lot thrown out at me. Let‚Äôs make that a bit easier. The above can also be written as y=mx+b, a basic linear function where m and x are both matrix‚Äôs. The better way to right that would be like so:\n\n\n\nalt text\n\n\nWhere n and i are how many layers or activation uits we have. This could then also be written as z = w * x + b where z, the ‚Äòaffine function‚Äô (linear function), is derived from a linear unit that clips negative values to zero from the bias.\nAnother way to visualize a neuron is like so:\n\nNow, when we are training our models, all we are doing is choosing a w and b so we can get our desired output for all of our inputs. We can help choose and navigate what are our best options by using a loss function to grade the final activations to the target for all of our inputs. To help minimize, a variation of gradient decent is used where we take the partial derivitive (gradient) of an activation with respect to w and b.\nIn laymans terms? Gradually tweaking w and b in order to make some loss function as close to zero as we can.\nThe next example shown in the paper is taking a function we‚Äôre familair with, Mean Squared Error, and showing us its derivitive (gradient): \nAt first glance that looks absolutely disgustingly terrifying. But let‚Äôs try to break it down into code instead and see if we can try to understand it better.\nSo first, the original where N is the number of inputs\ndef loss(N):\n  y = 0\n  for x in range(N):\n    y += (targ(x) - activ(x)) ** 2\n  return y/N\nOkay, doesn‚Äôt look too bad now. For all inputs, we take the square of our target minus our activation (or our answer). Let‚Äôs look at that derivitive now. I made two functions, actf and grad as we have that interior summation.\ndef grad(N, w, b):\n  y = 0\n  for x in range(N):\n    y += (targ(x) - actf(x)) ** 2\n  return y/N\n\ndef actf(x, w, b):\n  y = 0\n  for i in range(abs(x)):\n    y += (w[i] * x[i] + b)\n  return max(0, y)\nThat looks a bit better, we can see that w and x are both going to be matrix‚Äôs, weight and input respectivly, and b is our bias.\nAlright, not as scary anymore. The last bit I did was a review on the Scalar derivative rules, and attempting to recreate this in code. For this I found the sympy library a huge help, as we can visualize functions and their derivitives.\nFor example, say I have the equation \nWe can write this in code as y = 3*x**2. Well, if we want the derivitive all we have to do is first declare ‚Äòx‚Äô as a ‚ÄòSymbol‚Äô, then use the .diff function to get the derivitive!\n\nfrom sympy import *\nx = Symbol('x')\ny = 3*x**2\nyprime = y.diff(x)\nThe result will give us 6*x, what we were expecting."
  },
  {
    "objectID": "blog/Week-1.html#natural-language-processing",
    "href": "blog/Week-1.html#natural-language-processing",
    "title": "Summer Smackdown - Week 1",
    "section": "Natural Language Processing",
    "text": "Natural Language Processing\nAs I stated before, this course is not quite done yet, so as a result I‚Äôm going through the notebooks carefully to learn what I can and once the videos are released I will be rewatching them to make sure I did not miss anything and I understand all the concepts. But for now, here‚Äôs what I‚Äôve learned:\nThere are many ethical issues that are brought upon by NLP‚Äôs, such as Google Translate:\n\nHere we can see the same translation does not keep the proper pronouns as it should, and is bias towards men being doctors, not very 21st Century if you ask me!\nBut, onwards we must go. The topics I went into this week were on using Non-Negative Matrix Factorization (NMF) and Single Value Decomposition (SVD) for Topic Modeling. Topic modeling beings with something called a term-document matrix, which is a matrix of words by material. In the example used, we have the amount of times particular names showed up in a few classic books:\n\nThis is called a bags of words approach as we don‚Äôt care for the sentence structure or order words come in, just how often they appear.\nFor this lesson, Rachel used the Newsgroups dataset which consists of 18,000 blog posts that follow 20 topics on a forum, this was popular in the 80‚Äôs and 90‚Äôs apparently as the internet did not exist really then.\nFrom here, we went into a few topics: stop words, stemming, and lemmatization\nStop Words:\nStop words are ‚Äòextremely common words which are of little value in helping‚Äô. There is no single universal list of stop words, and each program uses a slightly different one. For example,the sklearn library has the following first twenty as theirs:\nfrom sklearn.feature_extraction import stop_words\nsorted(list(stop_words.ENGLISH_STOP_WORDS))[:20]\n['a',\n 'about',\n 'above',\n 'across',\n 'after',\n 'afterwards',\n 'again',\n 'against',\n 'all',\n 'almost',\n 'alone',\n 'along',\n 'already',\n 'also',\n 'although',\n 'always',\n 'am',\n 'among',\n 'amongst',\n 'amoungst']\nThese words are usually ignored and dropped, however for Machine Learning its been found recently that we (ML algorithms) may benefit more from their inclusion than exclusion.\nStemming and Lemmatization:\nStemming and lemmatization are used to generate the root forms of words. Lemmatization uses the rules of the original language, thus the tokens are all actually words. In contrast, stemming just chops the ends off of the words. These results won‚Äôt be actual words, but it is faster. ‚ÄúStemming is the poor-man‚Äôs lemmatization‚Äù (Noah Smith, 2011).\nTo visualize this, we can use the nltk library. Say we have a list of words: organize, organizes, organizing. We know that they all stem from organize in some degree or another. Let‚Äôs compare the two together. We can do this with the following code:\n import nltk\n from nltk import stem\n wl = ['organize', 'organizes', 'organizing']\n [wnl.lemmatize(word) for word in wl]\n [porter.stem(word) for word in wl]\nThe output of this code is radically different:\nLemmatization: [‚Äòorganize‚Äô, ‚Äòorganizes‚Äô, ‚Äòorganizing‚Äô]\nStemming: [‚Äòorgan‚Äô, ‚Äòorgan‚Äô, ‚Äòorgan‚Äô]\nLemmatization will allow us to have more context within the words we tokenize and is almost always better than using Stemming, especially with languages that have more compex morphologies.\nThe last topic I got through in the lesson was Spacy. Spacy is a modern and fast NLP library, which Fast.AI uses. Spacy always uses Lemmatization for it‚Äôs tokenizations, as it is considered better. This is the first example of an opinionated choice in a library.\nI did not get into Foundational course yet this week, so I won‚Äôt have an update on that until the following post, but thank you all for reading! The goals for the upcoming week for each course are:\n\nNLP: Finish topic modeling\nMatrix Calculus: Introduction to vector calculus and partial derivatives\nLinear Algebra: Finish the ‚ÄòWhy are we here‚Äô notebook and the NMF SVD Notebooks\nFoundations: Get through notebooks 01 and 02\n\nSee you next week!\nZach Mueller"
  },
  {
    "objectID": "blog/TabularNumpy.html",
    "href": "blog/TabularNumpy.html",
    "title": "Speeding up fastai Tabular with NumPy",
    "section": "",
    "text": "Speeding up fastai tabular training by 40%"
  },
  {
    "objectID": "blog/TabularNumpy.html#what-is-fastai-tabular-a-tldr",
    "href": "blog/TabularNumpy.html#what-is-fastai-tabular-a-tldr",
    "title": "Speeding up fastai Tabular with NumPy",
    "section": "What is fastai Tabular? A TL;DR",
    "text": "What is fastai Tabular? A TL;DR\nWhen working with tabular data, fastai has introduced a powerful tool to help with prerocessing your data: TabularPandas. It‚Äôs super helpful and useful as you can have everything in one place, encode and decode all of your tables at once, and the memory usage on top of your Pandas dataframe can be very minimal. Let‚Äôs look at an example of it.\nFirst let‚Äôs import the tabular module:\nfrom fastai2.tabular.all import *\nFor our particular tests today, we‚Äôll be using the ADULT_SAMPLE dataset, where we need to identify if a particular individual makes above or below $50,000. Let‚Äôs grab the data:\npath = untar_data(URLs.ADULT_SAMPLE)\nAnd now we can open it in Pandas:\ndf = pd.read_csv(path/'adult.csv')\ndf.head()\nNow that we have our DataFrame, let‚Äôs fit it into a TabularPandas object for preprocessing. To do so, we need to decalre the following:\n\nprocs (pre-processing our data, such as normalization and converting categorical values to numbers)\ncat_names (categorical variables)\ncont_names (continuous variables)\ny_names (our y columns)\n\nFor our case, these look like so:\ncat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\ncont_names = ['age', 'fnlwgt', 'education-num']\nprocs = [Categorify, FillMissing, Normalize]\ny_names = 'salary'\nWe‚Äôll also need to tell TabularPandas how we want to split our data. We‚Äôll use a random 20% subsample:\nsplits = RandomSplitter()(range_of(df))\nNow let‚Äôs make a TabularPandas!\nto = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,\n                   y_names=y_names, splits=splits)\nNow all of our data is pre-processed here and we can grab all of the raw values if we wanted to say use it with XGBoost like so:\nto.train.xs.iloc[:3]\nAndi it‚Äôs fully encoded! Now that we‚Äôre a bit familiar with TabularPandas, let‚Äôs do some speed tests!"
  },
  {
    "objectID": "blog/TabularNumpy.html#the-baseline",
    "href": "blog/TabularNumpy.html#the-baseline",
    "title": "Speeding up fastai Tabular with NumPy",
    "section": "The Baseline",
    "text": "The Baseline\nFor our tests, we‚Äôll run 4 different tests: 1. One batch of the training data 2. Iterating over the entire training dataset 3. Iterating over the entire validation set 4. Fitting for 10 epochs (GPU only)\nAnd for each of these we will compare the times on the CPU and the GPU.\n\nCPU:\nFirst let‚Äôs grab the first batch. The reason this is important is each time we iterate over the training DataLoader, we actually shuffle our data, which can add some time:\ndls = to.dataloaders(bs=128, device='cpu')\nTo test our times, we‚Äôll use %%timeit. It measures the execution time of a Python function for a certain amount of loops, and reports back the fastest one. For iterating over the entire DataLoader we‚Äôll look at the time per batch as well.\nFirst, a batch from training:\n%%timeit\n_ = next(iter(dls.train))\nNow the validation:\n%%timeit\n_ = next(iter(dls.valid))\nAlright, so first we can see that our shuffling function is adding almost 15 milliseconds on our time, something we can improve on! Let‚Äôs then go through the entire DataLoader:\n%%timeit\nfor _ in dls.train:\n    _\nNow let‚Äôs get an average time per batch:\nprint(661/len(dls.train))\nAbout 3.25 milliseconds per batch on the training dataset, let‚Äôs look at the validation:\n%%timeit\nfor _ in dls.valid:\n    _\nprint(159/len(dls.valid))\nAnd about 3.11 milliseconds per batch on the validation, so we can see that it‚Äôs about the same after shuffling. Now let‚Äôs compare some GPU times:\n\n\nGPU\ndls = to.dataloaders(bs=128, device='cuda')\n%%timeit\n_ = next(iter(dls.train))\n%%timeit\n_ = next(iter(dls.valid))\nSo first, grabbing just one batch we can see it added about a half a millisecond on the training and .2 milliseconds on the validation, so we‚Äôre not utilizing the GPU for this process much (which makes sense, TabularPandas is CPU bound). And now let‚Äôs iterate:\n%%timeit\nfor _ in dls.train:\n    _\nprint(693/len(dls.train))\n%%timeit\nfor _ in dls.valid:\n    _\nprint(163/len(dls.valid))\nAnd here we can see a little bit more being added here as well. Now that we have those baselines, let‚Äôs fit for ten epochs real quick:\nlearn = tabular_learner(dls, layers=[200,100], metrics=accuracy)\n%%time\nlearn.fit(10, 1e-2)\nAfter fitting, we got about 22.9 seconds in total and ~2.29 seconds per epoch! Now that we have our baselines, let‚Äôs try to speed that up!"
  },
  {
    "objectID": "blog/TabularNumpy.html#bringing-in-numpy",
    "href": "blog/TabularNumpy.html#bringing-in-numpy",
    "title": "Speeding up fastai Tabular with NumPy",
    "section": "Bringing in NumPy",
    "text": "Bringing in NumPy\n\nThe Dataset\nWith speeding everything up, I wanted to keep TabularPandas as it is, as it‚Äôs a great way to pre-process your data! So instead we‚Äôll create a new Dataset class where we will convert our TabularPandas object into a NumPy array. Why is that important? NumPy is a super-fast library that has been hyper-optimized by using as much C code as it possibly can which is leagues faster than Python. Let‚Äôs build our Dataset!\nWe‚Äôll want it to maintain the cats, conts, and ys from our TabularPandas object seperate. We can call to_numpy() on all of them because they are simply stored as a DataFrame! Finally, to deal with categorical versus continuous variables, we‚Äôll assign our cats as np.long and our conts as np.float32 (we also have our ys as np.int8, but this is because we‚Äôre doing classification):\nclass TabDataset():\n    \"A `NumPy` dataset from a `TabularPandas` object\"\n    def __init__(self, to):\n        self.cats = to.cats.to_numpy().astype(np.long)\n        self.conts = to.conts.to_numpy().astype(np.float32)\n        self.ys = to.ys.to_numpy()\nGreat! Now we need a few more bits for everything to work! For our Dataset to function, we need to be able to gather the values from it each time we call from it. We use the __getitem__ function to do so! For our particular problem, we need it to return some cats, conts, and our ys. And to save on more time we‚Äôll return a whole batch of values:\nclass TabDataset():\n    \"A `NumPy` dataset from a `TabularPandas` object\"\n    def __init__(self, to):\n        self.cats = to.cats.to_numpy().astype(np.long)\n        self.conts = to.conts.to_numpy().astype(np.float32)\n        self.ys = to.ys.to_numpy()\n\n    def __getitem__(self, idx):\n        idx = idx[0]\n        return self.cats[idx:idx+self.bs], self.conts[idx:idx+self.bs], self.ys[idx:idx+self.bs]\nYou‚Äôll notice we don‚Äôt explicitly pass in a batch size, so where is that coming from? This is added when we build our DataLoader, as we‚Äôll see later. Let‚Äôs finish up our Dataset class by adding in an option to get the length of the dataset (we‚Äôll do the length of our categorical table in this case).\nclass TabDataset():\n    \"A `NumPy` dataset from a `TabularPandas` object\"\n    def __init__(self, to):\n        self.cats = to.cats.to_numpy().astype(np.long)\n        self.conts = to.conts.to_numpy().astype(np.float32)\n        self.ys = to.ys.to_numpy()\n\n    def __getitem__(self, idx):\n        idx = idx[0]\n        return self.cats[idx:idx+self.bs], self.conts[idx:idx+self.bs], self.ys[idx:idx+self.bs]\n\n    def __len__(self): return len(self.cats)\nAnd now we can make some Datasets!\ntrain_ds = TabDataset(to.train)\nvalid_ds = TabDataset(to.valid)\nWe can look at some data real quick if we want to as well! First we need to assign a batch size:\ntrain_ds.bs = 3\nAnd now let‚Äôs look at some data:\ntrain_ds[[3]]\nWe can see that we output what could be considered a batch of data! The only thing missing is to make it into a tensor! Fantastic! Now let‚Äôs build the DataLoader, as there‚Äôs some pieces in it that we need, so simply having this Dataset won‚Äôt be enough\n\n\nThe DataLoader\nNow to build our DataLoader, we‚Äôre going to want to modify 4 particular functions:\n\ncreate_item\ncreate_batch\nget_idxs\nshuffle_ds\n\nEach of these play a particular role. First let‚Äôs look at our template:\nclass TabDataLoader(DataLoader):\n    def __init__(self, dataset, bs=1, num_workers=0, device='cuda', shuffle=False, **kwargs):\n        \"A `DataLoader` based on a `TabDataset`\"\n        super().__init__(dataset, bs=bs, num_workers=num_workers, shuffle=shuffle, \n                         device=device, drop_last=shuffle, **kwargs)\n        self.dataset.bs=bs\nAs you can see, our __init__ will build a DataLoader, and we keep track of our Dataset and set the Datasets batch size here as well\ndl = TabDataLoader(train_ds, bs=3)\ndl.dataset.bs\ndl.dataset[[0]]\nAnd we can see that we grab everything as normal in the Dataset! Great! Now let‚Äôs work on create_item and create_batch. create_item is very simple as we already do so when we make our call to the dataset, so we just pass it on. create_batch is also very simplistic. We‚Äôll take some index‚Äôs from our Dataset and convert them all to Tensors!\nclass TabDataLoader(DataLoader):\n    def __init__(self, dataset, bs=1, num_workers=0, device='cuda', shuffle=False, **kwargs):\n        \"A `DataLoader` based on a `TabDataset`\"\n        super().__init__(dataset, bs=bs, num_workers=num_workers, shuffle=shuffle, \n                         device=device, drop_last=shuffle, **kwargs)\n        self.dataset.bs=bs\n    \n    def create_item(self, s): return s\n\n    def create_batch(self, b):\n        cat, cont, y = self.dataset[b]\n        return tensor(cat).to(self.device), tensor(cont).to(self.device), tensor(y).to(self.device)\nNow we‚Äôre almost done. The last two pieces missing is get_idxs and shuffle_fn. These are needed as after each epoch we actually shuffle the dataset and we need to get a list of index‚Äôs for our DataLoader to use! To save on time (as we‚Äôre using array indexing), we can shuffle the interior dataset instead! A major benefit is slicing (consecutive idxs) instead of indexing (non-consecutive idxs). Let‚Äôs look at what that looks like:\nclass TabDataLoader(DataLoader):\n    def __init__(self, dataset, bs=1, num_workers=0, device='cuda', shuffle=False, **kwargs):\n        \"A `DataLoader` based on a `TabDataset`\"\n        super().__init__(dataset, bs=bs, num_workers=num_workers, shuffle=shuffle, \n                         device=device, drop_last=shuffle, **kwargs)\n        self.dataset.bs=bs\n    \n    def create_item(self, s): return s\n\n    def create_batch(self, b):\n        \"Create a batch of data\"\n        cat, cont, y = self.dataset[b]\n        return tensor(cat).to(self.device), tensor(cont).to(self.device), tensor(y).to(self.device)\n\n    def get_idxs(self):\n        \"Get index's to select\"\n        idxs = Inf.count if self.indexed else Inf.nones\n        if self.n is not None: idxs = list(range(len(self.dataset)))\n        return idxs\n\n    def shuffle_fn(self):\n        \"Shuffle the interior dataset\"\n        rng = np.random.permutation(len(self.dataset))\n        self.dataset.cats = self.dataset.cats[rng]\n        self.dataset.conts = self.dataset.conts[rng]\n        self.dataset.ys = self.dataset.ys[rng]\nAnd now we have all the pieces we need to build a DataLoader with NumPy! We‚Äôll examine it‚Äôs speed now and then we‚Äôll build some convience functions later. First let‚Äôs build the Datasets:\ntrain_ds = TabDataset(to.train)\nvalid_ds = TabDataset(to.valid)\nAnd then the DataLoader:\ntrain_dl = TabDataLoader(train_ds, device='cpu', shuffle=True, bs=128)\nvalid_dl = TabDataLoader(valid_ds, device='cpu', bs=128)\nAnd now let‚Äôs grab some CPU timings similar to what we did before:\n%%timeit\n_ = next(iter(train_dl))\n%%timeit\n_ = next(iter(valid_dl))\nRight away we can see that we are leagues faster than the previous version. Shuffling only added ~370 microseconds, which means we used 4% of the time! Now let‚Äôs iterate over the entire DataLoader:\n%%timeit\nfor _ in train_dl:\n    _\nprint(31.8/len(train_dl))\n%%timeit\nfor _ in valid_dl:\n    _\nprint(8.07/len(valid_dl))\nAnd as we can see, each individual batch of data is about 0.158 milliseconds! Yet again, about 6% of time time, quite a decrease! So we have sucessfully decreased the time! Let‚Äôs look at the GPU now:\ntrain_dl = TabDataLoader(train_ds, device='cuda', shuffle=True, bs=128)\nvalid_dl = TabDataLoader(valid_ds, device='cuda', bs=128)\n%%timeit\n_ = next(iter(train_dl))\n%%timeit\n_ = next(iter(valid_dl))\n%%timeit\nfor _ in train_dl:\n    _\nprint(51.5/len(train_dl))\n%%timeit\nfor _ in valid_dl:\n    _\nprint(12.8/len(valid_dl))\nWhich as we can see, it adds a little bit of time from converting the tensors over to cuda. You could save a little bit more by converting first, but as this should be seperate from the dataset I decided to just keep it here. Now that we have all the steps, finally we can take a look at training! First let‚Äôs build a quick helper function to make DataLoaders similar to what fastai‚Äôs tabular_learner would be expecting:\nclass TabDataLoaders(DataLoaders):\n    def __init__(self, to, bs=64, val_bs=None, shuffle_train=True, device='cpu', **kwargs):\n        train_ds = TabDataset(to.train)\n        valid_ds = TabDataset(to.valid)\n        val_bs = bs if val_bs is None else val_bs\n        train = TabDataLoader(train_ds, bs=bs, shuffle=shuffle_train, device=device, **kwargs)\n        valid = TabDataLoader(valid_ds, bs=val_bs, shuffle=False, device=device, **kwargs)\n        super().__init__(train, valid, device=device, **kwargs)\ndls = TabDataLoaders(to, bs=128, device='cuda')\nAnd now we can build our model and train! We need to build our own TabularModel here, so we‚Äôll need to grab the size of our embeddings and build a Learner. For simplicity we‚Äôll still use TabularPandas to get those sizes:\nemb_szs = get_emb_sz(to)\nnet = TabularModel(emb_szs, n_cont=3, out_sz=2, layers=[200,100]).cuda()\nlearn = Learner(dls, net, metrics=accuracy, loss_func=CrossEntropyLossFlat())\nAnd now let‚Äôs train!\n%%time\nlearn.fit(10, 1e-2)\nAs you can see, we cut the speed down 60%! So we saw a tremendous speed up! Let‚Äôs quickly revisit all of the times and results in a pretty table."
  },
  {
    "objectID": "blog/TabularNumpy.html#results",
    "href": "blog/TabularNumpy.html#results",
    "title": "Speeding up fastai Tabular with NumPy",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\n\n\n\n\n\n\nCPU?\nFirst Batch\nPer Batch\nPer Epoch\nTen Epochs\n\n\n\n\nfastai2\nYes\n18.3ms (train) 3.37ms (valid)\n3.25ms (train) 3.11ms (valid)\n\n\n\n\n\nNo\n18.8ms (train) 3.49ms (valid)\n3.41ms (train) 3.19ms (valid)\n2.29s\n22.9s\n\n\nNumPy\nYes\n0.669ms (train) 0.3ms (valid\n0.15ms (train) 0.15ms (valid)\n\n\n\n\n\nNo\n0.835ms (train) 0.451ms (valid)\n0.25ms (train) 0.25ms (valid)\n1.38s\n13.8s\n\n\n\nSo in summary, we first sped up the time to grab a single batch of data by converting everything from Pandas to NumPy. Afterwards we made a custom DataLoader that could handle these NumPy arrays and induce the speedup we saw! I hope this article helps you better understand how the interior DataLoader can be integrated in with NumPy, and that it helps you speed up your tabular training!\n\nSmall note: show_batch() etc will not work with this particular code base, this is simply a proof of concept"
  },
  {
    "objectID": "blog/Fastai-Curriculum.html",
    "href": "blog/Fastai-Curriculum.html",
    "title": "Summer Smackdown - An Introduction",
    "section": "",
    "text": "Fast.AI offers a number of courses nowadays for anyone to take that are invaluable to Data Science practitioners including:\n\nPractical Deep Learning for Coders\nDeep Learning: From the Foundations\nComputational Linear Algebra\nMatrix Calculus for Deep Learning\nNatural Language Processing\n\nAnd as my University offers nothing close to that, and after taking the Practical Deep Learning course, I want to do my best to try them all and learn as much as I can. To do this, I have tasked myself with a challenge:\nThe Fall semester begins in eight weeks. As such, in eight weeks time I want to have completed the Computational Linear Algebra, the Natural Language Processing, and Matrix Calculus for Deep Learning courses, and be well into most of the way through the Deep Learning: From the Foundations course. I will be calling this the Summer Smackdown\n\nNote: not affiliated with the WWE\n\nTo help keep me up to pace with this, I will be writing a series of weekly blogs here describing what I learned that week from each course, how I went and did something with it, and what challenges I may be facing.\nAt the end of all of the courses I want to have some form of a Capstone project for either each course, or for all of them combined. I have not quite figured out what that looks like, but I have a few ideas to play around with.\nI invite anyone who wants to tag along with me these next few months to join me. All of my notes and course materials are available here: Fast.AI Summer Curriculum\n** Please do note, the NLP course is not officially released yet, so while I will be going through it initially right now, I do plan to revisit it when the videos are out."
  },
  {
    "objectID": "blog/adhd.html",
    "href": "blog/adhd.html",
    "title": "Neuro-Diversity in Tech, The Slippery Slope to Burnout",
    "section": "",
    "text": "I‚Äôve been struggling for a few weeks trying to figure out exactly how I want this post written. Many individuals have either come to me privately or publicly saying I need to write about this, as lack of resources in our field on what ADHD in tech looks like, both the good and bad, is very limited.\nWhich I don‚Äôt disagree. When I first was investigating all I found were horror stories from Reddit saying how ‚ÄúADHD had ruined their life and field in tech.‚Äù\nThis is of course terrifying and disheartening to hear. So I‚Äôll state this now:\nWhen you know how to utilize the tools you were given, you will struggle some but you will be able to succeed. And you can survive in tech.\nHowever, if you don‚Äôt even know you have a tool in the toolbox, how do know to use it?\n\n\n\nTake all of this article with a grain of salt, as it is put through my own lens. Every single one of us is different, I am simply putting my story out as a datapoint for others. Your ADHD could show entirely different behaviors than mine, or none of what I say may even apply to you! And that is ok. I do not represent the entire community, and I would never say I would.\nBut, there is also value in stating what I went through and how I found and avoided future pitfalls.\nAlright, let‚Äôs discuss warning signs.\nFor me, this culminated in three different ways:\n\nInsane hyperfocus\nConstant burnout\n\n\n\n\nSo. Let‚Äôs talk about that. (Enter Rhett and Link).\nI have ‚Äúmixed‚Äù type ADHD. Which means I have both the energetic tendencies, and also on the other end of the threshold the hyperfocus tendencies.\nOh, so I‚Äôm just one of those people that plays games for 12+ hours a day right? That doesn‚Äôt mean anything.\nYou are right! There isn‚Äôt much of a link to extended video game time and ADHD, because video games are a form of stimulant.\nInstead, this culminates to working on one specific thing, without breaks (bar sleep) for a day if not days on end. This was fantastic in school, because I would do our programming projects 1-2 days before the due date, and knock out the entire thing in one go.\nBut, doing so leads to an explosion at the end: Burnout. I cannot overstate how easy it was for me to burn out doing things I enjoyed. Because it immediately took all of my focus for the weekend, the week, etc.\nBut then, if I didn‚Äôt finish it by the end of my focus session, I‚Äôd give up. Either due to fatigue, or (as I found out later) because some ADHD folks have problems successfully finishing long-term projects all the way.\nThey thrive on short effort (but considerable) and high return/reward.\nLet me provide an example to this.\nIf you‚Äôre familiar with me, you know that I wrote https://www.walkwithfastai.com. A mini course of Jeremy Howard‚Äôs wonderful fastai course v3, geared towards fastai version 2 and was the only course in existence covering v2 of fastai until the book and previous iteration of the current course.\nI had quite a bit planned for this course. A section on Computer Vision, Tabular, and NLP. I knew the bare basics of NLP, but I knew I had to keep it in there to be ‚Äúcomplete.‚Äù\nSo I burned and burned and burned through the notebooks. I was smart and got all of the easy ones out of the way within the first month or two (~14-16 notebooks or so). Things were going well. Some bits I was struggling with, but there was only 3 more notebooks to go, the NLP section.\nI wrote that first notebook and knew immediately it just wasn‚Äôt going to happen. I did not know enough about the topic, and that lesson was happening in 2 weeks after I made it to that point. And then, the burnout finally hit as well. Months-long prolonged burnout.\nSo, it didn‚Äôt. It vanished.\nI consider Walk with fastai both one of my greatest achievements but also one of my biggest failures. Because it was a considerable project I could not see through to the end.\nBut Zach! You made all those other wonderful lessons!\nYeah, but my brain has a hard time acknowledging the good there.\nSince then, I‚Äôve gotten much better at understanding what is the tipping point of my burnout, how I can balance it well, and worked with some wonderful folks to help me get onto my feet.\nBut I would never put myself through that again. In that specific way.\nI‚Äôm not some magical being. I‚Äôm human. If I could just hyperfocus all day long, who knows what I‚Äôd be able to solve! But I can‚Äôt, and it has real physical consequences when left unregulated.\nSo, if you do have this part of ADHD, remind yourself that you need to take breaks. That working for hours on end uninterrupted for too long can hurt you and will. Oh and, take your PTO. You need to rest. Communicate with your manager when this happens. They can‚Äôt ‚Äúmagically know‚Äù that you‚Äôre close or at a tipping point, because (at least for me), you don‚Äôt show signs until it‚Äôs far too late.\n\n\n\nOkay, so, that painted a very bleak picture. But I wanted to make sure you knew it wasn‚Äôt all rainbows and unicorns here.\nSo, what can I do then?\nFirst: be as top as you can about your mental state. You‚Äôve been living with it for all your life. You know how you work. This is me telling you that if you‚Äôre constantly burned out, you might be genuinely working too hard. Talk with your manager and see if something can be negotiated.\nSecond: Work. Out. I mean it. Studies have shown that working out increases the chemical in our brain that people with ADHD lack. Heavy lifting is even better. I work out every single day, and aim to be a power lifter how I can. There is genuine clarity for me for an hour or so after a heavy lift, where I feel ‚Äúnormal‚Äù. It‚Äôs a 2:1 benefit, you get to stay healthy and your brain actually works right!\nThird: Be educated and go with an assumption of might, and treat it (not chemically) as if its truth. The only harm here is you tried something and it just might not work. And that‚Äôs okay. It‚Äôs a step forward not a step backwards.\nThe biggest resource for me starting out was Ologies with Alie Ward. She had two episodes on ADHD, with the first having Dr.¬†Russell Barkley. It opened my world to ‚Äúhey, you might not be crazy and really actually have this thing!‚Äù And on top of that, it was approachable where even if you don‚Äôt have ADHD, but you know someone who does, I‚Äôm begging you to listen to it. It‚Äôs a very real world of the potential (on both sides) of how individuals with ADHD function, what the studies have shown, and how to be successful in their situations.\n\n\n\nAlright, now we‚Äôve finally hit the ‚Äúhow to win‚Äù part of this. Which is not a sure-all-fire-way, it‚Äôs just the way that worked for me. On top of this, I‚Äôll also include bits from that podcast that they mentioned as well.\nPeople with ADHD do exceptionally well in short time/effort high reward systems. Tech is actually one of those places where this works well, especially in a healthy startup. So, work with your manager to give yourself short-term projects (I‚Äôve found this to be about 2 weeks), that take chunks out of a larger project. This helps you stay on track, you‚Äôre not overworked, and by the end you‚Äôve finished an amazing goal!\nNext, as I said above please make sure to eat well and stay active how you can. It‚Äôs okay to have your food delivered if you need to, because you‚Äôre afraid you might overwork through lunch and suddenly it‚Äôs 7pm and you haven‚Äôt eaten since 6am. (Sounds like I‚Äôve done this before, doesn‚Äôt it heh). Meal prep if you can, or find easy ways to just microwave something quickly for you to eat throughout the day.\nThird, try to actually get tested if you‚Äôre not. And if so, try and talk to a doctor. There are horror stories from folks that have done this, and I have my own. It took 3 tries over a 4 month period before I was finally diagnosed and could get tested. For individuals who give up easily, that‚Äôs not a fun time. But it is worth it so keep it up. If you choose to explore medication, genuinely be honest with your doctor while you‚Äôre at it, as they only serve to help you.\n\n\n\nWell, if you‚Äôve made it to the end of this thank you. You‚Äôve somehow survived my rant and I truly hope you got something worthwile out of it. At the bare minimum, seeing how one instance of ADHD in tech looks like.\nBelow I have some links to folks I follow on twitter who are either ADHD advocates or have ADHD in tech. They‚Äôve been immensely helpful to have on my day-to-day twitter feed, and have helped me through my journey. Hopefully they can help you through yours as well.\n\nPart 1: Attention-Deficit Neuropsychology (ADHD) with Russel Barkley\n@ADHD_Alien\n@kefimochi\n[@AdhdAngsty](https://twitter.com/AdhdAngsty"
  },
  {
    "objectID": "blog/adhd.html#introduction",
    "href": "blog/adhd.html#introduction",
    "title": "Neuro-Diversity in Tech, The Slippery Slope to Burnout",
    "section": "",
    "text": "I‚Äôve been struggling for a few weeks trying to figure out exactly how I want this post written. Many individuals have either come to me privately or publicly saying I need to write about this, as lack of resources in our field on what ADHD in tech looks like, both the good and bad, is very limited.\nWhich I don‚Äôt disagree. When I first was investigating all I found were horror stories from Reddit saying how ‚ÄúADHD had ruined their life and field in tech.‚Äù\nThis is of course terrifying and disheartening to hear. So I‚Äôll state this now:\nWhen you know how to utilize the tools you were given, you will struggle some but you will be able to succeed. And you can survive in tech.\nHowever, if you don‚Äôt even know you have a tool in the toolbox, how do know to use it?"
  },
  {
    "objectID": "blog/adhd.html#the-warning-signs-for-me",
    "href": "blog/adhd.html#the-warning-signs-for-me",
    "title": "Neuro-Diversity in Tech, The Slippery Slope to Burnout",
    "section": "",
    "text": "Take all of this article with a grain of salt, as it is put through my own lens. Every single one of us is different, I am simply putting my story out as a datapoint for others. Your ADHD could show entirely different behaviors than mine, or none of what I say may even apply to you! And that is ok. I do not represent the entire community, and I would never say I would.\nBut, there is also value in stating what I went through and how I found and avoided future pitfalls.\nAlright, let‚Äôs discuss warning signs.\nFor me, this culminated in three different ways:\n\nInsane hyperfocus\nConstant burnout"
  },
  {
    "objectID": "blog/adhd.html#hyperfocus-and-burnout",
    "href": "blog/adhd.html#hyperfocus-and-burnout",
    "title": "Neuro-Diversity in Tech, The Slippery Slope to Burnout",
    "section": "",
    "text": "So. Let‚Äôs talk about that. (Enter Rhett and Link).\nI have ‚Äúmixed‚Äù type ADHD. Which means I have both the energetic tendencies, and also on the other end of the threshold the hyperfocus tendencies.\nOh, so I‚Äôm just one of those people that plays games for 12+ hours a day right? That doesn‚Äôt mean anything.\nYou are right! There isn‚Äôt much of a link to extended video game time and ADHD, because video games are a form of stimulant.\nInstead, this culminates to working on one specific thing, without breaks (bar sleep) for a day if not days on end. This was fantastic in school, because I would do our programming projects 1-2 days before the due date, and knock out the entire thing in one go.\nBut, doing so leads to an explosion at the end: Burnout. I cannot overstate how easy it was for me to burn out doing things I enjoyed. Because it immediately took all of my focus for the weekend, the week, etc.\nBut then, if I didn‚Äôt finish it by the end of my focus session, I‚Äôd give up. Either due to fatigue, or (as I found out later) because some ADHD folks have problems successfully finishing long-term projects all the way.\nThey thrive on short effort (but considerable) and high return/reward.\nLet me provide an example to this.\nIf you‚Äôre familiar with me, you know that I wrote https://www.walkwithfastai.com. A mini course of Jeremy Howard‚Äôs wonderful fastai course v3, geared towards fastai version 2 and was the only course in existence covering v2 of fastai until the book and previous iteration of the current course.\nI had quite a bit planned for this course. A section on Computer Vision, Tabular, and NLP. I knew the bare basics of NLP, but I knew I had to keep it in there to be ‚Äúcomplete.‚Äù\nSo I burned and burned and burned through the notebooks. I was smart and got all of the easy ones out of the way within the first month or two (~14-16 notebooks or so). Things were going well. Some bits I was struggling with, but there was only 3 more notebooks to go, the NLP section.\nI wrote that first notebook and knew immediately it just wasn‚Äôt going to happen. I did not know enough about the topic, and that lesson was happening in 2 weeks after I made it to that point. And then, the burnout finally hit as well. Months-long prolonged burnout.\nSo, it didn‚Äôt. It vanished.\nI consider Walk with fastai both one of my greatest achievements but also one of my biggest failures. Because it was a considerable project I could not see through to the end.\nBut Zach! You made all those other wonderful lessons!\nYeah, but my brain has a hard time acknowledging the good there.\nSince then, I‚Äôve gotten much better at understanding what is the tipping point of my burnout, how I can balance it well, and worked with some wonderful folks to help me get onto my feet.\nBut I would never put myself through that again. In that specific way.\nI‚Äôm not some magical being. I‚Äôm human. If I could just hyperfocus all day long, who knows what I‚Äôd be able to solve! But I can‚Äôt, and it has real physical consequences when left unregulated.\nSo, if you do have this part of ADHD, remind yourself that you need to take breaks. That working for hours on end uninterrupted for too long can hurt you and will. Oh and, take your PTO. You need to rest. Communicate with your manager when this happens. They can‚Äôt ‚Äúmagically know‚Äù that you‚Äôre close or at a tipping point, because (at least for me), you don‚Äôt show signs until it‚Äôs far too late."
  },
  {
    "objectID": "blog/adhd.html#you-know-i-have-adhd.-i-glazed-through-all-of-that.-what-does-it-mean",
    "href": "blog/adhd.html#you-know-i-have-adhd.-i-glazed-through-all-of-that.-what-does-it-mean",
    "title": "Neuro-Diversity in Tech, The Slippery Slope to Burnout",
    "section": "",
    "text": "Okay, so, that painted a very bleak picture. But I wanted to make sure you knew it wasn‚Äôt all rainbows and unicorns here.\nSo, what can I do then?\nFirst: be as top as you can about your mental state. You‚Äôve been living with it for all your life. You know how you work. This is me telling you that if you‚Äôre constantly burned out, you might be genuinely working too hard. Talk with your manager and see if something can be negotiated.\nSecond: Work. Out. I mean it. Studies have shown that working out increases the chemical in our brain that people with ADHD lack. Heavy lifting is even better. I work out every single day, and aim to be a power lifter how I can. There is genuine clarity for me for an hour or so after a heavy lift, where I feel ‚Äúnormal‚Äù. It‚Äôs a 2:1 benefit, you get to stay healthy and your brain actually works right!\nThird: Be educated and go with an assumption of might, and treat it (not chemically) as if its truth. The only harm here is you tried something and it just might not work. And that‚Äôs okay. It‚Äôs a step forward not a step backwards.\nThe biggest resource for me starting out was Ologies with Alie Ward. She had two episodes on ADHD, with the first having Dr.¬†Russell Barkley. It opened my world to ‚Äúhey, you might not be crazy and really actually have this thing!‚Äù And on top of that, it was approachable where even if you don‚Äôt have ADHD, but you know someone who does, I‚Äôm begging you to listen to it. It‚Äôs a very real world of the potential (on both sides) of how individuals with ADHD function, what the studies have shown, and how to be successful in their situations."
  },
  {
    "objectID": "blog/adhd.html#how-to-be-successful",
    "href": "blog/adhd.html#how-to-be-successful",
    "title": "Neuro-Diversity in Tech, The Slippery Slope to Burnout",
    "section": "",
    "text": "Alright, now we‚Äôve finally hit the ‚Äúhow to win‚Äù part of this. Which is not a sure-all-fire-way, it‚Äôs just the way that worked for me. On top of this, I‚Äôll also include bits from that podcast that they mentioned as well.\nPeople with ADHD do exceptionally well in short time/effort high reward systems. Tech is actually one of those places where this works well, especially in a healthy startup. So, work with your manager to give yourself short-term projects (I‚Äôve found this to be about 2 weeks), that take chunks out of a larger project. This helps you stay on track, you‚Äôre not overworked, and by the end you‚Äôve finished an amazing goal!\nNext, as I said above please make sure to eat well and stay active how you can. It‚Äôs okay to have your food delivered if you need to, because you‚Äôre afraid you might overwork through lunch and suddenly it‚Äôs 7pm and you haven‚Äôt eaten since 6am. (Sounds like I‚Äôve done this before, doesn‚Äôt it heh). Meal prep if you can, or find easy ways to just microwave something quickly for you to eat throughout the day.\nThird, try to actually get tested if you‚Äôre not. And if so, try and talk to a doctor. There are horror stories from folks that have done this, and I have my own. It took 3 tries over a 4 month period before I was finally diagnosed and could get tested. For individuals who give up easily, that‚Äôs not a fun time. But it is worth it so keep it up. If you choose to explore medication, genuinely be honest with your doctor while you‚Äôre at it, as they only serve to help you."
  },
  {
    "objectID": "blog/adhd.html#conclusion",
    "href": "blog/adhd.html#conclusion",
    "title": "Neuro-Diversity in Tech, The Slippery Slope to Burnout",
    "section": "",
    "text": "Well, if you‚Äôve made it to the end of this thank you. You‚Äôve somehow survived my rant and I truly hope you got something worthwile out of it. At the bare minimum, seeing how one instance of ADHD in tech looks like.\nBelow I have some links to folks I follow on twitter who are either ADHD advocates or have ADHD in tech. They‚Äôve been immensely helpful to have on my day-to-day twitter feed, and have helped me through my journey. Hopefully they can help you through yours as well.\n\nPart 1: Attention-Deficit Neuropsychology (ADHD) with Russel Barkley\n@ADHD_Alien\n@kefimochi\n[@AdhdAngsty](https://twitter.com/AdhdAngsty"
  },
  {
    "objectID": "blog/Pytorchtofastai.html",
    "href": "blog/Pytorchtofastai.html",
    "title": "Pytorch to fastai, Bridging the Gap",
    "section": "",
    "text": "Understanding how to bring Pytorch code into the fastai space with minimal headache"
  },
  {
    "objectID": "blog/Pytorchtofastai.html#addressing-the-elephant-in-the-room",
    "href": "blog/Pytorchtofastai.html#addressing-the-elephant-in-the-room",
    "title": "Pytorch to fastai, Bridging the Gap",
    "section": "Addressing the Elephant in the Room",
    "text": "Addressing the Elephant in the Room\nI recently posted a tweet asking about what people struggle with the most in fastai, and the resounding answer was how to integrate minimally with Pytorch. An impression seems to have been made that to use fastai you must use the complete fastai API only, and nothing else.\nLet‚Äôs clear up that misconception now:\n\nImportant: fastai at its core is a training loop, designed to be framework agnostic. You can use any flavor of Pytorch you want, and only use fastai to quickly and effictively train a model with state-of-the-art practices"
  },
  {
    "objectID": "blog/Pytorchtofastai.html#the-plan",
    "href": "blog/Pytorchtofastai.html#the-plan",
    "title": "Pytorch to fastai, Bridging the Gap",
    "section": "The Plan",
    "text": "The Plan\nNow that the misconceptions have been addressed, let‚Äôs walk through just how that is going to happen. We‚Äôre going to follow the official Pytorch CIFAR10 tutorial and show what needs to minimally happen in the fastai framework to take full advantage of the Learner. This will include:\n\nThe Dataset\nThe DataLoaders\nThe model\nThe optimizer"
  },
  {
    "objectID": "blog/Pytorchtofastai.html#the-dataset-and-dataloaders",
    "href": "blog/Pytorchtofastai.html#the-dataset-and-dataloaders",
    "title": "Pytorch to fastai, Bridging the Gap",
    "section": "The Dataset and DataLoaders",
    "text": "The Dataset and DataLoaders\nFollowing from the tutorial, we‚Äôre going to load in the dataset using only torchvision. First we‚Äôll grab our imports:\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nNext we‚Äôre going to definine some minimal transforms:\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\nBefore downloading our train and test sets:\n\nNote: I‚Äôm using naming conventions similar to how fastai names things, so you can see how these can relate to each other\n\ndset_train = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ndset_test = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\nNext we‚Äôll make our Dataloaders:\ntrainloader = torch.utils.data.DataLoader(dset_train, batch_size=4,\n                                          shuffle=True, num_workers=2)\ntestloader = torch.utils.data.DataLoader(dset_test, batch_size=4,\n                                         shuffle=False, num_workers=2)\nAnd that‚Äôs as far as we‚Äôll go from there for now, let‚Äôs move onto the model next"
  },
  {
    "objectID": "blog/Pytorchtofastai.html#the-model",
    "href": "blog/Pytorchtofastai.html#the-model",
    "title": "Pytorch to fastai, Bridging the Gap",
    "section": "The Model",
    "text": "The Model\nWe‚Äôll bring in the architecture from the tutorial and use it here:\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\nAnd finally we‚Äôll make an instance of it:\nnet = Net()"
  },
  {
    "objectID": "blog/Pytorchtofastai.html#loss-function-and-optimizer",
    "href": "blog/Pytorchtofastai.html#loss-function-and-optimizer",
    "title": "Pytorch to fastai, Bridging the Gap",
    "section": "Loss Function and Optimizer",
    "text": "Loss Function and Optimizer\nNext we‚Äôll bring in their loss function and optimizer.\nThe loss function is simple enough:\ncriterion = nn.CrossEntropyLoss()\nHowever the optimizer requires a little bit of fastai magic, specifically in the form of an OptimWrapper. Our optimizer function should be defined as below:\nfrom fastai.optimizer import OptimWrapper\nfrom torch import optim\ndef opt_func(params, **kwargs): return OptimWrapper(optim.SGD(params, lr=0.001))"
  },
  {
    "objectID": "blog/Pytorchtofastai.html#training",
    "href": "blog/Pytorchtofastai.html#training",
    "title": "Pytorch to fastai, Bridging the Gap",
    "section": "Training",
    "text": "Training\nNow we have everything needed to train a model, so now let‚Äôs bring in fastai‚Äôs training loop, also known as the Learner.\nfastai‚Äôs Learner expects DataLoaders to be used, rather than simply one DataLoader, so let‚Äôs make that:\n\nNote: fastai also expects a validation DataLoader to be present, so we‚Äôll be tying the testloader in here\n\nfrom fastai.data.core import DataLoaders\ndls = DataLoaders(trainloader, testloader)\nFinally we‚Äôre going to wrap it all up in a Learner. As mentioned before, the Learner is the glue that merges everything together and enables users to utilize Leslie Smith‚Äôs One-Cycle Policy, the learning rate finder, and other fastai training goodies.\nLet‚Äôs make it by passing in our dls, the model, the optimizer, and the loss function:\nfrom fastai.learner import Learner\nTo get fastai‚Äôs fancy-looking progress bar, we need to import the ProgressCallback:\nfrom fastai.callback.progress import ProgressCallback\nWe also need to pass in the CudaCallback so our batches can be pushed to the GPU (fastai‚Äôs DataLoaders can do this automatically)\nfrom fastai.callback.data import CudaCallback\nlearn = Learner(dls, net, loss_func=criterion, opt_func=opt_func, cbs=[CudaCallback])\nFinally, let‚Äôs do some minimal training.\nNow we have everything needed to do a basic fit: &gt; Note: Since we already passed in a learning rate to Learner we don‚Äôt need to pass one in here\nlearn.fit(2)"
  },
  {
    "objectID": "blog/Pytorchtofastai.html#whats-next",
    "href": "blog/Pytorchtofastai.html#whats-next",
    "title": "Pytorch to fastai, Bridging the Gap",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nGreat, so now we‚Äôve trained our model, but what do we do with it? How do I get it out?\nYour model lives in learn.model, and we‚Äôve already seen that we passed in a regular Pytorch model earlier. Since we‚Äôre using fastai‚Äôs base Learner class, the model itself was untouched. As a result, it‚Äôs still a regular Pytorch model we can save away:\ntorch.save(learn.model.state_dict(), './cifar_net.pth')\nAnd that‚Äôs really it! As you can see, the minimalist you can absolutely get with using the fastai framework is:\n\nPytorch DataLoader\nPytorch model\nfastai Learner\nfastai Optimizer"
  },
  {
    "objectID": "blog/Pytorchtofastai.html#closing-remarks",
    "href": "blog/Pytorchtofastai.html#closing-remarks",
    "title": "Pytorch to fastai, Bridging the Gap",
    "section": "Closing Remarks",
    "text": "Closing Remarks\nI hope this has enlightned you on just how flexible the fastai framework can truly be for your training needs with the idealistic goal of simply getting a model out there.\nAs we‚Äôve removed most of the fastai magic, from here on out you should be utilizing standard Pytorch, as fastai specific functions like test_dl and predict will no longer be able to be used, as you didn‚Äôt use a fastai DataLoader.\nThank you for reading!"
  },
  {
    "objectID": "blog/PyTorchInference.html",
    "href": "blog/PyTorchInference.html",
    "title": "Inference in PyTorch, what do the wrappers mean? What‚Äôs best?",
    "section": "",
    "text": "A tour through PyTorch‚Äôs various context managers, torch script, and comparing performance\nBenchmarks were ran on a NVIDIA RTX 3070 Laptop GPU"
  },
  {
    "objectID": "blog/PyTorchInference.html#why-are-we-doing-this",
    "href": "blog/PyTorchInference.html#why-are-we-doing-this",
    "title": "Inference in PyTorch, what do the wrappers mean? What‚Äôs best?",
    "section": "Why are we doing this?",
    "text": "Why are we doing this?\nEarlier today, Francesco Pochetti had pinged on the fastai discord asking if when using inference for a torch model, whether no_grad was needed when the model has been scripted. This then got me curious on timings, which as we‚Äôve seen previously I love to do benchmarks like this!\nSo, I followed along PyTorch‚Äôs fantastic inference tutorial using TorchScript and went to work!\nWhat we‚Äôll explore in this article are the three ‚Äúmodes‚Äù for running a torch model: - Regular - no_grad - inference_mode\nHow each of them differ in what they do, and overall how the timings for each performed.\nFor the initial testing, we‚Äôll use a resnet18 on three different batch sizes (1, 16, and 64) to see the full effect of our efforts"
  },
  {
    "objectID": "blog/PyTorchInference.html#okay-so-what-are-these-modes",
    "href": "blog/PyTorchInference.html#okay-so-what-are-these-modes",
    "title": "Inference in PyTorch, what do the wrappers mean? What‚Äôs best?",
    "section": "Okay, so what are these modes?",
    "text": "Okay, so what are these modes?\nWhen we simply call model(torch.tensor([...])), a trace of the functions called is stored, along with the results of each layer, so that later gradients can be calculated if needed. This can become a time sink, and greatly increases the memory being used during inference. To speed things up, in pytorch you would typically wrap the call to model() with a no_grad() context manager. Under this, computations are never recorded in the model as the inference is performed, and looks like so:\nwith torch.no_grad():\n    model(torch.tensor([...]))\nFinally, we get to inference_mode which is the extreme version of no_grad. With this context manager, you should assume that you‚Äôll never need to have any recordings done in the backwards of the graph (like no_grad), and any tensors made during inference mode won‚Äôt be used for any computations touching autograd later. Hence the naming, inference_mode.\nIt looks like so:\nwith torch.inference_mode():\n    model(torch.tensor([...]))"
  },
  {
    "objectID": "blog/PyTorchInference.html#making-a-baseline",
    "href": "blog/PyTorchInference.html#making-a-baseline",
    "title": "Inference in PyTorch, what do the wrappers mean? What‚Äôs best?",
    "section": "Making a Baseline",
    "text": "Making a Baseline\nWhen doing experiments, it‚Äôs always important to make proper benchmarks! Since we‚Äôre comparing three modes, as well as two different models (torch scripted vs not), the the initial benchmark will be of our base model and torch scripted model without any context managers.\nFirst let‚Äôs make our models:\nimport torch\nfrom torchvision.models import resnet18\n\nbaseline_resnet = resnet18(pretrained=True).eval()\nscripted_resnet = torch.jit.script(baseline_resnet).eval()\nNext we‚Äôll setup our batches:\nbs = [1,16,64]\nbatches = [torch.rand(size, 3, 224, 224) for size in bs]\nAnd finally set them all to CUDA\nbaseline_resnet.cuda()\nscripted_resnet.cuda()\nfor i in range(len(batches)):\n    batches[i] = batches[i].cuda()\nWe‚Äôll also keep this interesting by keeping track of the allocated memory used by each. First we‚Äôll grab the current memory being used:\ndef get_mb(key):\n    \"A helpful function to get a readable size of an allocation\"\n    sz = torch.cuda.memory_stats()[key]\n    return sz // 1024 // 1024\nget_mb(\"allocated_bytes.all.current\")\nThis is how much our current memory usage is, and then we can track the peak memory usage once we start doing inference!\nLastly we‚Äôll make some dictionaries to provide quick access to what we need:\nimport contextlib\n\nmodes = {\n    \"none\":contextlib.suppress,\n    \"no_grad\":torch.no_grad,\n    \"inference_mode\":torch.inference_mode\n}\n\nmodels = {\n    \"baseline\":baseline_resnet,\n    \"scripted\":scripted_resnet\n}\n\nranges = {\n    1:1000,\n    16:100,\n    64:10\n}\nNow we just wrap up our configuration to get us some benchmarks! The latter half of this blog will be looking at the data:\nimport time\nfrom prettytable import PrettyTable\n\ndef benchmark_modes():\n    overall_reports = []\n    for mode in [\"none\", \"no_grad\", \"inference_mode\"]:\n        for batch in batches:\n            num_times = ranges[batch.shape[0]]\n            for model_type in [\"baseline\", \"scripted\"]:  \n                total_time = 0\n                total_memory = 0\n                for i in range(num_times):\n                    torch.cuda.reset_peak_memory_stats()\n                    initial_memory = get_mb(\"allocated_bytes.all.current\")\n                    start_time = time.perf_counter_ns()\n                    with modes[mode]():\n                        _ = models[model_type](batch)\n                    torch.cuda.synchronize()\n                    total_time += (time.perf_counter_ns() - start_time)/1e6\n                    peak_memory = get_mb(\"allocated_bytes.all.peak\")\n                    total_memory += peak_memory - initial_memory\n                overall_reports.append(\n                    {\n                        \"mode\":mode,\n                        \"batch_size\":batch.shape[0],\n                        \"model_type\":model_type,\n                        \"time\":round(total_time/num_times, 2),\n                        \"memory_used\":round(total_memory/num_times, 2),\n                    }\n                )\n    return overall_reports\nresults = benchmark_modes()"
  },
  {
    "objectID": "blog/PyTorchInference.html#examining-the-results",
    "href": "blog/PyTorchInference.html#examining-the-results",
    "title": "Inference in PyTorch, what do the wrappers mean? What‚Äôs best?",
    "section": "Examining the Results",
    "text": "Examining the Results\nLet‚Äôs dive deep into our results. First we‚Äôll look at everything, based on the context manager and the batch size:\n#hide_input\nimport copy\nprint(\"Experiment Results:\")\ntable = PrettyTable([\"Context Mode\", \"Batch Size\", \"Model Type\", \"Average Time Taken (ms)\", \"Average Total Memory Used (mb)\"])\nprev_bs = None\nprev_mode = None\nreports = copy.deepcopy(results)\nfor report in reports:\n    if prev_mode == report[\"mode\"]:\n        report[\"mode\"] = \"\"\n    else:\n        prev_mode = None\n    if prev_bs == report[\"batch_size\"]:\n        report[\"batch_size\"] = \"\"\n    else:\n        prev_bs = None\n    if prev_bs is None: prev_bs = report[\"batch_size\"]\n    if prev_mode is None: prev_mode = report[\"mode\"]\n    table.add_row(report.values())\nprint(table)\nWe can see that generally the scripted model tends to be slightly faster, independent of the context manager being used. It also uses the same memory footprint as the non-scripted model.\nBut what if we compare by each context manager themselves?\n#hide\ntable = PrettyTable([\"Model Type\", \"Batch Size\", \"Context Mode\", \"Average Time Taken (ms)\", \"Average Total Memory Used (mb)\"])\nprev_bs = None\nprev_mode = None\nreports = copy.deepcopy(results)\nreports = sorted(reports, key=lambda x: (x[\"model_type\"], x[\"batch_size\"]))\nfor report in reports:\n    table.add_row([report[key] for key in [\"model_type\", \"batch_size\",  \"mode\", \"time\", \"memory_used\"]])\n    \n#hide_input\nfor bs, (start,end) in [[1,(0,3)],[16,(3,6)],[64,(6,9)]]:\n    print(\n        table.get_string(\n            sort_key= lambda x: (x[\"Batch Size\"], x[\"Model Type\"]),\n            fields = list(table.field_names)[2:],\n            title=f\"Baseline Model, Batch Size {bs}\",\n            start=start,\n            end=end,\n            hrules=0\n        )\n    )\nFor our non-scripted model, we find that for a batch size of 1, inference mode does the best! We see an average speedup of 12%!\nHowever, as the batch size increases, this speedup becomes less and less radical, becoming only a fraction of a millisecond.\n\nNote: Notice the importance of having any context manager vs none in the total memory used. We reduced it from 1.3gb to 392mb being used, which is important!\n\nDoes this pattern continue for our scripted model?\n#hide_input\nfor bs, (start,end) in [[1,(9,12)],[16,(12,15)],[64,(15,18)]]:\n    print(\n        table.get_string(\n            sort_key= lambda x: (x[\"Batch Size\"], x[\"Model Type\"]),\n            fields = list(table.field_names)[2:],\n            title=f\"Scripted Model, Batch Size {bs}\",\n            start=start,\n            end=end,\n            hrules=0\n        )\n    )\nAgain, we do see this pattern occur even here! But, it looks like we have a time decrease, doesn‚Äôt it? Our scripted model actually is a decent chunk faster in some cases:\n#hide_input\nimport operator\nfor bs, (start,end) in [[1,(18,12)],[16,(12,6)],[64,(6,0)]]:\n    print(\n        table.get_string(\n            sort_key=operator.itemgetter(2,3),\n            fields = table.field_names[:1] + table.field_names[2:],\n            reversesort=True,\n            sortby=\"Batch Size\",\n            title=f\"Scripted Model, Batch Size {bs}\",\n            start=end,\n            end=start,\n            hrules=0\n        )\n    )\nWe see it specifically packing the punch when there was a batch size of 1. Otherwise no matter the context manager used, it always added in a few hundredth‚Äôs of a second of time.\nBut when does the loss of value happen? Let‚Äôs find out.\nWe‚Äôll run a fresh set of benchmarks, examining the batch size from 1 to 8:\nfor i in range(1,9):\n    ranges[i] = 100\n\nbatches = [torch.rand(i, 3, 224, 224) for i in range(1,9)]\nfor i in range(len(batches)):\n    batches[i] = batches[i].cuda()\nresults = benchmark_modes()\nNext, we‚Äôll plot a chart of batch_size x time (ms), looking at the distribution based on each kind:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(results)\ndf.columns = 'mode', 'Batch Size', 'model_type', 'Time (ms)', 'memory_used'\nfig, ax = plt.subplots()\ni = 0\ncolors = \"b\",\"g\",'r','c','m','y'\nfor (key, grp) in df.groupby(['mode']):\n    for (key2, grp2) in grp.groupby([\"model_type\"]):\n        ax = grp2.plot(ax=ax, kind='scatter', x='Batch Size', y='Time (ms)', label=f'{key2}, {key}', c=colors[i])\n        i += 1\n\nplt.legend(loc='best')\nplt.show()\nWe see that for a single item and two, it‚Äôs extremely important to use the right context manager, but as we increase our batch size it matters less and less until we hit 8.\nFor morbid curiosity, I decided to check how 8 to 16 might look, and here‚Äôs those results:\nfor i in range(8,16):\n    ranges[i] = 100\n\nbatches = [torch.rand(i, 3, 224, 224) for i in range(8,16)]\nfor i in range(len(batches)):\n    batches[i] = batches[i].cuda()\nresults = benchmark_modes()\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(results)\ndf.columns = 'mode', 'Batch Size', 'model_type', 'Time (ms)', 'memory_used'\nfig, ax = plt.subplots()\ni = 0\ncolors = \"b\",\"g\",'r','c','m','y'\nfor (key, grp) in df.groupby(['mode']):\n    for (key2, grp2) in grp.groupby([\"model_type\"]):\n        ax = grp2.plot(ax=ax, kind='scatter', x='Batch Size', y='Time (ms)', label=f'{key2}, {key}', c=colors[i])\n        i += 1\n\nplt.legend(loc='best')\nplt.show()\nWe find yet again that the distribution between the different modes is ~ &lt;.1 milliseconds."
  },
  {
    "objectID": "blog/PyTorchInference.html#finally-what-about-cpu",
    "href": "blog/PyTorchInference.html#finally-what-about-cpu",
    "title": "Inference in PyTorch, what do the wrappers mean? What‚Äôs best?",
    "section": "Finally, what about CPU?",
    "text": "Finally, what about CPU?\nHere‚Äôs our experiment again, performed on a CPU:\n#hide\nfor i in range(1,8):\n    ranges[i] = 10\n\nbatches = [torch.rand(i, 3, 224, 224) for i in range(1,8)]\nfor i in range(len(batches)):\n    batches[i] = batches[i].cpu()\nfor model in models.values():\n    model.cpu()\nresults = benchmark_modes()\n#hide_input\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(results)\ndf.columns = 'mode', 'Batch Size', 'model_type', 'Time (ms)', 'memory_used'\nfig, ax = plt.subplots()\ni = 0\ncolors = \"b\",\"g\",'r','c','m','y'\nfor (key, grp) in df.groupby(['mode']):\n    for (key2, grp2) in grp.groupby([\"model_type\"]):\n        ax = grp2.plot(ax=ax, kind='scatter', x='Batch Size', y='Time (ms)', label=f'{key2}, {key}', c=colors[i])\n        i += 1\n\nplt.legend(loc='best')\nplt.show()\nWe see a different story on CPU, where the difference of running no_grad or inference_mode matters much more, (which we‚Äôd expect), and inference_mode on occasion being slightly faster than no_grad."
  },
  {
    "objectID": "blog/PyTorchInference.html#what-to-take-away-from-this",
    "href": "blog/PyTorchInference.html#what-to-take-away-from-this",
    "title": "Inference in PyTorch, what do the wrappers mean? What‚Äôs best?",
    "section": "What to take away from this?",
    "text": "What to take away from this?\nThe takeaway from this experiment is that unless you‚Äôre dealing with smaller batch sizes (such as single image inference), the context manager you use and whether to use a scripted model could be negligible when it comes to the gains in performance on a GPU. The cost of evaluating the model exceeded the performance gain of not tracking the operations as the batch size increases.\nI‚Äôd think this also could matter more once a bigger model is involved, as there‚Äôs even more calculations and the time sink would increase."
  },
  {
    "objectID": "blog/nbquarto.html",
    "href": "blog/nbquarto.html",
    "title": "Introducing nbquarto: A framework for Processing Jupyter Notebooks",
    "section": "",
    "text": "Let‚Äôs take a few steps back in time, to when I (Zach), was using tools like nbdev out of fastai. I wasn‚Äôt using it for its intended purpose (writing libraries in Jupyter Notebooks), I wasn‚Äôt a big fan of that after a while. Instead, I was trying to hack it in any way I could to help further the course websites I was writing. To help further the documentation I was generating.\nThen, nbdev 2.0.0 comes out, showing this integration to a wonderful rendering and processing framework: Quarto. Immediatly I could see the potential with this framework for teaching purposes:\n\nQuick and easy ways for me to create interactible documentation?\nA way for me use my Jupyter Notebooks but still have special syntax it uses to render specific items?\n\nAnd most of all:\n\nnbdev 2.0.0 introduced the concept of a Processor class.\n\nWithout diving into too much detail yet, it was a moduler way for me to interact with the cells in a Jupyter Notebook and do whatever I wanted with them.\nI know, I know. Might have not quite sold you on it yet, and that‚Äôs alright.\nThis idea of post-processing these notebooks allowed me to create small ‚Äúshortcut‚Äù commands for quarto to remove a ton of boilerplate code, and soon let me create one of my favorite pieces, Code Notes, a way to annotate code from within a single code cell and trailing markdown cells, to be rendered later side-by-side:\n \nFor years as I was adventuring with nbdev I was creating custom forks, versions, and more to get something remotely close to this. And now thanks to this Processor, I could do it!\nBut I also wasn‚Äôt satisfied there. nbdev has very real problems that need solving, which this article will address and explore how my new library, nbquarto, attempts to solve them.\n\n\n\nFirst let‚Äôs talk about the base framework of this idea: the Processor. nbdev designed this as a way to use special comment cells (#| {name}) that Quarto calls directives to export code to particular files that were defined in the cell block, or to interact with the documentation in particular ways as it was being auto-generated through their framework.\nFor those familar with Quarto, this should sound exactly like how Quarto Extensions are made in Lua, but built in Python!\nThis was a great idea, but it had a few problems:\n\nnbdev itself is a framework built upon extreme abstraction and is entirely unreadable for those who truly want to know what‚Äôs going on. It‚Äôs a liability more than a tool, which is not good.\nAlso, nbdev is too many things at once, leading to the above. It tries to be a documentation wrapper framework, a tool that helps manage tests, and a tool that tries to export source code from notebooks to .py files.\nFinally, the way it was done is exceedingly ‚Äúmagical‚Äù. Errors are exceedingly hard to read and track, workflows would randomly break due to a random dependency upgrade that was also magical.\n\nAs a result, I decided to see how I could write nbdev in such a way that can be:\n\nReadable\nDo exactly what it needs to do (process notebooks)\nBe functional enough without much change to the overall API, because at a high level it was still quite good!\n\n\n\n\nThis framework is built on the base of nbdev. Quite literally with source code ripped from the project, and then rewritten to follow basic readability practices. As a result, code is easier to dive into and understand.\nThis framework has zero base dependencies for what it needs to do. As a result, any and all over-abstraction has been eliminated to make way for a framework that anyone can understand through a basic viewership of the code. In fact only one class from fastcore (the foundational library for nbdev) made it through: the AttributeDictionary, which basically makes a dictionary act as a namespace object by having its keys be accessible as attributes. The value of this class outweighted its ‚Äúmagicalness‚Äù, however it was still rewritten in a way that for a basic Python user, it‚Äôs clear as to what is happening:\nclass AttributeDictionary(dict):\n    \"\"\"\n    `dict` subclass that also provides access to keys as attributes.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; d = AttrDict({'a': 1, 'b': 2})\n        &gt;&gt;&gt; d.a\n        1\n        &gt;&gt;&gt; d.b\n        2\n        &gt;&gt;&gt; d.c = 3\n        &gt;&gt;&gt; d['c']\n        3\n        ```\n    \"\"\"\n\n    def __getattr__(self, k):\n        if k not in self:\n            raise AttributeError(k)\n        return self[k]\n\n    def __setattr__(self, k, v):\n        is_private = k[0] == \"_\"\n        if is_private:\n            super().__setattr__(k, v)\n        else:\n            self[k] = v\n\n    def __dir__(self):\n        res = [*self.keys()]\n        res.extend(super().__dir__())\n        return res\nFinally, this framework is designed to be modularized and flexible, so that even if you‚Äôre not processing for Quarto specifically you can still use this framework to process your notebooks in any way you want!\nI‚Äôve used the base idea of this framework to create courses, various blogs and other materials for educational purposes.\nI truly believe that nbquarto is the right blend of utilizing exploratory programming practices with documentation to keep your code where it needs to go (and how the rest of the world expects it to go), while giving you the freedom to modify your documentation however you see fit.\nJust as an example, out-of-the-box with the current implementation of nbquarto you can do the following:\n\nCreate auto-populated API documentation built on the same tooling that Hugging Face uses\nCreate Code Notes that allow you to annotate code from within a single code cell and trailing markdown cells, to be rendered later side-by-side as mentioned earlier.\n\nThese two alone have improved my ability to write and create interactive documetation by leagues ahead of what it could before, and write it in a way that‚Äôs sensible and succinct.\nAs a result, at it‚Äôs core this is a framework that:\n\nHas minimal magic possible while also:\nBeing extremely hackable and flexible\nAnd is built on good software practices to ensure that this code will be stable and usable for years to come.\n\n\n\n\nIf you‚Äôd like to learn more about nbquarto, I invite you to check out the Getting Started page which is a lenghty tutorial on how to use every aspect of nbquarto and how to get started with it. And of course feel free to peruse the source code, openly available here."
  },
  {
    "objectID": "blog/nbquarto.html#introduction",
    "href": "blog/nbquarto.html#introduction",
    "title": "Introducing nbquarto: A framework for Processing Jupyter Notebooks",
    "section": "",
    "text": "Let‚Äôs take a few steps back in time, to when I (Zach), was using tools like nbdev out of fastai. I wasn‚Äôt using it for its intended purpose (writing libraries in Jupyter Notebooks), I wasn‚Äôt a big fan of that after a while. Instead, I was trying to hack it in any way I could to help further the course websites I was writing. To help further the documentation I was generating.\nThen, nbdev 2.0.0 comes out, showing this integration to a wonderful rendering and processing framework: Quarto. Immediatly I could see the potential with this framework for teaching purposes:\n\nQuick and easy ways for me to create interactible documentation?\nA way for me use my Jupyter Notebooks but still have special syntax it uses to render specific items?\n\nAnd most of all:\n\nnbdev 2.0.0 introduced the concept of a Processor class.\n\nWithout diving into too much detail yet, it was a moduler way for me to interact with the cells in a Jupyter Notebook and do whatever I wanted with them.\nI know, I know. Might have not quite sold you on it yet, and that‚Äôs alright.\nThis idea of post-processing these notebooks allowed me to create small ‚Äúshortcut‚Äù commands for quarto to remove a ton of boilerplate code, and soon let me create one of my favorite pieces, Code Notes, a way to annotate code from within a single code cell and trailing markdown cells, to be rendered later side-by-side:\n \nFor years as I was adventuring with nbdev I was creating custom forks, versions, and more to get something remotely close to this. And now thanks to this Processor, I could do it!\nBut I also wasn‚Äôt satisfied there. nbdev has very real problems that need solving, which this article will address and explore how my new library, nbquarto, attempts to solve them."
  },
  {
    "objectID": "blog/nbquarto.html#why-not-nbdev",
    "href": "blog/nbquarto.html#why-not-nbdev",
    "title": "Introducing nbquarto: A framework for Processing Jupyter Notebooks",
    "section": "",
    "text": "First let‚Äôs talk about the base framework of this idea: the Processor. nbdev designed this as a way to use special comment cells (#| {name}) that Quarto calls directives to export code to particular files that were defined in the cell block, or to interact with the documentation in particular ways as it was being auto-generated through their framework.\nFor those familar with Quarto, this should sound exactly like how Quarto Extensions are made in Lua, but built in Python!\nThis was a great idea, but it had a few problems:\n\nnbdev itself is a framework built upon extreme abstraction and is entirely unreadable for those who truly want to know what‚Äôs going on. It‚Äôs a liability more than a tool, which is not good.\nAlso, nbdev is too many things at once, leading to the above. It tries to be a documentation wrapper framework, a tool that helps manage tests, and a tool that tries to export source code from notebooks to .py files.\nFinally, the way it was done is exceedingly ‚Äúmagical‚Äù. Errors are exceedingly hard to read and track, workflows would randomly break due to a random dependency upgrade that was also magical.\n\nAs a result, I decided to see how I could write nbdev in such a way that can be:\n\nReadable\nDo exactly what it needs to do (process notebooks)\nBe functional enough without much change to the overall API, because at a high level it was still quite good!"
  },
  {
    "objectID": "blog/nbquarto.html#enter-nbquarto",
    "href": "blog/nbquarto.html#enter-nbquarto",
    "title": "Introducing nbquarto: A framework for Processing Jupyter Notebooks",
    "section": "",
    "text": "This framework is built on the base of nbdev. Quite literally with source code ripped from the project, and then rewritten to follow basic readability practices. As a result, code is easier to dive into and understand.\nThis framework has zero base dependencies for what it needs to do. As a result, any and all over-abstraction has been eliminated to make way for a framework that anyone can understand through a basic viewership of the code. In fact only one class from fastcore (the foundational library for nbdev) made it through: the AttributeDictionary, which basically makes a dictionary act as a namespace object by having its keys be accessible as attributes. The value of this class outweighted its ‚Äúmagicalness‚Äù, however it was still rewritten in a way that for a basic Python user, it‚Äôs clear as to what is happening:\nclass AttributeDictionary(dict):\n    \"\"\"\n    `dict` subclass that also provides access to keys as attributes.\n\n    Example:\n        ```python\n        &gt;&gt;&gt; d = AttrDict({'a': 1, 'b': 2})\n        &gt;&gt;&gt; d.a\n        1\n        &gt;&gt;&gt; d.b\n        2\n        &gt;&gt;&gt; d.c = 3\n        &gt;&gt;&gt; d['c']\n        3\n        ```\n    \"\"\"\n\n    def __getattr__(self, k):\n        if k not in self:\n            raise AttributeError(k)\n        return self[k]\n\n    def __setattr__(self, k, v):\n        is_private = k[0] == \"_\"\n        if is_private:\n            super().__setattr__(k, v)\n        else:\n            self[k] = v\n\n    def __dir__(self):\n        res = [*self.keys()]\n        res.extend(super().__dir__())\n        return res\nFinally, this framework is designed to be modularized and flexible, so that even if you‚Äôre not processing for Quarto specifically you can still use this framework to process your notebooks in any way you want!\nI‚Äôve used the base idea of this framework to create courses, various blogs and other materials for educational purposes.\nI truly believe that nbquarto is the right blend of utilizing exploratory programming practices with documentation to keep your code where it needs to go (and how the rest of the world expects it to go), while giving you the freedom to modify your documentation however you see fit.\nJust as an example, out-of-the-box with the current implementation of nbquarto you can do the following:\n\nCreate auto-populated API documentation built on the same tooling that Hugging Face uses\nCreate Code Notes that allow you to annotate code from within a single code cell and trailing markdown cells, to be rendered later side-by-side as mentioned earlier.\n\nThese two alone have improved my ability to write and create interactive documetation by leagues ahead of what it could before, and write it in a way that‚Äôs sensible and succinct.\nAs a result, at it‚Äôs core this is a framework that:\n\nHas minimal magic possible while also:\nBeing extremely hackable and flexible\nAnd is built on good software practices to ensure that this code will be stable and usable for years to come."
  },
  {
    "objectID": "blog/nbquarto.html#learn-more",
    "href": "blog/nbquarto.html#learn-more",
    "title": "Introducing nbquarto: A framework for Processing Jupyter Notebooks",
    "section": "",
    "text": "If you‚Äôd like to learn more about nbquarto, I invite you to check out the Getting Started page which is a lenghty tutorial on how to use every aspect of nbquarto and how to get started with it. And of course feel free to peruse the source code, openly available here."
  },
  {
    "objectID": "blog/ClassConfusion.html",
    "href": "blog/ClassConfusion.html",
    "title": "Class Confusion, Analyzing Fastai Model Behaviors",
    "section": "",
    "text": "In my research at the University, I have found that one major weakness of mine is trying to explain how these models are working and their behavior. After enough discussion on the fastai forums, I decided to make a new widget entitled ‚ÄúClass Confusion‚Äù, which can be used in regular jupyter environments and in Google Colaboratory."
  },
  {
    "objectID": "blog/ClassConfusion.html#images",
    "href": "blog/ClassConfusion.html#images",
    "title": "Class Confusion, Analyzing Fastai Model Behaviors",
    "section": "Images:",
    "text": "Images:\nFor image classification problems, the widget works by going into the confusion matrix, finding the images that were confused, and plotting them onto the screen for the user to look at. The fastai library already has ImageCleaner, but this did not work with Google Colaboratory, a platform I do most of my machine learning work on, so I ported something close here.\nAs an example, I can pass in a class combination of: [('Ragdoll', 'Birman'), ('British_Shorthair', 'Russian_Blue')], call the function as such: ClassConfusion(interp, classlist, is_ordered=True) and our output looks something like so:\n\nWe have seperate tabs for each combination, along with the associated filename to find those images, if we want to delete them or modify them to some degree."
  },
  {
    "objectID": "blog/ClassConfusion.html#tabular",
    "href": "blog/ClassConfusion.html#tabular",
    "title": "Class Confusion, Analyzing Fastai Model Behaviors",
    "section": "Tabular:",
    "text": "Tabular:\nHere is where most of the bells and whistles live in the program. I designed it so we could make a better use of the Confusion Matrix fastai provides, by exploring the various distrobutions within our data.\nThe constructor has the varlist attribute where you can pass in specific variables to examine, else it will go through all of them. If the variable is categorical, it will go through the combination and plot the relative distributions for that particular variable. For example, using the ADULT_SAMPLE from the fastai library, I can call the function as such: ClassConfusion(interp, ['&gt;=50k', '&lt;50k'], varlist=['age', 'education', 'relationship']) and the result is the following set of bar graphs:\n\nWith this, there is also support for looking at the ‚ÄòTrue‚Äô classifications too, where the model was correct. To do this we pass in whatever truth we want into our array of combinations along with is_ordered=True, eg: ClassConfusion(interp, [['&gt;=50k', '&gt;=50k'], ['&gt;=50k', '&lt;50k']], varlist=['age', 'education', 'relationship'],                is_ordered=True, figsize=(12,12))\nAnd out from it we receive the following distributions:\n\nI hope this widget will help others be able to explain their models to non-ML individuals and non-Deep Learning individuals. For Colaboratory users, the repo to use is available here.\nThanks for reading!\nZach"
  },
  {
    "objectID": "blog/DataBlockAPI.html",
    "href": "blog/DataBlockAPI.html",
    "title": "fastai and the New DataBlock API",
    "section": "",
    "text": "A quick glance at the new top-level api"
  },
  {
    "objectID": "blog/DataBlockAPI.html#what-is-the-datablock-api",
    "href": "blog/DataBlockAPI.html#what-is-the-datablock-api",
    "title": "fastai and the New DataBlock API",
    "section": "What is the DataBlock API?",
    "text": "What is the DataBlock API?\nThe DataBlock API is certainly nothing new to fastai. It was here in a lesser form in the previous version, and the start of an idea. This idea was: ‚ÄúHow do we let the users of the fastai library build DataLoaders in a way that is simple enough that someone with minimal coding knowledge could get the hang of it, but be advanced enough to allow for exploration.‚Äù The old version was a struggle to do this from a high-level API standpoint, as you were very limited in what you could do: variables must be passed in a particular order, the error checking wasn‚Äôt very explanatory (to those unaccustomed to debugging issues), and while the general idea seemed to flow, sometimes it didn‚Äôt quite work well enough. For our first example, we‚Äôll look at the Pets dataset and compare it from fastai version 1 to fastai version 2\nThe DataBlock itself is built on ‚Äúbuilding blocks‚Äù, think of them as legos. (For more information see fastai: A Layered API for Deep Learning) They can go in any order but together they‚Äôll always build something. Our lego bricks go by these general names:\n\nblocks\nget_items\nget_x/get_y\ngetters\nsplitter\nitem_tfms\nbatch_tfms\n\nWe‚Äôll be exploring each one more closely throughout this series, so we won‚Äôt hit on all of them today"
  },
  {
    "objectID": "blog/DataBlockAPI.html#importing-from-the-library",
    "href": "blog/DataBlockAPI.html#importing-from-the-library",
    "title": "fastai and the New DataBlock API",
    "section": "Importing from the library",
    "text": "Importing from the library\nThe library itself is still split up into modules, similar to the first version where we have Vision, Text, and Tabular. To import from these libraries, we‚Äôll be calling their .all files. Our example problem for today will involve Computer Vision so we will call from the .vision library\nfrom fastai2.vision.all import *"
  },
  {
    "objectID": "blog/DataBlockAPI.html#pets",
    "href": "blog/DataBlockAPI.html#pets",
    "title": "fastai and the New DataBlock API",
    "section": "Pets",
    "text": "Pets\nPets is a dataset in which you try to identify one of 37 different species of cats and dogs. To get the dataset, we‚Äôre going to use functions very familiar to those that used fastai version 1. We‚Äôll use untar_data to grab the dataset we want. In our case, the Pets dataset lives in URLs.PETS\nURLs.PETS\npath = untar_data(URLs.PETS)\n\nLooking at the dataset\nWhen starting to look at adapting the API for a particular problem, we need to know just how the data is stored. We have an image problem here so we can use the get_image_files function to go grab all the file locations of our images and we can look at the data!\nfnames = get_image_files(path/'images')\nTo investigate how the files are named and where they are located, let‚Äôs look at the first one:\nfnames[0]\nNow as get_image_files grabs the filename of our x for us, we don‚Äôt need to include our get_x here (which defaults to None) as we just want to use this filepath! Now onto our file paths and how they relate to our labels. If we look at our returned path, this particular image has the class of pug.\nWhere do I see that?\nHere: Path(‚Äò/root/.fastai/data/oxford-iiit-pet/images/pug_119.jpg‚Äô)\nAll the images follow this same format, and we can use a Regular Expression: to get it out. In our case, it would look something like so:\npat = r'([^/]+)_\\d+.*$'\nHow do we know it worked? Let‚Äôs apply it to the first file path real quick with re.search where we pass in the pattern followed by an item to try and find a match in the first group (set of matches) with a Regular Expression\nre.search(pat, str(fnames[0])).group(1)\nWe have our label! So what parts do we have so far? We know how to grab our items (get_items and get_x), our labels (get_y), what‚Äôs left? Well, we‚Äôll want some way to split our data and our data augmentation. Let‚Äôs focus on the prior.\n\n\nSplitting and Augmentation\nAny time we train a model, the data must be split between a training and validation dataset. The general idea is that the training dataset is what the model adjusts and fits its weights to, while the validation set is for us to understand how the model is performing. fastai2 has a family of split functions to look at that will slowly get covered throughout these blogs. For today we‚Äôll randomly split our data so 80% goes into our training set and 20% goes into the validation. We can utilize RandomSplitter to do so by passing in a percentage to split by, and optionally a seed as well to get the same validation split on multiple runs\nsplitter = RandomSplitter(valid_pct=0.2, seed=42)\nHow is this splitter applied? The splitter itself is a function that we can then apply over some set of data or numbers (an array). It works off of indexes. What does that look like? Let‚Äôs see:\nsplitter(fnames)\nThat doesn‚Äôt look like filenames! Correct, instead its the location in our list of filenames and what group it belongs to. What this special looking list (or L) also tells us is how many items are in each list. In this example, the first (which is our training data) has 5,912 samples and the second (which is our validation) contains 1,478 samples.\nNow let‚Äôs move onto the augmentation. As noted earlier, there are two kinds: item_tfms and batch_tfms. Each do what it sounds like: an item transform is applied on an individual item basis, and a batch transform is applied over each batch of data. The role of the item transform is to prepare everything for a batch level (and to apply any specific item transformations you need), and the batch transform is to further apply any augmentations on the batch level efficently (normalization of your data also happens on a batch level). One of the biggest differences between the two though is where each is done. Item transforms are done on the CPU while batch transforms are performed on the GPU.\nNow that we know this, let‚Äôs build a basic transformation pipeline that looks something like so: 1. Resize our images to a fixed size (224x224 pixels) 2. After they are batched together, choose a quick basic augmentation function 3. Normalize all of our image data\nLet‚Äôs build it!\nitem_tfms = [Resize(224, method='crop')]\nbatch_tfms=[*aug_transforms(size=256), Normalize.from_stats(*imagenet_stats)]\nWoah, woah, woah, what in the world is this aug_transforms thing you just showed me I hear you ask? It runs a series of augmentations similar to the get_transforms() from version 1. The entire list is quite exhaustive and we‚Äôll discuss it in a later blog, but for now know we can pass in an image size to resize our images to (we‚Äôll make our images a bit larger, doing 256x256).\nAlright, we know how we want to get our data, how to label it, split it, and augment it, what‚Äôs left? That block bit I mentioned before.\n\n\nThe Block\nBlock‚Äôs are used to help nest transforms inside of pre-defined problem domains.\nLazy-man‚Äôs explaination?\nIf it‚Äôs an image problem I can tell the library to use Pillow without explicitly saying it, or if we have a Bounding Box problem I can tell the DataBlock to expect two coordinates for boxes and to apply the transforms for points, again without explicitly saying these transforms.\nWhat will we use today? Well let‚Äôs think about our problem: we are using an image for our x, and our labels (or y‚Äôs) are some category. Is there blocks for this? Yes! And they‚Äôre labeled ImageBlock and CategoryBlock! Remember how I said it just ‚Äúmade more sense?‚Äù This is a direct example. Let‚Äôs define them:\nblocks = (ImageBlock, CategoryBlock)"
  },
  {
    "objectID": "blog/DataBlockAPI.html#now-lets-build-this-datablock-thing-already",
    "href": "blog/DataBlockAPI.html#now-lets-build-this-datablock-thing-already",
    "title": "fastai and the New DataBlock API",
    "section": "Now let‚Äôs build this DataBlock thing already!",
    "text": "Now let‚Äôs build this DataBlock thing already!\nAlright we have all the pieces now, let‚Äôs see how they fit together. We‚Äôll all wrap them up in a nice little package of a DataBlock. Think of the DataBlock as a list of instructions to do when we‚Äôre building batches and our DataLoaders. It doesn‚Äôt need any items explicitly to be done, and instead is a blueprint of how to operate. We define it like so:\nblock = DataBlock(blocks=blocks,\n                  get_items=get_image_files,\n                  get_y=RegexLabeller(pat),\n                  splitter=splitter,\n                  item_tfms=item_tfms,\n                  batch_tfms=batch_tfms)\nOnce we have our DataBlock, we can build some DataLoaders off of it. To do so we simply pass in a source for our data that our DataBlock would be expecting, specifically our get_x and get_y, so we‚Äôll follow the same idea we did above to get our filenames and pass in a path to the folder we want to use along with a batch size:\ndls = block.dataloaders(path, bs=64)\nWhile it‚Äôs a bit long, you can understand why we had to define everything the way that we did. If you‚Äôre used to how fastai v1 looked with the ImageDataBunch.from_x, well this is stil here too:\ndls = ImageDataLoaders.from_name_re(path, fnames, pat, item_tfms=item_tfms,\n                                            batch_tfms=batch_tfms, bs=64)\nI‚Äôm personally a much larger fan of the first example, and if you‚Äôre planning on using the library quite a bit you should get used to it more as well! This blog series will be focusing on that nomenclature specifically. To make sure everything looks okay and we like our augmentation we can show a batch of images from our DataLoader. It‚Äôs as simple as:\ndls.show_batch()"
  },
  {
    "objectID": "blog/DataBlockAPI.html#fitting-a-model",
    "href": "blog/DataBlockAPI.html#fitting-a-model",
    "title": "fastai and the New DataBlock API",
    "section": "Fitting a Model",
    "text": "Fitting a Model\nNow from here everything looks and behaves exactly how it did in fastai version 1: 1. Define a Learner 2. Find a learning rate 3. Fit\nWe‚Äôll quickly see that fastai2 has a quick function for transfer learning problems like we are doing, but first let‚Äôs build the Learner. This will use cnn_learner, as we are doing transfer learning, and we‚Äôll tell the function to use a resnet34 architecture with accuracy metrics\nlearn = cnn_learner(dls, resnet34, metrics=accuracy)\nNow normally we would do learn.lr_find() and find a learning rate, but with the new library, we now have a fine_tune() function we can use instead specifically designed for transfer learning scenarios. It runs a specified number of epochs (the number of times we fully go through the dataset) on a frozen model (where all but the last layer‚Äôs weights are not trainable) and then the last few will be on an unfrozen model (where all weights are trainable again). When just passing in one set of epochs, like below, it will run frozen for one and unfrozen for the rest. Let‚Äôs try it!\nlearn.fine_tune(3)\nAs we can see we did pretty goood just with this default! Generally when the accuracy is this high, we want to turn instead to error_rate for our metric, as this would show ~6.5% and is a better comparison when it gets very fine tuned.\nBut that‚Äôs it for this first introduction! We looked at how the Pets dataset can be loaded into the new high-level DataBlock API, and what it‚Äôs built with. In the next blog we will be exploring more variations with the DataBlock as we get more and more creative. Thanks for reading!"
  },
  {
    "objectID": "blog/DockerforDataScience.html",
    "href": "blog/DockerforDataScience.html",
    "title": "Docker for Data Science, Efficient Image Instancing without System Issues",
    "section": "",
    "text": "Covering how and why you should setup Docker for your local environment when developing Machine Learning models and products"
  },
  {
    "objectID": "blog/DockerforDataScience.html#what-is-docker-why-do-i-love-it-and-why-should-you-suddenly-worship-the-almighty-whale",
    "href": "blog/DockerforDataScience.html#what-is-docker-why-do-i-love-it-and-why-should-you-suddenly-worship-the-almighty-whale",
    "title": "Docker for Data Science, Efficient Image Instancing without System Issues",
    "section": "What is Docker? Why do I love it? And why should you suddenly worship the Almighty Whale?",
    "text": "What is Docker? Why do I love it? And why should you suddenly worship the Almighty Whale?\nIshmael once set out to conquer the seas by taking down the leviathan: Moby Dick. The unkillable being that haunted the seas. However, in his endevour, he failed. The Whale completely destroying his ship and crew, leaving him as the sole survivor.\nBut have we ever stopped and thought: should we instead be worshiping this Whale? Is it too strong for mortal man? And how can we do so, and what gifts will it bring forth?\nToo weird? Too weird. Anyways, back on topic to somewhat whale-related things.\nDocker is a containerization platform.\nWhat does this mean? Let‚Äôs do some quick introductions.\nSome of you may be familiar with the concept of a Virtual Machine (VM). These VM‚Äôs are often ran to boot Linux onto your Windows machine and work in an environment that has mild access to your internal system. What can get annoying very quickly with VM‚Äôs however is they require a hefty amount of system resources to be used, even if you might not get through all of it. Typically you have to direct the virtual machine to allocate X Gb of memory for you to use, X Gb of storage for it to utilize, and so forth. Limiting how much you can do.\nThe result? Running over 3 virtual machines in a standard environment is near impossible to run efficiently.\nAlright, what about a ‚Äúcontainer‚Äù? Containers are a light-weight infrastructure that runs atop your current environment. It doesn‚Äôt have it‚Äôs own allocation in memory, and so it shares directly with what your system is doing.\nThe result?\nYou can quite easily spin up and run hundreds of low-resource intensive Docker images at once, have them do their jobs, and then shut down, without too much headache. (Think something along the lines of run a shell script that writes to a random txt document 1,000 times: low intensive).\nThe image below is a good visualization of this, taken from the Virtual machines vs Docker Containers YouTube video:\n\n\n\nDocker vs VMs\n\n\nYou can see that while both the VM and the Docker containers require underlying infrastructure and a base host operating system (think what your every day environment is on your computer), VM‚Äôs require their own explicit Guest OS‚Äôs to be installed, while Docker containers are just a thin layer that lives within the Docker Daemon.\nAs a result, the containers are extremely lightweight, typically don‚Äôt step on each others toes, and are quite efficient."
  },
  {
    "objectID": "blog/DockerforDataScience.html#moving-on-what-does-this-have-to-do-with-data-science",
    "href": "blog/DockerforDataScience.html#moving-on-what-does-this-have-to-do-with-data-science",
    "title": "Docker for Data Science, Efficient Image Instancing without System Issues",
    "section": "Moving on, what does this have to do with Data Science?",
    "text": "Moving on, what does this have to do with Data Science?\nThis sounds like quite a bit of DevOps work, doesn‚Äôt it? (That‚Äôs because it is!) Let‚Äôs try and tie this back into our world of Data Science and Machine Learning.\nSetting up a local environment (be it GPU enabled or not) is one of the most frustrating aspects of our field. You can follow a million tutorials and still not get it right. God forbid you mess up your conda environment too, and then you‚Äôre really in a pickle.\nThe nice thing about these Docker containers is so long as we supply it with the right underlying hardware to use (yes, that means we‚Äôll be covering installing CUDA and whatnot sadly), managing your environment is as simple as throwing up a new Docker image. But what if I break that environment? Rebuild it. I don‚Äôt lose my data (if we configure it properly), and I can just say goodbye to that environment configuration and start anew.\nWe get access to this capability through a shared resource called the Docker Hub. Through here, we can host our own pre-built Docker configurations and can run them on our local machines!\n\nImportant Notice: We will not be talking about writing Dockerfile‚Äôs here. We will be discussing utilizing them instead. For a good tutorial/curriculum, my course I took in school utilized Docker 101, and here‚Äôs a good best practices reference"
  },
  {
    "objectID": "blog/DockerforDataScience.html#awesome-so-how-do-i-setup-my-environment",
    "href": "blog/DockerforDataScience.html#awesome-so-how-do-i-setup-my-environment",
    "title": "Docker for Data Science, Efficient Image Instancing without System Issues",
    "section": "Awesome, so how do I setup my environment?",
    "text": "Awesome, so how do I setup my environment?\nI‚Äôll be following you through the exact same steps it took me to setup my new MSI Laptop, which came with a NVIDIA GPU attached with it. It‚Äôll be a process, with many links, but it should not take you more than 30 minutes in total!\nThis will be the shortest part of this article, due to quite ltierally how quick this was. So, ready? Let‚Äôs get started!"
  },
  {
    "objectID": "blog/DockerforDataScience.html#downloading-ubuntu",
    "href": "blog/DockerforDataScience.html#downloading-ubuntu",
    "title": "Docker for Data Science, Efficient Image Instancing without System Issues",
    "section": "Downloading Ubuntu",
    "text": "Downloading Ubuntu\nFirst step is (if you so choose) installing Ubuntu onto your machine. I did because I wanted the entire seperated operating system, but do note: this is not required. I didn‚Äôt want to deal with any thoroughput issues with WSL2, and I like staying on Windows 10, so this was the other main reason why I just went ahead with rolling Ubuntu.\nYou will need a thumb drive of some form for this, as we will be making a bootable image to install from.\nFirst: Download and install Rufus.\nAfterwards, navigate to the Ubuntu Desktop homepage. Download the ISO, open it in Rufus, and hit START.\nIt should prompt you saying we‚Äôll completely wipe that thumb drive, that‚Äôs expected, then wait for it to finish.\nOnce that is done, I‚Äôd personally recommend allocating a chunk of your hard drive to be your Linux partition.\nConsidering I do a solid 50/50 of my work between Windows and Ubuntu, I just left half of my hard drive partitioned. Read how to do so here &gt; Note: Do not choose to allocate the storage as an NTFS drive, leave it directly unallocated. Ubuntu can‚Äôt utilize NTFS drives for booting"
  },
  {
    "objectID": "blog/DockerforDataScience.html#installing-ubuntu",
    "href": "blog/DockerforDataScience.html#installing-ubuntu",
    "title": "Docker for Data Science, Efficient Image Instancing without System Issues",
    "section": "Installing Ubuntu",
    "text": "Installing Ubuntu\nNext, you‚Äôll want to install Ubuntu. Word of advice: bring this article up on your phone so you can keep reading. As during it, you won‚Äôt have access to your computer.\nYou will need: - Your laptop, turned off - Our newly-made image installer on our thumb drive - An Ethernet cable.\nYes, you heard me: an Ethernet cable. Even if you‚Äôre doing this through your laptop, you‚Äôll want one.\nOut-of-the-box, Linux will likely not support your wireless card, and we‚Äôll hit a switch during install to get that.\nAlright, are we all set? Great.\nBoot up your computer and open your BIOS (Google it for your laptop brand, for some it‚Äôs F7, F9, or even Delete).\nOnce there, you‚Äôll want to navigate to the Boot section, and find the Boot Order. If your BIOS supports you directly booting off of some hardware, navigate to your USB Bus (it should say Ubuntu on it), and hit ‚ÄúBoot‚Äù. If not, adjust the boot order to start with our little thumb drive, and then hit the ‚ÄúSave and Exit‚Äù button.\nUpon restarting, you should be on a new prompt asking you what you want to start in. You will get extremely familiar with the GNOME prompt, as it is what will handle telling your computer to either boot in Windows or Ubuntu. Here you just want to press the ‚ÄúTry Ubuntu‚Äù option (the first).\nAfter it boots, you should see on your desktop a ‚ÄúInstall Ubuntu XYZ‚Äù shortcut, go ahead and click that to start installing it. &gt; Note: I had some issues with Ubuntu being laggy, it was just due to this preview. It went away post installation\nMore or less just follow along the directions. HOWEVER: At one point it will prompt you with a radial button saying ‚ÄúInstall just what I need‚Äù, with another option saying install 3rd-party drivers.\nFor the love of everything, hit that button (this is step You need to install 3rd-party drivers to have access to bluetooth, internet, and a slew of other things. So do it.\nWait about 10-15 minutes if that for it to install, and you should have a functioning Linux install!\nNow boot up your new Ubuntu OS, and we can begin the fun!"
  },
  {
    "objectID": "blog/DockerforDataScience.html#installing-nvidia-and-the-drivers",
    "href": "blog/DockerforDataScience.html#installing-nvidia-and-the-drivers",
    "title": "Docker for Data Science, Efficient Image Instancing without System Issues",
    "section": "Installing NVIDIA and the Drivers",
    "text": "Installing NVIDIA and the Drivers\nWe‚Äôre at the scary part now, installing the drivers. Follow this tutorial exactly and you will be fine. Ensure you find the right driver for your GPU, by looking here.\nI had heard a rumour in a tweet that Ubuntu enjoyed eating up our GPU RAM for rendering our screen. That‚Äôs not good! I want as much VRAM as I can for running ML!\nIs there a solution? Of course. And here it is on a silver platter:\nA wonderful little gist on Setting up integrated graphics for display\nPretty much follow that gist 1:1, and by the end you will have CUDA installed without hating yourself. Below are a few important notes I‚Äôll highlight:\n\nThe path to the xorg.conf file should be /etc/X11/xorg.conf and not /etx/X11/xorg.conf\n\nAnd especially this post by @scaomath further in the comments:\n\nThanks for the post. I was able to get the iGPU in 10th gen Intel running using the xorg.conf given, as well as CUDA running VSCode. However, I got all pixelated and weird color strips across the screen. So I did some Google-fu and found that in order that X works on Intel UHD 630 (8th-10th gen desktop CPU). The iGPU part needs to be changed to:\n\nSection \"Device\"\n    Identifier  \"Intel Graphics\"\n    Driver      \"modesetting\"\n    BusID       \"PCI:0:2:0\"\nEndSection\n\nThe driver has to be modesetting which is in xserver-xorg-core. Just to be safe, it is recommended to uninstall xserver-xorg-video-intel as well.\n\nI did both, and afterwards I got ~10mb of constant usage out of my monitor from the GPU. Not perfect, but 10mb won‚Äôt kill me or my performance too much"
  },
  {
    "objectID": "blog/DockerforDataScience.html#next-up-docker",
    "href": "blog/DockerforDataScience.html#next-up-docker",
    "title": "Docker for Data Science, Efficient Image Instancing without System Issues",
    "section": "Next up: Docker",
    "text": "Next up: Docker\nOkay, that was the hardest part of this entire thing. Seriously.\nAll that‚Äôs left is for you to download Docker-GPU, and we‚Äôre ready to rock and roll.\nFirst, follow the docker installation directions from this digital ocean article: - Installation directions\nOnce that‚Äôs done, we‚Äôll follow the directions for installing the CUDA capability for Docker here\n\nImportant: Skip the ‚ÄúSetting up Docker‚Äù portion, you have already done so.\n\nAnd, I kid you not, you‚Äôre done!\nYou can quickly test if it‚Äôs working by doing the following:\nTest: nvidia-smi and make sure your GPU comes up\nRun: docker run --rm --gpus all nvidia/cuda:XX.X-base nvidia-smi\n(replace XX.X with the CUDA version that showed up in nvidia-smi). you should see the GPU show there as well.\nThe rest of this article will be covering some basic docker know-how for you to help utilize this great process, as well as some bash shortcuts I‚Äôve made to speed up my development."
  },
  {
    "objectID": "blog/DockerforDataScience.html#docker-the-basics",
    "href": "blog/DockerforDataScience.html#docker-the-basics",
    "title": "Docker for Data Science, Efficient Image Instancing without System Issues",
    "section": "Docker: The Basics",
    "text": "Docker: The Basics\nIn all reality, you need to know about four commands:\n\ndocker container list\ndocker search\ndocker pull\ndocker run\ndocker kill\n\nLet‚Äôs go over each quickly:\n\ndocker container list\nThis command will list all active docker containers and their current status. You can see an example output of mine below:\n\n\nNote: the terminal doesn‚Äôt look too pretty or readable here, for now that‚Äôs fine. I wrote something to deal with that later.\n\nWhat‚Äôs important to read here: - Names: A nickname for a particular container, when referencing them in docker commands you can either use this or the CONTAINER ID\n\nImage: What the base image of the container is using\nPorts: What exposed ports are running that you can connect to (more on this later).\n\nThe rest you can look at or see if you need, but these are the three I pay the most attention to (along with Status).\n\n\ndocker search\nThis is what you (should) be using to find any docker containers you‚Äôre interested in running. this will search the Docker Hub and find what you need. Below is an example of me searching for fastai images:\n\nLet‚Äôs keep a note of how the official fastai image is fastdotai/fastai. We‚Äôll need this for later.\n\n\ndocker pull\ndocker pull is what will actually take that docker image on the hub, and bring it to our local device. It‚Äôs a fairly straightfoward command as well. To download that fastai image, we do:\ndocker pull fastdotai/fastai.\nThat‚Äôs about it!\n\n\ndocker run\nDocker run is how you will start up most of your docker images. For a full list of parameters you can pass in, see the documentation here, but for our sake I‚Äôll TL;DR the most important ones for you:\n\n-v specifies a volume for you to mount. It follows the syntax of path/on/mine/from/root:/mnt/docker_volume/name, where the latter half of that path is where it appears locally on your docker image. This will directly link to your local system, allowing you to touch your local files directly.\n\n\nNote: Docker containers mount different file storage systems. Ensure you figure out the direct path it boots up in and then setup your volume there. This will alleviate quite a number of headaches\n\n\n-p specifies exposed ports on your system. This is extremely important for say running Jupyter servers inside the image, and wanting to talk with them. Typically they follow a format of -p 8888:8888. The 8888 is the port on your system, and the second 8888 is the port inside the docker image. So, for example if inside the docker image is a server spin-up script, that exposes a jupyter port on localhost:8888, we could just leave it be and connect to port 8888. But we run into issues if I try and run two of this image, as the servers would mess with each other. By changing that first port number, we can perform port forwarding, and we will tell the docker container to send all data coming from port 8888 to 8887, something not in use.\n\n\nNote: Depending on the setup, it may still say ‚Äúconnect to 127.x.x.x:8888‚Äù. Remember that this is not the port we exposed, so trust your port changing and go to the right web address\n\n\n--gpus specifies the number of GPUs to use. You can specify 1, 2, or all. I typically just use all. This is specific from us installing the CUDA version of docker earlier, so do take note to run your images with --gpus if you‚Äôre hoping to make use of your GPU in your system.\n\nIf we wanted to run that downloaded image of fastai we grabbed earlier and spin it up in our current working directory, here‚Äôs how that would look: (I‚Äôll explain the fancy bash)\ndocker run $(pwd):/home/runner/$(basename $(pwd)) -p 8888:8888 --gpus all fastdotai/fastai ./run_jupyter.sh\nNow, let‚Äôs quickly talk about what this does.\n\npwd grabs your current working directory, and its full path\nbasename takes the last path in a long path (such as one returned by pwd)\n\nDoing this should now expose a jupyter server running on localhost:8888 (that requires a token), and is utilizing your GPU!\n\n\ndocker kill\nFinally, we have the death command: docker kill. With this, you simply specify either the container id from earlier, or the nickname it gave you, and it will immediatly shut it down. An example of shutting down our container from earlier (the nbdev one running in the script) would look like the following:\ndocker kill happy_poincare\n\nNote: Since we can run multiple containers of the same image, this is the importance of us specifying the nickname affiliated with it"
  },
  {
    "objectID": "blog/DockerforDataScience.html#my-secret-sauce",
    "href": "blog/DockerforDataScience.html#my-secret-sauce",
    "title": "Docker for Data Science, Efficient Image Instancing without System Issues",
    "section": "My Secret Sauce",
    "text": "My Secret Sauce\nWhat follows now will be my own secret commands I‚Äôve written up, along with a library, to make my own life easier.\nI threw all of these into ./bash_aliases.\nWhen it comes to docker commands, I‚Äôm lazy and don‚Äôt want to write docker container, so we have the following:\n\nalias dkill=\"docker kill (kill a container)\nalias dp=\"docker pull (pull a container)\nalias dr=\"docker run (run a container)\n\nThey still maintain their arguments (and respective helps), while also keeping my own sanity.\nI also have a few of the following for Python: - alias python=python3 - alias py=python3\nFinally, we have some that handle running docker run, but before we get too deep into it, just set the following:\n\nalias start=\"dr \"\n\n\nBy setting a space at the end, we can have it trail off into commands directly below it, which we will utilize. You‚Äôll understand why for semantics I have ‚Äústart‚Äù instead of dr in a moment"
  },
  {
    "objectID": "blog/DockerforDataScience.html#docker-buddy",
    "href": "blog/DockerforDataScience.html#docker-buddy",
    "title": "Docker for Data Science, Efficient Image Instancing without System Issues",
    "section": "Docker Buddy",
    "text": "Docker Buddy\nAs I was working my way through this, I didn‚Äôt like how some of the outputs looked (very bland, unreadbale). So as a result, I wrote docker_buddy, which utilizes Rich to introduce clean and concise versions of the same commands I just ran earlier.\nThere are three alternatives in it:\n\nds for docker search\ndls for docker container list\ndi for inspecting docker containers, I nickname it as docker investigate in my head (though is not directly docker)\n\nTo install:\npip install docker_buddy\nAnd also do:\nsudo apt install npm\nnpm i -g rekcod\nFor an example of how those help us out, take a quick peek at the images I provided for docker search and docker container list. Here‚Äôs our new ones:\nDocker Search (ds): \nDocker Container List (dls): \nThanks entirely to Rich, we‚Äôre able to pull off this beautiful looking UI.\nI also wrote di as a way for us to see what inherit arguments are available for the docker image specifically, i.e.¬†their configurations. An example output is below:\n\nNow that we have docker_buddy on our system, here‚Äôs those last two cheats I have in my alias:\n\nalias nbdev=\"-v $(pwd):/home/runner/$(basename $(pwd)) -p 8888:8888 fastdotai/nbdev\nalias jupyt=\"-v $(pwd):/home/jovyan/$(basename $(pwd)) -p 8886:8888 jupyter/minimal-notebook\n\nCombined, these all now allow me to quickly do:\nstart fastai\nor\nstart jupyt\nTo skip all of the long configuration setups to start my server. If needed we can of course go through and run them manually, but these are quick shortcuts I wrote for myself."
  },
  {
    "objectID": "blog/DockerforDataScience.html#conclusion",
    "href": "blog/DockerforDataScience.html#conclusion",
    "title": "Docker for Data Science, Efficient Image Instancing without System Issues",
    "section": "Conclusion",
    "text": "Conclusion\nI truly hope this has helped many of you setup some brand new environemnts, and use CUDA with as minimal headache as possible. The moment I got this working my mind got racing with all the possibilities this could bring, which is what drove me to write this article.\nIf you‚Äôve enjoyed this, please do let me know!\nYou can find me on my socials at:\n\nTwitter: @TheZachMueller\nLinkedIn: Zach Mueller"
  },
  {
    "objectID": "blog/Suspecto.html",
    "href": "blog/Suspecto.html",
    "title": "Suspecto - Analyzing News Articles with Natural Language Processing to Score Credibility",
    "section": "",
    "text": "Annually, the University of West Florida runs an event called ‚ÄúCodeFest‚Äù[1]. The idea behind the event is teams of 5 to 6 individuals generate an end user experience on a topic and idea of your choosing related to the theme of the year. This year, the theme was ‚Äúsmart‚Äù within a context of: helping people to make smarter decisions, smart cities, or forming smart habits. The goal is to develop this implementation within a 48-hour period.\n\nThe Project:\n\nDevelop an interface where a user may enter a selection of text and is served a score of how ‚Äúcredible‚Äù that text may be.\n\nThe Team:\n\n\nMyself\nCarson Wilber, dual major in Cybersecurity and Computer Science with a minor in Mathematics\nChristian Um Kaman, major in Computer Science and has a B.S. in Psychology\nSarah Pham, major in Computer Science specializing in Software Engineering\nBasil Kuloba, major in Computer Science.\n\n\n\nThe Data:\n\n\nThe Fake News Corpus[6] is ‚Äúan open source dataset composed of millions of news articles mostly scraped from a curated list of 1001 domains from http://www.opensources.co/. Because the list does not contain many reliable websites, additionally NYTimes and WebHose English News Articles articles has been included to better balance the classes.‚Äù\n\nIn order to download the data to your local machine, run the following in your Jupyter notebook:\nwget https://storage.googleapis.com/researchably-fake-news-recognition/news_cleaned_2018_02_13.csv.zip\nThe CSV document downloaded is approximately 30 GB in size and includes 8.5 million articles. For the sake of the competition and time available, the language model only used the first 120,000 articles. The metric we wanted to produce measures the credibility of an input text based on its similarity to this dataset. We call it the Eddy Score, named after Dr.¬†Brian Eddy, one of our dear mentors and the creator of CodeFest.\n\nULMFiT\n\nULMFiT [2][3] , or Universal Language Model Fine-Tuning, is used for text classification. It originates by building a language model that is trained on the English language, or what corpus of language your model will be using. Then, the model is refined using a corpus of text from the specific domain; in our case, it was approximately 30 GB of sample news articles. Finally, the data includes the specific domain labels, which are used to train the final classifier built on top of the language model. ULMFiT essentially operates by inputting an article and taking the first word from the article, and using the model from the second stage, attempts to guess what the next word will be in that sentence. This allows it to perform a semantic comparison with the classes previously used by the model based upon predicted content versus real content, producing a logit similarity of each class. At the end of the article, we have an overall percentage of how well the writing style fit into the categories.\nThe categories in which the data are labeled are as follows:\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nFake News\nSources that entirely fabricate information, disseminate deceptive content, or grossly distort actual news reports.\n\n\nSatire\nSources that use humor, irony, exaggeration, ridicule, and false information to comment on current events\n\n\nExtreme Bias\nSources that come from a particular point of view and may rely on propaganda, decontextualized information, and opinions distorted as facts.\n\n\nConspiracy Theory\nSources that are well-known promoters of kooky conspiracy theories.\n\n\nState News\nSources in repressive states operating under government sanction.\n\n\nJunk Science\nSources that promote pseudoscience, metaphysics, naturalistic fallacies, and other scientifically dubious claims.\n\n\nHate News\nSources that actively promote racism, misogyny, homophobia, and other forms of discrimination.\n\n\nClick-Bait\nSources that provide generally credible content, but use exaggerated, misleading, or questionable headlines, social media descriptions, and/or images.\n\n\nUnreliable\nSources that may be reliable but whose contents require further verification.\n\n\nPolitical\nSources that provide generally verifiable information in support of certain points of view or political orientations.\n\n\nCredible/Reliable\nSources that circulate news and information in a manner consistent with traditional and ethical practices in journalism (Remember: even credible sources sometimes rely on clickbait-style headlines or occasionally make mistakes. No news organization is perfect, which is why a healthy news diet consists of multiple sources of information.)\n\n\n\nWhen we reach the end of an article or series of text, a culmination of these percentages has been done, and standardly it would output the highest percentage found. For our case though, this is where the Eddy Score comes in and why it is important. Our credibility score, which is a percentage, takes initially into account the credibility rating the model found, then we subtract a variety of criteria listed below. The reasoning is this is not simply black and white. While the political category in itself may not be bias and verifiable, if it is political and has extreme bias, we need to factor that in. The Eddy Score is calculated as such:\n\n&lt;img src=\"media/blogs/Suspecto/06.png\" /&gt;\n\nThe result is a score from 0-100 denoting the relative credibility of a sentence or series of sentences.\nBefore continuing, it is important to make a note on the ethical implications of this model: this application alone is not sufficiently thorough enough to take for granted. It provides a baseline for further work, but presently does not analyze any claims or facts stated for authenticity. As a result, regardless of the Eddy Score of a selection of text, always be skeptical.\nNow for the application‚Äôs build process. I utilized the Fast.AI libraries below:\n\n from fastai import *  from fastai.text import * \n\nWhen importing the dataset, a small subset was used for memory and time constraints (another indicator of further work to be done). As mentioned before, training was performed using 120,000 of the 8.5 million samples of the Fake News Corpus. The data was read into pandas, split and labeled into ‚ÄòTraining‚Äô and ‚ÄòValidation‚Äô sets, recombined, and then packaged into a TextDataBunch.\n\n dfTrain = pd.read_csv(‚Äònews_cleaned_2018_02_13.csv‚Äô, nrows=100000)  dfValid = pd.read_csv(‚Äònews_cleaned_2018_02_13.csv‚Äô, names=[‚Äòtype‚Äô,‚Äôcontent‚Äô], skiprows=10000, nrows=20000)  dfDatasetTrain = pd.DataFrame() dfDatasetTrain[‚Äòtype‚Äô] = dfTrain[‚Äòtype‚Äô] dfDatasetTrain[‚Äòcontent‚Äô] = dfTrain[‚Äòcontent‚Äô] dfDatasetTrain[‚Äòis_valid‚Äô] = ‚ÄòTrue‚Äô dfDatasetValid = pd.DataFrame() dfDatasetValid[‚Äòtype‚Äô] = dfTrain[‚Äòtype‚Äô] dfDatasetValid[‚Äòcontent‚Äô] = dfTrain[‚Äòcontent‚Äô] dfDatasetValid[‚Äòis_valid‚Äô] = ‚ÄòTrue‚Äô  dfAll = pd.concat([dfDatasetTrain, dfDatasetValid]) dfAll.to_csv(‚Äògood_small_dataset.csv‚Äô) data_lm= TextDataBunch.from_csv(‚Äô‚Äò, ‚Äôgood_small_dataset.csv‚Äô) \n\nNow that we have a databunch, we can create a language_model_learner using a long short-term memory (LSTM) architecture, known as AWD_LSTM. This will allow us to have that initial model that understands the corpus of what we are planning to be looking at. Afterwards, we can find the proper learning rate, and train the first section of layers within our model.\n\n learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5) learn.lr_find() learn.recorder.plot() \n\nHere is an output of what our model‚Äôs summary looks like, as well as the learning rate plot:\nThe model:\n\nThe LR plot:\n\nWith this, we can now pick a learning rate of roughly 1e-2 and train for one epoch at our current layer-levels. We are doing what is called ‚Äúgradually unfreezing‚Äù of our model. This is often done with transfer-learning so we can re-use related weights, and start from a pretty close baseline to what we want to get to. Most people should be familiar with the image-version of this, when we use the ImageNet weights!\n\n learn.fit_one_cycle(1, 1e-2) \n\n\n\nOne thing to note here, I was using Google Colab at the time and this was before Jeremy Howard and Sylvain Gugger had managed to bring down that training time.\n\nNow we can unfreeze our weights, do another instance of lr_find() and train for one more epoch.\n\n learn.fit_one_cycle(1, 1e-3) \n\n\nAfter training, the model understands how to construct primitive sentences in the given language. The overall accuracy achieved was approximately 40%, which for an overall language model is not bad at all! In total, training 2 epochs took 8 hours on a free GPU instance on Google Colaboratory.\nNow that we have this done, why not have some fun and make sure we are doing okay, text-generatio wise? Fast.AI comes with a wonderful learn.predict() function, which in this case can allow us to pass any string of text, and we can query the model for the next few words. Let‚Äôs try the following sentence, along with the next six words: ‚ÄúKim Kardashian released a new photo depicting her doing‚Äù\nlearn.predict(\"Kim Kardashian released a new photo depicting her doing\", n_words=5)\n‚ÄúKim Kardashian released a new photo depicting her doing humanitarian acts with Korean immigrants‚Äù\nInteresting choice of words! This demonstrates that our model has a very basic understand of the fundamental grammer within the corpus language. Now that we have this, we can work on our classification model next.\nFor this, we will need a TextClasDatabunch, with arguments for vocab, which should be equal to the language model‚Äôs vocabulary.\n\n data_clas = TextClasDataBunch.from_csv(path, ‚ÄòDataset_clean.csv‚Äô, vocab=data_lm.train_ds.vocab, text_cols=‚Äòcontent‚Äô, label_cols=‚Äòtype‚Äô, bs=16) \n\nNext we can create our new Learner object:\n\n learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5) \n\nThis model has an initial encoder of our language model, followed by a classification layer set that will help us determine what class of language words fall into: \nThis new model was then trained for another 3 epochs, with the gradual unfreezing being applied. Afterwards, the overall accuracy of the model was approximately 93.3% on the training set, not bad!\n\nWith the complete model, any application may be built to compute an Eddy Score and provide this information to the user. For the purposes of CodeFest, an interface was built using the Starlette Python library deployed on the simple web deployment platform Render. During competitive demonstration, a more consumer oriented brand was designed and deployed on React.js with a link to download an eventual Chrome Extension. The implementation in the end was quite simple: ask the user to copy and paste their article, pass this selection into the model, and produce an Eddy Score within a few seconds. The score and an associated warning or affirmation regarding the content are then displayed to the user.\nHere is an example of the web-page the team developed for this project:\n\nNow onto some things I learned from this experience and very stressful three days:\nFirst: Make sure when you make your databunch you built it correctly. I made one mistake in the selection process and wound up generating a dictionary of category names‚Ä¶ Oops! (Make sure you‚Äôre using the right index when selecting the data!)\nSecond: While Google Colab is a wonderful free resource, I turned to Paperspace for a Jupyter notebook due to the RAM and GPU requirements this model needed. The language model needs a lot of memory to run, and building the databunch needs a large amount of RAM. It is why my selection of articles was the first 120,000. Anything more and I would run out of memory!\nThird: I learned how to deploy a web-app that was not an image classifier! Thankfully, Fast.AI has a very streamlined process, so I could get it running locally within an hour using the Render guide as a resource.\nFourth: No idea is too complex for you to do. I had never touched a natural language processing model before this, and while I was extremely intimidated, I pushed through.\nOverall, this was a wonderful experience and I am honored to have built such an application with my team.\nI hope you all find this helpful, feel free to find me on the Fast.AI forums[5] with any questions, my handle is muellerzr.\nThank you for reading!\nZachary Mueller\n\nResources/Further Reading\n\n[1] CodeFest [2] ULMFiT Paper [3] LSTM Language Model Paper [4] Fast.AI [5] Fast.AI Forums [6] OpenSources"
  },
  {
    "objectID": "blog/Walkwithfastai.html",
    "href": "blog/Walkwithfastai.html",
    "title": "Announcing Walk with fastai, the missing pieces for success",
    "section": "",
    "text": "Details about my newest course, available at https://thezachmueller.gumroad.com/l/walkwithfastai"
  },
  {
    "objectID": "blog/Walkwithfastai.html#why-do-this-whats-different",
    "href": "blog/Walkwithfastai.html#why-do-this-whats-different",
    "title": "Announcing Walk with fastai, the missing pieces for success",
    "section": "Why do this? What‚Äôs different?",
    "text": "Why do this? What‚Äôs different?\nFirst let‚Äôs talk about what‚Äôs new. This course will still take you from someone who either has completed Jeremy Howards course or who hasn‚Äôt even touched fastai at all and take you to a level above most practitioners. We still start at an understanding of some Python, and maybe some PyTorch and take you further beyond.\nHowever it‚Äôs been three years since the first came out and much has changed. New techiques have been released, changes to the framework have occured, and in these times concepts like distributed computing mean much more! As a result over 50% of the course material is completely new.\nBut wait, what about the old material?\nThe old material is also completely redone and added verbose content thanks to the power of Quarto and nbdev. Just look at the before and after:\n\nBefore:\n\n\n\nAfter:\n\nNo longer is this course just a passing resource, but instead it could be read entirely on its own without the accompaning lecture content and be infinitely more valuable than its predecessor."
  },
  {
    "objectID": "blog/Walkwithfastai.html#what-about-the-original-course",
    "href": "blog/Walkwithfastai.html#what-about-the-original-course",
    "title": "Announcing Walk with fastai, the missing pieces for success",
    "section": "What about the original course?",
    "text": "What about the original course?\nThe original course will always be free and a readily-available resource to all, however its content will not be updated for latest iterations of the framework and so forth. The YouTube videos will not be going anywhere, and the code repository wil still be publically available. Always.\nThat being said, this new version of the course will go much futher beyond than the original did, and has enough material to have it stand out as its own and not just be a complete repeat of what was presented three years ago."
  },
  {
    "objectID": "blog/Walkwithfastai.html#not-convinced-heres-what-others-have-to-say-about-the-original-course",
    "href": "blog/Walkwithfastai.html#not-convinced-heres-what-others-have-to-say-about-the-original-course",
    "title": "Announcing Walk with fastai, the missing pieces for success",
    "section": "Not convinced? Here‚Äôs what others have to say about the original course:",
    "text": "Not convinced? Here‚Äôs what others have to say about the original course:\n\nZach Mueller is probably the most active member of the fastai community. He is also very actively involved with fastai development. If there is one person who understands the fastai library through and through (apart from Jeremy Howard and Sylvain Gugger), that is certainly Zach! He went out of his way to create a fabulous learning resource for us, Walk With Fastai. Numerous notebooks explaining how to work with the library to achieve wonderful results. A must read! - Radek Osmulski, Senior Data Scientist at NVIDIA AI\n\n\nWalk with Fastai is the best source available for utilizing fastai and goes above and beyond what is available in the documentation. Zach somehow extracted all of the knowledge I wanted to know about fastai and combined it into blog posts, videos, and source code that are easy to follow and build on. If you want to use fastai, this is a great resource to reference. I‚Äôve had to go back and forth a couple of times between the best source (between it and fastbook) and I think I‚Äôve gotten more out of Walk with fastai. - Kevin Bird\n\n\nI really enjoyed the vision section Zach‚Äôs Walk with fastai series. His extension of the fastai keypoint example from a single point to multipoint was really great and importantly included covering the basics of doing ML well such as cleaning the data, ensuring the transforms make sense and so forth before starting any modelling - super critical to teaching deep learning. I loved that he then goes deeper and shows off how to customize a UNET for key point regression (because why not!) and really get the most out of the fastai API. Highly recommended as an accompaniment to the fastai course! - Morgan McGuire, Growth ML Engineer at Weights and Biases\n\n\nYour addendum course videos in Walk with fastai is so super useful. I‚Äôve learnt so much more about using fastai effectively in new problem spaces by combining Jeremy‚Äôs lessons with your own, and am still learning as I am still progressing through them. - Nissan Dookeran"
  },
  {
    "objectID": "blog/Walkwithfastai.html#how-do-i-sign-up",
    "href": "blog/Walkwithfastai.html#how-do-i-sign-up",
    "title": "Announcing Walk with fastai, the missing pieces for success",
    "section": "How do I sign up?",
    "text": "How do I sign up?\nPreorders are now live, which you can find here.\nI value your trust in me as a teacher and a creator so if you preorder before the course begins you will be able to not only attend the lectures live and directly as me questions, but you will also get weekly updates from me detailing progress, teases, and overall how the course is coming along.\nIf you do not preorder you will receive access to the material as a lesson finishes, within a day for the unedited lecture and within a week for the edited ones.\nAlso starting today preorders are now 50% off, so be sure to get them while they‚Äôre available!"
  },
  {
    "objectID": "blog/NLP-Capstone.html",
    "href": "blog/NLP-Capstone.html",
    "title": "Capstone Project - Revisiting IMDB",
    "section": "",
    "text": "Very recently, Rachel Thomas and Jeremy Howard released a new course focused on Natural Language Processing. I had stated previously in my blogs I wanted to document my progress and come up with a ‚ÄúCapstone‚Äù project for the course. I have been quiet these past few weeks due to my busy schedule, and I have not had the time to really focus on these blogs. As such, here is the capstone project, along with where the new NLP course fits in, and what those practices are."
  },
  {
    "objectID": "blog/NLP-Capstone.html#overall-how-it-works",
    "href": "blog/NLP-Capstone.html#overall-how-it-works",
    "title": "Capstone Project - Revisiting IMDB",
    "section": "Overall How it Works",
    "text": "Overall How it Works\nFor those unfamiliar with the methodology, fastai popularized and engineered the ULM-FiT model, or Universal Language Model Fine-tuning. In it, we first train a language model that is pre-trained on our initial language. In our case, this looks like the following:\n- Take an English model trained on WikiText103 - Train this model further on our corpus (our overall language) - Use that model as an embedding for the classification model\nThis is how our general overview for our training will do, with a few exceptions. We will be training the language model on everything we have available to use. In terms of IMDB, there is a folder with more than twice the amount of data for unsupervised text."
  },
  {
    "objectID": "blog/NLP-Capstone.html#whats-new",
    "href": "blog/NLP-Capstone.html#whats-new",
    "title": "Capstone Project - Revisiting IMDB",
    "section": "What‚Äôs New?",
    "text": "What‚Äôs New?\n\nBackwards\nThe first new state-of-the-art practice that was taught in the course is backwards models. Essentially the language model learns from backwards word orders. For example, take the following sentence: ‚ÄúHe went to the beach.‚Äù Our new language model would instead be fed: ‚Äúbeach the to went He‚Äù (after tokenization and other data preparation). What Jeremy described we could do from here is something called an ensemble, where we take two different models that were trained for the same task, average their predictions, and we can generally perform better than either one individually.\nTo utilize this feature, when we create a databunch we include backwards=True the following for both our language databunch and the classifier databunch:\ndata = (TextList.from_folder(path)\n        .split_by_rand_pct(0.1, seed=42)\n        .label_for_lm()\n        .databunch(bs=128, num_workers=4, backwards=True))\n\n\nTokenization\nAlong with a new model to train, Jeremy and the folks at fastai integrated SentencePiece into the library. Before, fastai only supported Spacy tokenization and so this was the benchmark used for the IMDB Movie Review sentiment analysis problem. To utilize this, we need to first be on the most recent version of the fastai library.\nWhen we want to declare a tokenizer, we add it to that initial TextList.from_ call as a processor. For example, here is what Spacy‚Äôs tokenization looks like (it is used automatically if nothing is passed in):\ntokenizer = Tokenizer(SpaceTokenizer, 'en')\nprocessor = [TokenizeProcessor(tokenizer=tokenizer), \n             NumericalizeProcessor(max_vocab=30000)]\nThis creates a processor that will tokenize our items and then numericalize each of those, or map each token to a number.\nTo use SentencePiece, it is a quick one-liner, plus OpenFileProcessor (used to read in the text from files):\nprocessor = [OpenFileProcessor(), SPProcessor()]\n\ndata = (TextList.from_folder(path, processor=processor)\n        .split_by_rand_pct(0.1, seed=42)\n        .label_for_lm()\n        .databunch(bs=128, num_workers=4, backwards=True))\nAnd now we are using SentencePiece!"
  },
  {
    "objectID": "blog/NLP-Capstone.html#the-capstone-project",
    "href": "blog/NLP-Capstone.html#the-capstone-project",
    "title": "Capstone Project - Revisiting IMDB",
    "section": "The Capstone Project",
    "text": "The Capstone Project\nThe project itself is based on an idea Jeremy discussed in the lectures: what if someone were to try to utilize an ensemble of both a fowards and backwards model, trained twice on both SentencePiece and on Spacy. His theory is that there could be very-close-to-if-not state of the art results. So that is our goal. We will create four models, utilizing both tokenizers and model-functionalities.\n\nThe Language Model\nFor the langauge model, here is how the four databunches were generated: (for nomenclature sake, each databunch and model will have the following: x_y_z_a where x is either data or learn, y is either lm or cls, z is either spy or spp, and a is either forwards or backwards)\ndata_lm_spp_fwd = (TextList.from_folder(path, processor=[OpenFileProcessor(), SPProcessor()])\n                  .split_by_rand_pct(0.1, seed=42)\n                  .label_for_lm()\n                  .databunch(bs=128, num_workers=4, backwards=False))\n                  \ndata_lm_spp_bwd = (TextList.from_folder(path, processor=[OpenFileProcessor(), SPProcessor()])\n                  .split_by_rand_pct(0.1, seed=42)\n                  .label_for_lm()\n                  .databunch(bs=128, num_workers=4, backwards=True))\n\ndata_lm_spy_fwd = (TextList.from_folder(path)\n                  .split_by_rand_pct(0.1, seed=42)\n                  .label_for_lm()\n                  .databunch(bs=64, num_workers=4, backwards=False))\n\ndata_lm_spy_bwd = (TextList.from_folder(path)\n                  .split_by_rand_pct(0.1, seed=42)\n                  .label_for_lm()\n                  .databunch(bs=64, num_workers=4, backwards=True))\nOne thing to note here is the batch size difference. When I trained on my 1060, I noticed that I could push double the batches using SentencePiece than with Spacy, leading me to believe it is more efficient GPU wise.\nFrom here, I generated our typical learners while also utilizing Mixed Precision to help get the most out of my CUDA cores. This has been seen to reduce training time astronomically in some cases, especially with language models. We can apply this to our models with to_fp16()\nlearn_lm_spy_fwd = language_model_learner(data_lm_spy_fwd, AWD_LSTM, drop_mult=1.).to_fp16()\nlearn_lm_spy_bwd = language_model_learner(data_lm_spy_bwd, AWD_LSTM, drop_mult=1.).to_fp16()\nlearn_lm_spp_fwd = language_model_learner(data_lm_spp_fwd, AWD_LSTM, drop_mult=1.).to_fp16()\nlearn_lm_spp_bwd = language_model_learner(data_lm_spp_bwd, AWD_LSTM, drop_mult=1.).to_fp16()\nFrom here, each model was trained in the same fashion using the following function:\ndef train_spy(models:list):\n    names = ['fwd', 'bwd']\n    x = 0\n    for model in models:\n        lr = 1e-2\n        lr *= 64/48\n        \n        model.fit_one_cycle(1, lr, moms=(0.8,0.7))\n        model.unfreeze()\n        model.fit_one_cycle(10, lr/10, moms=(0.8,0.7))\n        \n        model.save(f'spp_{names[x]}_fine_tuned_10')\n        model.save_encoder(f'spp_{names[x]}_fine_tuned_enc_10')\n    return models\nEach language model was trained for 11 epochs, with each achieving roughly 33% by the end. Jeremy‚Äôs rule of thumb for language models is regardless of the original language, if you have 30%, you‚Äôre good to move on.\nOne small note, when I tested this initially with IMDB_SAMPLE, I found that I could not get the language model with SentencePiece to fully train for a large number of epochs when compared to Spacy, whereas the full dataset I saw little differentiation. I believe SentencePiece needs more data as a minimum to train on than Spacy.\nFor every language model, after 11 epochs their accuracy was 33.93%.\nThe Spacy epochs were each an average of 21:39 minutes, whereas the SentencePiece epochs were an average of 11:04 minutes.\n\n\nThe Classifier\nNow for the main tasks at hand, building the sentiment analysis classifier. Here I wound up having GPU memory issues, which lead to a much longer training time, and I also could not get to_fp16() to work, so I could not take advantage of Mixed Precision.\nEach model had a batch size of 8 and was trained for 5 epochs total, with an initial learning rate of 2e-2, before degrading from there using the following function:\nres = []\ntargs = []\nfor learn in learns:\n    learn.fit_one_cycle(1, lr, moms=(0.8,0.7))\n    learn.freeze_to(-2)\n    learn.fit_one_cycle(1, slice(lr/(2.6**4), lr), moms=(0.8,0.7))\n    learn.freeze_to(-3)\n    learn.fit_one_cycle(1, slice(lr/2/(2.6**4), lr/2), moms=(0.8,0.7))\n    learn.unfreeze()\n    learn.fit_one_cycle(2, slice(lr/10/(2.6**4), lr/10), moms=(0.8,0.7))\n    \n    preds, targ = learn.get_preds(ordered=True)\n    res.append(preds)\n    targs.append(targ)"
  },
  {
    "objectID": "blog/NLP-Capstone.html#results",
    "href": "blog/NLP-Capstone.html#results",
    "title": "Capstone Project - Revisiting IMDB",
    "section": "Results",
    "text": "Results\nTo gather the results of the ensemble, I took the raw predictions and averaged them all:\npreds_avg = (res[0] + res[1] + res[2] + res[3])/4\naccuracy(preds_avg, targs[0])\nThis ensembled accuracy was 94.94%. Jeremy et al‚Äôs paper shows they achieved 95% accuracy, so we did not quite achieve what they got but we were close. But let‚Äôs consider it from a different standpoint. How much improvement was adding the SentencePiece and the forwards and backwards models together? The table below compares those results:\n\n\n\nName\nAccuracy\n\n\n\n\nSpacy Forward\n94.49%\n\n\nSpacy Forward and Backwards\n94.77%\n\n\nSentencePiece Forward\n94.55%\n\n\nSentencePiece Forward and Backwards\n94.66%\n\n\nSpacy and SentencePiece Forward\n94.86%\n\n\nSpacy and SentencePiece Backwards\n94.79%\n\n\nSpacy Forward and Backward and SentencePiece Forward\n94.89%\n\n\nSpacy Forward and Backward and SentencePiece Backwards\n94.88%\n\n\nSpacy and SentencePiece Forward and Backwards\n94.94%\n\n\n\nSo now let‚Äôs compare. We can see that compared to the test of a Spacy Model alone forwards and backwards, a SentencePiece model is pretty close, achieving 1/10th of a percent below. But when we start ensembling the various models together, we saw an improvement of 0.17%. While this may seem negligible, think of an error rate in a realistic scope. For every 1,000 reviews, we classify 17 more correctly now."
  },
  {
    "objectID": "blog/NLP-Capstone.html#closing-thoughts",
    "href": "blog/NLP-Capstone.html#closing-thoughts",
    "title": "Capstone Project - Revisiting IMDB",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nFirst, I know this can be further improved. Jeremy et al‚Äôs paper shows they achieved 95% accuracy with Spacy, something I could not quite match. I believe I‚Äôm missing something and I need to look at what.\nSecond, while the results seem negligibly better, I believe they are telling. I tried comparing when either the forwards or the backwards model was there as well, and there was a stark increase in accuracy when comparing them, putting merit to the thought this setup can achieve a new state of the art.\nI want to revisit this after some talk on the fastai forums as to exactly what I may be missing within my training when compared with the paper and run again.\nAll the source code is available at my Github here\nThanks for reading!"
  },
  {
    "objectID": "blog/happiness.html",
    "href": "blog/happiness.html",
    "title": "How to be happy, or ways to try your best",
    "section": "",
    "text": "For a while now I‚Äôve been going back to two particular studies that stuck with me. They provide guidelines on scientifically acknowledged ways to be happy. I‚Äôve been trying to follow them, and this year I plan to try even more of them.\nThis article will focus on these two studies, what I believe are valuable from them, and how I plan to implement them into my life.\n\n\n\nCheck out this table, and these papers\n\n\n\nGiven a pool of 20 or so experts, the study asked them to think of ways to be happier. But even further, they were asked to focus on methods that increased happiness in the context of life satisfaction.\n\nLife satisfaction means your overall enjoyment of life as a whole.\n\nSpecifically they were asked two questions:\n\nWhat policies are most likely to yield greater happiness for a greater number of cities in nations\n\n\nWhat individual strategies are most likely to enhance people‚Äôs happiness in the long run\n\nThis article will focus solely on the latter portion of the questions.\nThese results were ranked based on effectiveness, cost-effectiveness, and feasability.\nNow that you have the background, let‚Äôs go into the results. I‚Äôve split the ones that I personally have highlighted into different groupings based on their life category.\n\n\nThe first on the list in financial was investing in experiences. Experiences don‚Äôt need to be big or fancy, like a vacation overseas or a trip to a theme park. I‚Äôve found that smaller, simpler experiences to invest in are just as good. For example, I had a routine of going to the aquarium, a local bar for dinner, and finally sometimes a concert once a week or so for a few months. Just getting out of the house and being around the aquarium (something I‚Äôm very passionate about and love) was phenominal. Mix that in with memories of good food and a great concert, and it‚Äôs something I cannot wait to do again once concert season hits.\nThe second is enrolling in automatic savings. The paper discusses how monitary items don‚Äôt really attribute towards overall happiness, with savings being the one exception to the list. Due to the fact that you can safeguard emergencies, future large purchases, and retirement, it makes sense that it would be ranked highly on their list. If you don‚Äôt do this, the rule of thumb I grew up with is to save at least 10% of your income if you can. I need to do more of this myself.\n\n\n\nThese next options are more feasible than the first for many. They involve taking care of your physical body without a ‚Äúmagic solution.‚Äù\nThe first was to get physical exercise. Other studies have shown that exercise releases endorphins, which are chemicals that improve your mood and in those with ADHD it reduces symtoms. I can personally attest to that as well. This can come in many different forms, such as the gym or going for a walk. However I will also mention another study which discussed how relative happiness was seen when compared to the length of time someone spends outside a week. While there was a cliff, it was generally found that going outside around 120 minutes a week was the sweet spot for happiness. This can be done by going for a walk, going to the park, or even just sitting outside. After reading these papers I decided to add a 30 minute walk to my daily routine in a spot completely devoid of urban life and it was an excellent way for me to start my day mentally and I anecdotally felt happier. I also have noticed the lack of it as the colder months have come and I‚Äôve been less inclined to go outside for a walk and instead head directly to the gym.\nThe second was to check your health. This includes regular visits to the doctor, going to the dentist, and talking to a therapist. Post-pandemic I need to do this much more often myself, and I intend to do so in the 2023 year. Take care of your body and it will return the favor tenfold.\nThe last one was regular and ample sleep. Studies have also shown that aligning your circatium rythum to the sun can have increased benefits as well. To help with this I use the Android ‚ÄúSleep‚Äù app which tracks my sleep and let‚Äôs me set alarms based on time fully asleep. As a result my average sleep over the last few months has been a proper 7-8 hours of sleep, rather than my 5-6 hours I used to get. I haven‚Äôt tried aligning my circatium rythum to the sun yet, but I plan to do so for three months to see how it compares to my current sleep schedule.\n\n\n\nThis category blends in with a few of the previous ones in their recommendations, compounding their importance.\nTwo of which were experience nature and maximize sunlight. If you‚Äôre following these tips, then in my opinion you‚Äôve already succeeded in these two. If you have a west-facing window it may be good to spend more time there to help with ‚Äúmaximizing sunlight‚Äù. I personally work out of a cave (den) as my office, so my new apartment I plan on looking for one with more windows to help with it. I also plan on trying bonsai trees and other plants to help more with the ‚Äúexperience nature‚Äù aspect.\nThe last one is to optimize your bedroom for sleep. The Sleep Foundation states that the following are some of the best recommendations for a good night‚Äôs sleep:\n\nKeep your bedroom at 65 degrees Fahrenheit (18 degrees Celsius) if possible. A healthy adult will have their core temperature drop when they sleep, so lowering the temperature at night helps induce this. A range between 60 and 71 should be suitable for most people.\nMinimizing the noise in your room is their second recommendation. Strive to keep the bedroom as quiet as possible.\nKeep as much natural lighting as possible. Exposure to artificial lighting in the evening can delay circadian rhythms and make it harder to fall asleep. A lux of 10 or higher in the evening leads to a more restless sleep.\n\n\n\n\nThe life style category is aimed at helping your overall lifestyle be happy and healthy.\nThe first of which is to be active physically and mentally. As before, being active and going for walks every day, working out, or bringing some physical routine into your life is recommended. Having a hobby or job that you can mentally work yourself towards is also recommended. I generally do this with my job, but I also have a few hobbies that let me utilize this as well, mostly centered around coding or creating courses.\nThe second is to set goals. I have mixed feelings about this recommendation, because typical ‚Äúgoals‚Äù don‚Äôt work well for me. What I have found works best is to use goals as milemarkers, not end-all-be-all deadlines. Doing this over the last year I‚Äôve found I‚Äôve done more of what I‚Äôve wanted to do without the guilt of not getting something done in time.\nThe third recommendation is to enjoy things. Clear, I know. Generally what this means though is to find something you enjoy, be it a walk in the park, a hobby, or your job. Anything that let‚Äôs you experience happiness as you do it.\nThe final recommendation, and hardest one, is to finda way of life that fits you. This requires a gigantic amount of self-reflection and understanding your own personal values, but is the most worthwhile in the long run. I spent the last year exploring this and to some degree I still am, however actually focusing on just ‚Äúme‚Äù and what I want out of my life has helped align my thoughts and goals for work, hobbies, and relationships.\n\n\n\nFinding meaning in life is also extremely important. But how can you do this? The paper recommends the following three opportunities:\nFirst, be generous. Holding the door open for someone, being kind to everyone you meet, this one is very easy for you to get started today.\nThe second and third recommnedations I will tie together: don‚Äôt seek happiness, seek purpose. Chasing happiness will (in my opinion) leave you forever dissatisfied because you‚Äôre running after an idea rather than something tangeble. It‚Äôs the same thought process as a man can own everything in the world but still not be happy, because experiencing life and people for who they truly are is an immaterial thing. Instead, seek purpose. What is your purpose in life? What do you want to do? What do you want to be? What do you want to accomplish? These are all questions that can help you find meaning in your life and align yourself with your values.\n\n\n\nThese are ways to help you develop your mind and improve your mental health. There were not many under this category, considering much of these are already covered in the other categories. However, there were a few that stood out.\nThe first was to keep learning. Never be satisfied with the knowledge you have, and enter every conversation with the mindset of someone will teach you something new. I‚Äôve constantly done this with the field of tech for the last few years and it‚Äôs kept work extremely enjoyable and entertaining.\nThe second is to seek challenges. Constantly push yourself to do more, to learn more, to be more. Try out that new project, see if you can go for that gold Kaggle medal, or just try to add a new habit to your daily routine. Something that forces you to tackle a new hurdle and be adamant about overcoming it.\n\n\n\nThis next category is all about how you view the world. It‚Äôs about how you view yourself, how you view others, and how you view the world around you. The main one they recommended implementing, which is also single-handedly the hardest on their list, is to accept yourself for who you are. I could tangent on this all day, but the end-all-be-all is to just love yourself. Discover what makes you tick, and acknowledge the strengths and weaknesses you have. Fighting yourself is a tiring battle, and changing yourself dramatically is fighting a flood. Instead, learn about who you are, hone your strengths, and accept your weaknesses.\n\n\n\nHumans are social beings, and we need to be around others to be happy. The paper recommends the following ways to improve your social bonds:\nFirst, invest in your friends and family. Investing doesn‚Äôt mean to spend lavish gifts on them, but instead to spend time with them. Go out to dinner, go for a walk, or just sit down and talk with them. Your time is the investment and everyone knows that your time is a precious resource.\nThe second and third recommendations I again pull together: act nicely and focus on the happiness of others. Going back to the idea of chasing happiness will never let you get there, focusing on the happiness of others instead of your own will provide you with a much more fulfilling life.\n\n\n\nThe last category were items related to work. Overall there weren‚Äôt many recommendations here that actually were shown to make a difference, but the main one is to limit your work hours if possible. Have the seperation between work and life.\nIt‚Äôs also important to know that building wealth was not advised as part of living a happier life. This includes both getting a part-time job and working for yourself. This is mostly because they are ineffective and not very feasable for most individuals. This is counterintuitive to the building of savings earlier mentioned, but I don‚Äôt actually believe so since one is aimed at financial security and the other is adding more work to your life.\n\n\n\n\nHopefully you found this post interesting and useful. Slowly I have been adding more and more of these practices into my life, and so far I have found they only benefit. I wanted a location to keep track of these, and bookmark these two papers, considering I go back to them constantly and thus here we are.\nI wish you luck on your journey to happiness, and hopefully some of these ideas can help you along the way."
  },
  {
    "objectID": "blog/happiness.html#motivation",
    "href": "blog/happiness.html#motivation",
    "title": "How to be happy, or ways to try your best",
    "section": "",
    "text": "For a while now I‚Äôve been going back to two particular studies that stuck with me. They provide guidelines on scientifically acknowledged ways to be happy. I‚Äôve been trying to follow them, and this year I plan to try even more of them.\nThis article will focus on these two studies, what I believe are valuable from them, and how I plan to implement them into my life."
  },
  {
    "objectID": "blog/happiness.html#tldr",
    "href": "blog/happiness.html#tldr",
    "title": "How to be happy, or ways to try your best",
    "section": "",
    "text": "Check out this table, and these papers"
  },
  {
    "objectID": "blog/happiness.html#ways-to-greater-happiness-a-delphi-study",
    "href": "blog/happiness.html#ways-to-greater-happiness-a-delphi-study",
    "title": "How to be happy, or ways to try your best",
    "section": "",
    "text": "Given a pool of 20 or so experts, the study asked them to think of ways to be happier. But even further, they were asked to focus on methods that increased happiness in the context of life satisfaction.\n\nLife satisfaction means your overall enjoyment of life as a whole.\n\nSpecifically they were asked two questions:\n\nWhat policies are most likely to yield greater happiness for a greater number of cities in nations\n\n\nWhat individual strategies are most likely to enhance people‚Äôs happiness in the long run\n\nThis article will focus solely on the latter portion of the questions.\nThese results were ranked based on effectiveness, cost-effectiveness, and feasability.\nNow that you have the background, let‚Äôs go into the results. I‚Äôve split the ones that I personally have highlighted into different groupings based on their life category.\n\n\nThe first on the list in financial was investing in experiences. Experiences don‚Äôt need to be big or fancy, like a vacation overseas or a trip to a theme park. I‚Äôve found that smaller, simpler experiences to invest in are just as good. For example, I had a routine of going to the aquarium, a local bar for dinner, and finally sometimes a concert once a week or so for a few months. Just getting out of the house and being around the aquarium (something I‚Äôm very passionate about and love) was phenominal. Mix that in with memories of good food and a great concert, and it‚Äôs something I cannot wait to do again once concert season hits.\nThe second is enrolling in automatic savings. The paper discusses how monitary items don‚Äôt really attribute towards overall happiness, with savings being the one exception to the list. Due to the fact that you can safeguard emergencies, future large purchases, and retirement, it makes sense that it would be ranked highly on their list. If you don‚Äôt do this, the rule of thumb I grew up with is to save at least 10% of your income if you can. I need to do more of this myself.\n\n\n\nThese next options are more feasible than the first for many. They involve taking care of your physical body without a ‚Äúmagic solution.‚Äù\nThe first was to get physical exercise. Other studies have shown that exercise releases endorphins, which are chemicals that improve your mood and in those with ADHD it reduces symtoms. I can personally attest to that as well. This can come in many different forms, such as the gym or going for a walk. However I will also mention another study which discussed how relative happiness was seen when compared to the length of time someone spends outside a week. While there was a cliff, it was generally found that going outside around 120 minutes a week was the sweet spot for happiness. This can be done by going for a walk, going to the park, or even just sitting outside. After reading these papers I decided to add a 30 minute walk to my daily routine in a spot completely devoid of urban life and it was an excellent way for me to start my day mentally and I anecdotally felt happier. I also have noticed the lack of it as the colder months have come and I‚Äôve been less inclined to go outside for a walk and instead head directly to the gym.\nThe second was to check your health. This includes regular visits to the doctor, going to the dentist, and talking to a therapist. Post-pandemic I need to do this much more often myself, and I intend to do so in the 2023 year. Take care of your body and it will return the favor tenfold.\nThe last one was regular and ample sleep. Studies have also shown that aligning your circatium rythum to the sun can have increased benefits as well. To help with this I use the Android ‚ÄúSleep‚Äù app which tracks my sleep and let‚Äôs me set alarms based on time fully asleep. As a result my average sleep over the last few months has been a proper 7-8 hours of sleep, rather than my 5-6 hours I used to get. I haven‚Äôt tried aligning my circatium rythum to the sun yet, but I plan to do so for three months to see how it compares to my current sleep schedule.\n\n\n\nThis category blends in with a few of the previous ones in their recommendations, compounding their importance.\nTwo of which were experience nature and maximize sunlight. If you‚Äôre following these tips, then in my opinion you‚Äôve already succeeded in these two. If you have a west-facing window it may be good to spend more time there to help with ‚Äúmaximizing sunlight‚Äù. I personally work out of a cave (den) as my office, so my new apartment I plan on looking for one with more windows to help with it. I also plan on trying bonsai trees and other plants to help more with the ‚Äúexperience nature‚Äù aspect.\nThe last one is to optimize your bedroom for sleep. The Sleep Foundation states that the following are some of the best recommendations for a good night‚Äôs sleep:\n\nKeep your bedroom at 65 degrees Fahrenheit (18 degrees Celsius) if possible. A healthy adult will have their core temperature drop when they sleep, so lowering the temperature at night helps induce this. A range between 60 and 71 should be suitable for most people.\nMinimizing the noise in your room is their second recommendation. Strive to keep the bedroom as quiet as possible.\nKeep as much natural lighting as possible. Exposure to artificial lighting in the evening can delay circadian rhythms and make it harder to fall asleep. A lux of 10 or higher in the evening leads to a more restless sleep.\n\n\n\n\nThe life style category is aimed at helping your overall lifestyle be happy and healthy.\nThe first of which is to be active physically and mentally. As before, being active and going for walks every day, working out, or bringing some physical routine into your life is recommended. Having a hobby or job that you can mentally work yourself towards is also recommended. I generally do this with my job, but I also have a few hobbies that let me utilize this as well, mostly centered around coding or creating courses.\nThe second is to set goals. I have mixed feelings about this recommendation, because typical ‚Äúgoals‚Äù don‚Äôt work well for me. What I have found works best is to use goals as milemarkers, not end-all-be-all deadlines. Doing this over the last year I‚Äôve found I‚Äôve done more of what I‚Äôve wanted to do without the guilt of not getting something done in time.\nThe third recommendation is to enjoy things. Clear, I know. Generally what this means though is to find something you enjoy, be it a walk in the park, a hobby, or your job. Anything that let‚Äôs you experience happiness as you do it.\nThe final recommendation, and hardest one, is to finda way of life that fits you. This requires a gigantic amount of self-reflection and understanding your own personal values, but is the most worthwhile in the long run. I spent the last year exploring this and to some degree I still am, however actually focusing on just ‚Äúme‚Äù and what I want out of my life has helped align my thoughts and goals for work, hobbies, and relationships.\n\n\n\nFinding meaning in life is also extremely important. But how can you do this? The paper recommends the following three opportunities:\nFirst, be generous. Holding the door open for someone, being kind to everyone you meet, this one is very easy for you to get started today.\nThe second and third recommnedations I will tie together: don‚Äôt seek happiness, seek purpose. Chasing happiness will (in my opinion) leave you forever dissatisfied because you‚Äôre running after an idea rather than something tangeble. It‚Äôs the same thought process as a man can own everything in the world but still not be happy, because experiencing life and people for who they truly are is an immaterial thing. Instead, seek purpose. What is your purpose in life? What do you want to do? What do you want to be? What do you want to accomplish? These are all questions that can help you find meaning in your life and align yourself with your values.\n\n\n\nThese are ways to help you develop your mind and improve your mental health. There were not many under this category, considering much of these are already covered in the other categories. However, there were a few that stood out.\nThe first was to keep learning. Never be satisfied with the knowledge you have, and enter every conversation with the mindset of someone will teach you something new. I‚Äôve constantly done this with the field of tech for the last few years and it‚Äôs kept work extremely enjoyable and entertaining.\nThe second is to seek challenges. Constantly push yourself to do more, to learn more, to be more. Try out that new project, see if you can go for that gold Kaggle medal, or just try to add a new habit to your daily routine. Something that forces you to tackle a new hurdle and be adamant about overcoming it.\n\n\n\nThis next category is all about how you view the world. It‚Äôs about how you view yourself, how you view others, and how you view the world around you. The main one they recommended implementing, which is also single-handedly the hardest on their list, is to accept yourself for who you are. I could tangent on this all day, but the end-all-be-all is to just love yourself. Discover what makes you tick, and acknowledge the strengths and weaknesses you have. Fighting yourself is a tiring battle, and changing yourself dramatically is fighting a flood. Instead, learn about who you are, hone your strengths, and accept your weaknesses.\n\n\n\nHumans are social beings, and we need to be around others to be happy. The paper recommends the following ways to improve your social bonds:\nFirst, invest in your friends and family. Investing doesn‚Äôt mean to spend lavish gifts on them, but instead to spend time with them. Go out to dinner, go for a walk, or just sit down and talk with them. Your time is the investment and everyone knows that your time is a precious resource.\nThe second and third recommendations I again pull together: act nicely and focus on the happiness of others. Going back to the idea of chasing happiness will never let you get there, focusing on the happiness of others instead of your own will provide you with a much more fulfilling life.\n\n\n\nThe last category were items related to work. Overall there weren‚Äôt many recommendations here that actually were shown to make a difference, but the main one is to limit your work hours if possible. Have the seperation between work and life.\nIt‚Äôs also important to know that building wealth was not advised as part of living a happier life. This includes both getting a part-time job and working for yourself. This is mostly because they are ineffective and not very feasable for most individuals. This is counterintuitive to the building of savings earlier mentioned, but I don‚Äôt actually believe so since one is aimed at financial security and the other is adding more work to your life."
  },
  {
    "objectID": "blog/happiness.html#conclusion",
    "href": "blog/happiness.html#conclusion",
    "title": "How to be happy, or ways to try your best",
    "section": "",
    "text": "Hopefully you found this post interesting and useful. Slowly I have been adding more and more of these practices into my life, and so far I have found they only benefit. I wanted a location to keep track of these, and bookmark these two papers, considering I go back to them constantly and thus here we are.\nI wish you luck on your journey to happiness, and hopefully some of these ideas can help you along the way."
  },
  {
    "objectID": "blog/Decorators.html",
    "href": "blog/Decorators.html",
    "title": "Decorators",
    "section": "",
    "text": "An introduction to decorators including when they can be useful and how they‚Äôre written"
  },
  {
    "objectID": "blog/Decorators.html#tldr",
    "href": "blog/Decorators.html#tldr",
    "title": "Decorators",
    "section": "TL;DR",
    "text": "TL;DR\nHere‚Äôs the basic decorator written in this article (though I recommend still reading and going through it so it makes sense!)\nfrom functools import partial\n\ndef addition_decorator(function:callable = None, verbose:bool=False):\n    \"A simple decorator that will ensure `function` is only called with `function(1,1)`\"\n    if function is None:\n        return partial(addition_decorator, verbose=verbose)\n    def decorator(*args, **kwargs):\n        while True:\n            try:\n                return function(*args, **kwargs)\n            except ValueError as e:\n                is_a = \"`a`\" in e.args[0]\n                a,b = args\n                args = (a-1, b) if is_a else (a, b-1)\n                if verbose: \n                    print(f'Args are now {args}')\n    return decorator\n\n@addition_decorator(verbose=True)\ndef addition(a,b):\n    \"Adds `a` and `b` together\"\n    if a &gt; 1: \n        raise ValueError(\"`a` is greater than 1\")\n    if b &gt; 1: \n        raise ValueError(\"`b` is greater than 1\")\n    return a+b"
  },
  {
    "objectID": "blog/Decorators.html#what-is-a-decorator",
    "href": "blog/Decorators.html#what-is-a-decorator",
    "title": "Decorators",
    "section": "What is a decorator?",
    "text": "What is a decorator?\nWhat is a decorator, and why is it useful?\nA decorator can be thought of as code that ‚Äúwraps‚Äù around other code. It encapsulates it, kind of like a blanket! They‚Äôre extremely helpful for setting up a particular configuration for the ‚Äúdecorated‚Äù (a function that has a decorator) function, or to do some particular behavior when something occurs in the code its wrapped around.\nIn python these are generally denoted with an @ symbol followed by the decorator function name, and these are placed above an actual function."
  },
  {
    "objectID": "blog/Decorators.html#what-is-this-article-going-to-show",
    "href": "blog/Decorators.html#what-is-this-article-going-to-show",
    "title": "Decorators",
    "section": "What is this article going to show?",
    "text": "What is this article going to show?\nI‚Äôm not going to dive into ‚Äúhow to make your own decorators from scratch‚Äù and so forth. There‚Äôs a W3 article on this.\nInstead, we‚Äôre going to focus on writing a decorator given what you‚Äôd find everyone else using, a few applications of them, and how to understand just what a decorator is doing and how it‚Äôs written. What I‚Äôm hoping you will take out of this is patterns when writing decorators so you can understand them easier both from a reading and writing perspective, as well as when debugging functions to see what outside behaviour is being performed"
  },
  {
    "objectID": "blog/Decorators.html#example-case-a-retry-decorator",
    "href": "blog/Decorators.html#example-case-a-retry-decorator",
    "title": "Decorators",
    "section": "Example case: a Retry Decorator",
    "text": "Example case: a Retry Decorator\nThe decorator we‚Äôll focus on is a ‚Äúretry‚Äù loop. Say I have a function and I want to be able to catch if a particular call gets raised while it‚Äôs run.\nFrom a Deep Learning perspective this could be something like a CUDA Out-of-Memory for instance.\nIf this Exception has been raised, I want to be able to run the code again slightly modifying one aspect of it to potentially avoid the error being raised.\nFor this example I‚Äôll make a simplistic addition function that will only be ran if 1+1 is being done as its inputs.\n(Does this make sense in the real world? Probably not. But you can get the simple idea!)\ndef addition(a,b):\n    \"Adds `a` and `b` together\"\n    if a &gt; 1: \n        raise ValueError(\"`a` is greater than 1\")\n    if b &gt; 1: \n        raise ValueError(\"`b` is greater than 1\")\n    return a+b\nNow logically let‚Äôs think of how we‚Äôd want to catch this. We raise the same error type, but how do we know what input to change?\nWe can read Exception.args to get the actual message being sent, and use it to see which argument we should adjust.\nGenerally decorators are written as a function with an inner function. This inner function is what is then called when truly calling the function. Meanwhile the decorator takes the function in as the first parameter:\ndef addition_decorator(function: callable, verbose:bool=False):\n    \"\"\"\n    A simple decorator that will ensure `function` is only called with `function(1,1)`\n    \"\"\"\n    def decorator(*args, **kwargs):\n        # This contains the args and kwargs for our `function`\n        # We then do a `while` loop:\n        while True:\n            try:\n                return function(*args, **kwargs)\n            except ValueError as e:\n                # We can then see if we need to adjust `a` or `b`\n                is_a = \"`a`\" in e.args[0]\n                # and then we adjust the current `args` based on this result:\n                a,b = args\n                # We can also print our attempt here:\n                if verbose:\n                    print(f'Tried to do {a} + {b}, but at least one argument is greater than 1!')\n                if is_a:\n                    if verbose:\n                        print(f'Reducing `a` by 1: {a-1}')\n                    args = (a-1, b)\n                else:\n                    if verbose:\n                        print(f'Reducing `b` by 1: {b-1}')\n                    args = (a, b-1)\n    # Finally we return the inner function! *Very* important!\n    return decorator\nWith this simple decorator, we will continuously loop over and try calling function until both a and b are equal to 1.\nNow how do we actually apply this decorator?\nWe can do this one of two ways. We can ‚Äúwrap‚Äù around the function and call it as a normal function. For example:\nfunc = addition_decorator(addition)\nfunc(2,2)\nNow let‚Äôs pass in verbose=True to see just how it was really called:\nfunc = addition_decorator(addition, verbose=True)\nfunc(2,2)\nYou can see that we continuously reduced an input we passed in and passed it to the function before finally getting the value we want!\nNow how do I write this in such a way that doesn‚Äôt require me to build this new function func and call it? How can I just call addition and still have addition_decorator?\nFirst we would declare addition_decorator, and then add @addition_decorator to the top of our addition function like below:\n@addition_decorator\ndef addition(a,b):\n    \"Adds `a` and `b` together\"\n    if a &gt; 1: \n        raise ValueError(\"`a` is greater than 1\")\n    if b &gt; 1: \n        raise ValueError(\"`b` is greater than 1\")\n    return a+b\nNow when we see how addition is declared we can see it points to the decorator:\naddition\nAnd when we call addition it will have the same effect:\naddition(2,2)\nSo how is it doing this?\nBy wrapping our function it uses that function to fill in the first parameter in our decorator by itself! This leaves the rest of them to their default values however."
  },
  {
    "objectID": "blog/Decorators.html#making-it-a-bit-more-complex-passing-in-arguments",
    "href": "blog/Decorators.html#making-it-a-bit-more-complex-passing-in-arguments",
    "title": "Decorators",
    "section": "Making it a bit more complex, passing in arguments",
    "text": "Making it a bit more complex, passing in arguments\nSo how could we configure that verbose argument then?\nHere‚Äôs where a bit of magic comes in through partial functions.\nPartial‚Äôs allow us to create a loaded function with some values already filled in for us by default. This makes use of the functools library. First we‚Äôll write a small function to test our point:\ndef subtraction(a,b): \n    \"Subtract two numbers\"\n    return a-b\nNext we‚Äôll import partial and create our partial version of subtraction. For any values you wish to fill you should pass them as keyword arguments:\nfrom functools import partial\npartial_subtraction = partial(subtraction, a=2)\nNow if I call partial_subtraction, I can just pass in b and it will work:\npartial_subtraction(b=1)\n\n\nWe can perform a similar idea for our decorator, where we return a partial function first filling in our values that we want if `function` is `None`.\n\nThe reason for this is the decorator gets applied *before* the function is called within it, so we setup the parameters we want first. I'll print out when this occurs in our decorator as well so you can see this behavior, and I've also simplified it to show the point I'm trying to visualize for you:\n\n&gt; Note: Since everything can now be passed in as kwargs, each value in the function parameters **must** contain a default of some sort. Usually this would be `None`, which I've done here\n\n```python\ndef addition_decorator(function:callable = None, verbose:bool=False):\n    \"A simple decorator that will ensure `function` is only called with `function(1,1)`\"\n    if function is None:\n        # We return our original function with the `verbose` param\n        print(f'Creating a new `addition_decorator` function with verbose={verbose}')\n        return partial(addition_decorator, verbose=verbose)\n    def decorator(*args, **kwargs):\n        while True:\n            try:\n                return function(*args, **kwargs)\n            except ValueError as e:\n                is_a = \"`a`\" in e.args[0]\n                a,b = args\n                if is_a:\n                    if verbose:\n                        print(f'Reducing `a` by 1: {a-1}')\n                    args = (a-1, b)\n                else:\n                    if verbose:\n                        print(f'Reducing `b` by 1: {b-1}')\n                    args = (a, b-1)\n    # Finally we return the inner function! *Very* important!\n    return decorator\nTo add this parameter, we pass it into the @ function itself. Once we‚Äôve declared our addition function, we should see a print statement immediatly:\n@addition_decorator(verbose=True)\ndef addition(a,b):\n    \"Adds `a` and `b` together\"\n    if a &gt; 1: \n        raise ValueError(\"`a` is greater than 1\")\n    if b &gt; 1: \n        raise ValueError(\"`b` is greater than 1\")\n    return a+b\nAnd now if we call addition:\naddition(2,2)\nWe can see that it prints out the same information we had earlier!\nYou now know almost enough to be on your way with decorators"
  },
  {
    "objectID": "blog/Decorators.html#the-most-extreme-example-with-nonlocal.",
    "href": "blog/Decorators.html#the-most-extreme-example-with-nonlocal.",
    "title": "Decorators",
    "section": "The most extreme example with nonlocal.",
    "text": "The most extreme example with nonlocal.\nI‚Äôm going to provide a real example of what using nonlocal can actually provide for you and perform (that weird python thing no one really does?)\nThe choice to explain it like this is simply because coming up with a similar situation is something I can‚Äôt quite think of well, and this serves as a good example.\nThis example is that ‚Äúcuda out of memory‚Äù I showed earlier in accelerate\nThe API can be thought of as so:\n\nWrite a training function that takes a batch_size as the first argument\nDecorate this training function with the find_executable_batch_size decorator\nHave it continuously try and run, and if cuda OOM is hit, retry the loop by reducing the batch size in half.\n\nLet‚Äôs see how this is implemented: &gt; Note: this will be a simplified version of the official decorator for teaching purposes\nimport gc, torch\n\ndef find_executable_batch_size(function:callable = None, starting_batch_size:int = 128):\n    \"\"\"\n    Decorator that will attempt to execute `function` with `starting_batch_size`.\n    If CUDA Out-of-Memory is reached, the batch size is reduced in half and tried again.\n\n    `function` must take in `batch_size` as its first parameter\n    \"\"\"\n    if function is None:\n        return partial(find_executable_batch_size, starting_batch_size=starting_batch_size)\n    \n    # Keep a `batch_size` variable that gets updated and modified\n    batch_size = starting_batch_size\n\n    def decorator(*args, **kwargs):\n        # Bring it into context\n        nonlocal batch_size\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        while True:\n            if batch_size == 0:\n                raise RuntimeError(\"No executable batch size found\")\n            try:\n                return function(batch_size, *args, **kwargs)\n            except RuntimeError as e:\n                if \"CUDA out of memory\" in e.args[0]:\n                    # We reduce the batch size and clear the memory\n                    gc.collect()\n                    torch.cuda.empty_cache()\n                    batch_size //= 2\n                else:\n                    # Otherwise raise the original error\n                    raise\n\n    return decorator\nHere we make use of our starting_batch_size and use it as a parameter that constantly changes and adapts based on what happened in our try/except loop below."
  },
  {
    "objectID": "blog/Decorators.html#conclusion",
    "href": "blog/Decorators.html#conclusion",
    "title": "Decorators",
    "section": "Conclusion",
    "text": "Conclusion\nHopefully this gave you a better insight into decorators some! My next article will be discussing context managers and when you should do one vs the other. Thanks for reading!"
  },
  {
    "objectID": "blog/dear-me-2022.html",
    "href": "blog/dear-me-2022.html",
    "title": "Dear Me of 2022",
    "section": "",
    "text": "It‚Äôs been a bit. 2022 was scary, intimidating, but you made it through.\nI know we were quite worried about what would happen after graduating. Were things going to work out still at Hugging Face? Would we still be in Maryland?\nWe did. And so much more.\nYou found yourself some amazing coworkers, and now your manager is even one of the people you looked up to at the start of your whole Deep Learning journey. Sounds scary but honestly he‚Äôs really chill and it‚Äôs great.\nOh. Hey. Remember how your friend kept saying you totally have ADHD? Yeah, we do. Both kinds, as it comes to find out. But you found a really good place to go and you‚Äôre able to do so much more.\nThe music even stops now!\nYour coworkers and team have also been super supportive about it. Honestly, the entire company your with has changed your life. You have most of the live you‚Äôve dreamed years for.\nYou also get past your frustrationgs with Walk with fastai and decide to do it again. It‚Äôs actually just about to start, I‚Äôm working on finishing the last few lessons currently!\nThere‚Äôs tons of people signed up who believe in you, so you‚Äôre doing absolutely fantastic. I promise.\nYou‚Äôre getting closer and closer towards finding ‚Äúyourself‚Äù finally. If I were to sum up the entire year into one idea, it‚Äôd be that. It‚Äôs okay to obsess, it‚Äôs okay to have fun. These are parts of you and all are lovely. unironically here Zach.\nWe did not only our first, but second trip out of the country this year! You finally went to Paris! Though I can‚Äôt say I recommend going around Bastille day, far too many people crowded around a tiny block in Paris.\nYou were also able to go to Australia and meet all of your Deep Learning idols again. Jeremy helped you pick out yet another food, this time meat pies. You also finally met Sanyam, Radek, Wasim, and Hamel. Such amazing people gathered in one location, the ideas completely flurshed and that‚Äôs why you‚Äôre doing this course again.\nWe also went to Steve Irwin‚Äôs zoo. I didn‚Äôt know you could pet wallaby‚Äôs but I guess in Australia they‚Äôre kind of like our chickens in that way. And of course we went scuba diving too. Almost every single fish we used to have in the old life we saw there, and lots of our corals. One day let‚Äôs try to setup the aquarium again but do it right. Fully automated, full-proof, and maybe a hint of DL mixed in eh?\nI‚Äôm sitting here a year later talking to you and honestly I still cannot believe the life we have. We are very lucky to be here. We can still visit our family at Christmas, though that had its own fun (thanks Southwest). We saw everyone in Syracuse multiple times. And next year we‚Äôre only going to do more.\nKeep pushing forward and trust yourself. You‚Äôre on a great path that no one, not even I, can predict where it ends. Remember that it‚Äôs okay to take breaks and experience life, not everything is about the next paycheck, the next loan paid off, the next bill. Go have fun: see those concerts, go to the aquarium, cherish time with your friends. Those too are all critical parts of life.\nAnd it‚Äôs okay to not date anyone right now, nor want to. You‚Äôve still got much to figure out and learn about yourself, so don‚Äôt feel rushed. Take your journey slow and at your own pace. It still hasn‚Äôt ended, but that‚Äôs not bad. We‚Äôre doing just fine on our own and happy!\nBe happy, be confident, and be kind. Life is even better after college, and it‚Äôs everything you could have wanted. So go, live in it. Embrase it. Give it as much as you can, and it will return the favor tenfold.\nLove,\nZach"
  },
  {
    "objectID": "blog/dear-me-2022.html#hey-you",
    "href": "blog/dear-me-2022.html#hey-you",
    "title": "Dear Me of 2022",
    "section": "",
    "text": "It‚Äôs been a bit. 2022 was scary, intimidating, but you made it through.\nI know we were quite worried about what would happen after graduating. Were things going to work out still at Hugging Face? Would we still be in Maryland?\nWe did. And so much more.\nYou found yourself some amazing coworkers, and now your manager is even one of the people you looked up to at the start of your whole Deep Learning journey. Sounds scary but honestly he‚Äôs really chill and it‚Äôs great.\nOh. Hey. Remember how your friend kept saying you totally have ADHD? Yeah, we do. Both kinds, as it comes to find out. But you found a really good place to go and you‚Äôre able to do so much more.\nThe music even stops now!\nYour coworkers and team have also been super supportive about it. Honestly, the entire company your with has changed your life. You have most of the live you‚Äôve dreamed years for.\nYou also get past your frustrationgs with Walk with fastai and decide to do it again. It‚Äôs actually just about to start, I‚Äôm working on finishing the last few lessons currently!\nThere‚Äôs tons of people signed up who believe in you, so you‚Äôre doing absolutely fantastic. I promise.\nYou‚Äôre getting closer and closer towards finding ‚Äúyourself‚Äù finally. If I were to sum up the entire year into one idea, it‚Äôd be that. It‚Äôs okay to obsess, it‚Äôs okay to have fun. These are parts of you and all are lovely. unironically here Zach.\nWe did not only our first, but second trip out of the country this year! You finally went to Paris! Though I can‚Äôt say I recommend going around Bastille day, far too many people crowded around a tiny block in Paris.\nYou were also able to go to Australia and meet all of your Deep Learning idols again. Jeremy helped you pick out yet another food, this time meat pies. You also finally met Sanyam, Radek, Wasim, and Hamel. Such amazing people gathered in one location, the ideas completely flurshed and that‚Äôs why you‚Äôre doing this course again.\nWe also went to Steve Irwin‚Äôs zoo. I didn‚Äôt know you could pet wallaby‚Äôs but I guess in Australia they‚Äôre kind of like our chickens in that way. And of course we went scuba diving too. Almost every single fish we used to have in the old life we saw there, and lots of our corals. One day let‚Äôs try to setup the aquarium again but do it right. Fully automated, full-proof, and maybe a hint of DL mixed in eh?\nI‚Äôm sitting here a year later talking to you and honestly I still cannot believe the life we have. We are very lucky to be here. We can still visit our family at Christmas, though that had its own fun (thanks Southwest). We saw everyone in Syracuse multiple times. And next year we‚Äôre only going to do more.\nKeep pushing forward and trust yourself. You‚Äôre on a great path that no one, not even I, can predict where it ends. Remember that it‚Äôs okay to take breaks and experience life, not everything is about the next paycheck, the next loan paid off, the next bill. Go have fun: see those concerts, go to the aquarium, cherish time with your friends. Those too are all critical parts of life.\nAnd it‚Äôs okay to not date anyone right now, nor want to. You‚Äôve still got much to figure out and learn about yourself, so don‚Äôt feel rushed. Take your journey slow and at your own pace. It still hasn‚Äôt ended, but that‚Äôs not bad. We‚Äôre doing just fine on our own and happy!\nBe happy, be confident, and be kind. Life is even better after college, and it‚Äôs everything you could have wanted. So go, live in it. Embrase it. Give it as much as you can, and it will return the favor tenfold.\nLove,\nZach"
  },
  {
    "objectID": "blog/CAMVID.html",
    "href": "blog/CAMVID.html",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "",
    "text": "Exploring how baselines are being made and where fastai can fit in\nNote: this blog is the result of joint efforts between myself and Juvian on the forums"
  },
  {
    "objectID": "blog/CAMVID.html#the-metric",
    "href": "blog/CAMVID.html#the-metric",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "The Metric:",
    "text": "The Metric:\nFirst, the reported metrics are different. Instead of accuracy the Mean Intersection Over Union (mIOU) is reported, as well as individual IOU‚Äôs per class"
  },
  {
    "objectID": "blog/CAMVID.html#the-number-of-classes",
    "href": "blog/CAMVID.html#the-number-of-classes",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "The Number of Classes:",
    "text": "The Number of Classes:\nIn the original fastai version of the dataset, 31 classes are present, with an additional void classes that is ignored in the resulting benchmarks.\nResearchers have since changed this class distribution to 11 total classes: Building, Tree, Sky, Car, Sign, Road, Pedestrian, Fence, Pole, Sidewalk, and Cyclist, with one more twelveth void class that is again, not taken into account.\nThis change in classes allows for a higher mIOU being reported without having the rarely-seen classes scew the results, so if you were running mIOU on the class notebooks and getting ~20% and being confused why it doesn‚Äôt align, this is why!"
  },
  {
    "objectID": "blog/CAMVID.html#the-splits",
    "href": "blog/CAMVID.html#the-splits",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "The Splits",
    "text": "The Splits\nWhen we train with fastai, we wind up mixing in the baseline evaluation dataset with the training data! Not something we want at all! The train/validation/test split in most papers tends to be: 367/101/233. That is correct, there is two-times as many test images as there are validation."
  },
  {
    "objectID": "blog/CAMVID.html#the-segnet-version",
    "href": "blog/CAMVID.html#the-segnet-version",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "The SegNet Version",
    "text": "The SegNet Version\nThis is a version that has images and labels coming in at a size of 360x480 pixels, which is half the size of what fastai‚Äôs source dataset is, but has its labels with the 11 classes. What is different paper to paper however is how they use the dataset, which can lead to issues. Let‚Äôs look at the current options and their pros/cons:"
  },
  {
    "objectID": "blog/CAMVID.html#using-the-segnet-dataset",
    "href": "blog/CAMVID.html#using-the-segnet-dataset",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "Using the SegNet Dataset:",
    "text": "Using the SegNet Dataset:\nIf we decide to use only this dataset, there is not much room for fastai‚Äôs tricks (such as progressive resizing and Pre-Sizing). That being said, there are papers which use this. If you look on the CAMVID leaderboard however, you‚Äôll notice the best model placed at 8th. So what‚Äôs next?"
  },
  {
    "objectID": "blog/CAMVID.html#well-what-is-the-sota-were-comparing-against-then",
    "href": "blog/CAMVID.html#well-what-is-the-sota-were-comparing-against-then",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "Well, what is the SOTA we‚Äôre comparing against then?",
    "text": "Well, what is the SOTA we‚Äôre comparing against then?\nWhile below is a benchmark, we can‚Äôt truly compare against it. However, if we wish to, we will be focusing on the models that have an ImageNet backbone:"
  },
  {
    "objectID": "blog/CAMVID.html#using-the-fastai-images-with-smaller-labels",
    "href": "blog/CAMVID.html#using-the-fastai-images-with-smaller-labels",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "Using the fastai Images with Smaller Labels",
    "text": "Using the fastai Images with Smaller Labels\nfastai uses the high-quality 720x960 images and labels, so it would make logical sense to train on them and use these smaller masks as the labels, which is being done on all the upper benchmarks.\n\nThe Issue\nThere is a very big issue with this though, which Jeremy pointed out to us while we were discussing these new benchmark approaches. Simply upscaling the labels, without any adjustments to the fastai images, on its own sounds ‚Äúweird.‚Äù Instead, what we do is resize the images back down to the 360x480 size before then upsampling them. This winds up increasing the final accuracy"
  },
  {
    "objectID": "blog/CAMVID.html#downloading-the-dataset",
    "href": "blog/CAMVID.html#downloading-the-dataset",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "Downloading the Dataset",
    "text": "Downloading the Dataset\nThe dataset currently lives in a the repository, so we will go ahead and clone it and make it our working directory:\n!git clone https://github.com/alexgkendall/SegNet-Tutorial.git\n%cd SegNet-Tutorial/\nNow we still want to use fastai‚Äôs input images, so we‚Äôll go ahead and pull their CAMVID dataset. First let‚Äôs import fastai‚Äôs vision module:\nfrom fastai.vision.all import *\nThen grab the data:\npath_i = untar_data(URLs.CAMVID)\nLet‚Äôs see how both datasets are formatted:\npath_l = Path('')\npath_i.ls()\npath_l.ls()\nSo we can see that fastai has the usual images and labels folder, while we can‚Äôt quite tell where the annotations are in our second one. Let‚Äôs narrow down to the CamVid folder:\npath_l = path_l/'CamVid'\npath_l.ls()\nAnd we can see a better looking dataset! The three folders we will be caring about are trainannot, valannot and testannot, as these are where the labels live."
  },
  {
    "objectID": "blog/CAMVID.html#datablock",
    "href": "blog/CAMVID.html#datablock",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "DataBlock",
    "text": "DataBlock\nAs we saw how the data was split up, fastai currently doesn‚Äôt have something to work along those lines, the closest is GrandparentSplitter. We‚Äôll write something similar called FolderSplitter, which can accept names for the train and validation folders:\ndef _folder_idxs(items, name):\n    def _inner(items, name): return mask2idxs(Path(o).parents[0].name == name for o in items)\n    return [i for n in L(name) for i in _inner(items, n)]\n\ndef FolderSplitter(train_name='train', valid_name='valid'):\n    \"Split `items` from parent folder names `parent_idx` levels above the item\"\n    def _inner(o):\n        return _folder_idxs(o, train_name),_folder_idxs(o, valid_name)\n    return _inner\nNext we will need a way to get our x images, since they live differently than our labels. We can use a custom function to do so:\ndef get_x(o): return path_i/'images'/o.name\nFinally we need a get_y that will use that same filename to go grab our working masks:\ndef get_mask(o): return o.parent.parent/(o.parent.name + 'annot')/o.name\nWe have almost all the pieces to making our dataset now. We‚Äôll use fastai‚Äôs progressive resizing when training, and pass in a set of codes for our dataset:\ncodes = ['Sky', 'Building', 'Pole', 'Road', 'Pavement', 'Tree', 'SignSymbol', 'Fence', 'Car', 'Pedestrian', 'Bicyclist', 'Unlabelled']\nhalf, full = (360, 480), (720, 960)\nNow for those transforms. I mentioned earlier we will be downscaling and then upscaling the images, this way the same upscaling is applied to our labels and our images, though the images start from a higher quality. Since we want to train small, we‚Äôll resize it back down in the batch transforms as well as normalize our inputs:\nitem_tfms = [Resize(half), Resize(full)]\nbatch_tfms = [*aug_transforms(size=half), Normalize.from_stats(*imagenet_stats)]\nAnd with this we can now build the DataBlock and DataLoaders:\ncamvid = DataBlock(blocks=(ImageBlock, MaskBlock(codes=codes)),\n                   get_items=get_image_files,\n                   splitter=FolderSplitter(valid_name='val'),\n                   get_x=get_x,\n                   get_y=get_mask,\n                   item_tfms=item_tfms,\n                   batch_tfms=batch_tfms)\nWe‚Äôll call the .summary() to make sure our images and masks do crop to the half size:\ncamvid.summary(path_s)\nWe can see the final input and mask size is (360,480), which is what we want! Let‚Äôs go ahead and make them DataLoaders:\ndls = camvid.dataloaders(path_l, bs=4)\nSince we have a void column, our c attribute in the DataLoaders needs to be one less:\ndls.c = len(codes) - 1"
  },
  {
    "objectID": "blog/CAMVID.html#metrics",
    "href": "blog/CAMVID.html#metrics",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "Metrics",
    "text": "Metrics\nFor the next part Juvian was the one to bring this to life! We want class-wise IOU as well as mIOU, which are defined below:\nclass IOU(AvgMetric):\n    \"Intersection over Union Metric\"\n    def __init__(self, class_index, class_label, axis, ignore_index=-1): store_attr('axis,class_index,class_label,ignore_index')\n    def accumulate(self, learn):\n        pred, targ = learn.pred.argmax(dim=self.axis), learn.y\n        intersec = ((pred == targ) & (targ == self.class_index)).sum().item()\n        union = (((pred == self.class_index) | (targ == self.class_index)) & (targ != self.ignore_index)).sum().item()\n        if union: self.total += intersec\n        self.count += union\n  \n    @property\n    def name(self): return self.class_label\nfrom sklearn.metrics import confusion_matrix\n\nclass MIOU(AvgMetric):\n    \"Mean Intersection over Union Metric\"\n    def __init__(self, classes, axis): store_attr()\n\n    def accumulate(self, learn):\n        pred, targ = learn.pred.argmax(dim=self.axis).cpu(), learn.y.cpu()\n        pred, targ = pred.flatten().numpy(), targ.flatten().numpy()\n        self.total += confusion_matrix(targ, pred, range(0, self.classes))\n\n    @property\n    def value(self): \n        conf_matrix = self.total\n        per_class_TP = np.diagonal(conf_matrix).astype(float)\n        per_class_FP = conf_matrix.sum(axis=0) - per_class_TP\n        per_class_FN = conf_matrix.sum(axis=1) - per_class_TP\n        iou_index = per_class_TP / (per_class_TP + per_class_FP + per_class_FN)\n        iou_index = np.nan_to_num(iou_index)\n        mean_iou_index = (np.mean(iou_index))    \n\n        return mean_iou_index\n\n    @property\n    def name(self): return 'miou'\nWith our metric functions defined, let‚Äôs combine them all. We‚Äôll want a mIOU, as well as 11 IOU for each class:\nmetrics = [MIOU(11, axis=1)]\n\nNote: we do not need to pass in an ignore_index here, as any values larger than 10 get ignored\n\nAnd now let‚Äôs declare our IOU‚Äôs. Since there‚Äôs so many we‚Äôll just make a function instead that relies on our codes:\nfor x in range(11): metrics.append(IOU(x, codes[x], axis=1, ignore_index=11))\nWith this we can finally move over to our model and training:"
  },
  {
    "objectID": "blog/CAMVID.html#the-model-and-training",
    "href": "blog/CAMVID.html#the-model-and-training",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "The Model and Training",
    "text": "The Model and Training\nFor the model we will use a pretrained ResNet34 backbone architecture that has Mish on the head of the Dynamic Unet:\nconfig = unet_config(self_attention=False, act_cls=Mish)\nOur optimizer will be ranger:\nopt_func = ranger\nAnd finally, since we have an ignore_index we need to pass this into our loss function as well, otherwise we will trigger a CUDA error: device-side assert triggered\nloss_func = CrossEntropyLossFlat(ignore_index=11, axis=1)\nNow let‚Äôs pass this all into unet_learner:\nlearn = unet_learner(dls, resnet34, metrics=metrics, opt_func=opt_func, \n                     loss_func=loss_func, config=config)\n\nPhase 1\nWe‚Äôll find a good learning rate, fit for ten epochs frozen with GradientAccumulation to help with stability before unfreezing and training for a few more:\nlearn.lr_find()\nA good learning rate is around 2e-3, so we‚Äôll train with that using fit_flat_cos as the ranger optimizer should be paired with it:\nlr = 2e-3\nlearn.fit_flat_cos(10, slice(lr), cbs=[GradientAccumulation(n_acc=16)])\nNext we‚Äôll unfreeze and train for 12 more epochs. When training we will adjust the learning rate and apply the EarlyStoppingCallback to help prevent overfitting:\nlrs = slice(lr/400, lr/4)\nlearn.unfreeze()\nlearn.fit_flat_cos(12, lrs, cbs=[GradientAccumulation(n_acc=16)])\nWe‚Äôll save away this model and quickly check how it‚Äôs doing on our test set:\nlearn.save(\"360\")\nfnames = get_image_files(path_l/'test')\ntest_dl = learn.dls.test_dl(fnames, with_labels=True)\nmetrics = learn.validate(dl=test_dl)[1:]\nnames = list(map(lambda x: x.name, learn.metrics))\nfor value, metric in zip(metrics, names):\n  print(metric, value)\nWe can see a starting mIOU of 65% almost matching the mid-tier performer, let‚Äôs see if we can take it further by using the full sized images\n\n\nPhase 2:\nFirst let‚Äôs free up our memory:\ndel learn\ntorch.cuda.empty_cache()\nimport gc\ngc.collect()\nWe‚Äôll adjust our transforms to instead keep our full sized images:\nitem_tfms = [Resize(half), Resize(full)]\nbatch_tfms = [*aug_transforms(size=full), Normalize.from_stats(*imagenet_stats)]\nAnd simply train again from there:\ncamvid = DataBlock(blocks=(ImageBlock, MaskBlock(codes=codes)),\n                   get_items=get_image_files,\n                   splitter=FolderSplitter(valid_name='val'),\n                   get_x=get_x,\n                   get_y=get_mask,\n                   item_tfms=item_tfms,\n                   batch_tfms=batch_tfms)\n\ndls = camvid.dataloaders(path_l, bs=2)\ndls.c = len(codes) - 1\nWe‚Äôll need to re-declare our metrics as the current ones have memory of our last training session:\nmetrics = [MIOU(11, axis=1)]\nfor x in range(11): metrics.append(IOU(x, codes[x], axis=1, ignore_index=11))\nAnd now let‚Äôs train:\nlearn = unet_learner(dls, resnet34, metrics=metrics, opt_func=opt_func,\n                     config=config, loss_func=loss_func)\nlearn.load('360');\nlearn.freeze()\n\nlr = 1e-3\nlearn.fine_tune(12, lr, cbs=[GradientAccumulation(n_acc=16), EarlyStoppingCallback()])\nLet‚Äôs check it‚Äôs final IOU:\nfnames = get_image_files(path_l/'test')\ntest_dl = learn.dls.test_dl(fnames, with_labels=True)\nmetrics = learn.validate(dl=test_dl)[1:]\nnames = list(map(lambda x: x.name, learn.metrics))\nfor value, metric in zip(metrics, names):\n  print(metric, value)"
  },
  {
    "objectID": "blog/CAMVID.html#results-and-discussion",
    "href": "blog/CAMVID.html#results-and-discussion",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "Results and Discussion",
    "text": "Results and Discussion\nAt first we tried a standard Unet without any special tricks, and we got a test mIOU of around 59%. From this baseline we tried applying Self-Attention, Label Smoothing, and the Mish activation function (as the default is ReLU).\nWhat we found is that by simply applying Mish we could boost that 59% to around 64%, and do note that Mish was only applied to the head of the model, not in the ResNet backbone. (with the highest we got around 67% mIOU)\nSelf Attention did not seem to help as much, bringing down the mIOU to 62% when training even with the Mish activation function.\nApplying Label Smoothing led to a very different result baked inside of each individual IOU. While the mIOU was not as high as a flat Mish model, the distributions of the IOU‚Äôs changed.\nWhen applying the proper presizing techniques demonstrated here, we saw a boost of 10% mIOU, confirming an idea that simply blowing up your masks to match the original image resolution can diminish the value inside of them."
  },
  {
    "objectID": "blog/CAMVID.html#conclusions",
    "href": "blog/CAMVID.html#conclusions",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "Conclusions",
    "text": "Conclusions\nWhat conclusions can we actually make from this study? Not as much as you would think, and the reason lies within current issues in Academia. Right now there are three different datasets being used:\n\nfastai images with SegNet masks\nSegNet images and masks\nfastai images and labels while ignoring all the other classes\n\nWell‚Ä¶ who is right then? Technically 2 and 3 are right, but the three cannot be compared equally. Remember that benchmark table I showed earlier? If you go and read the papers each use one of the three techniques done here.\nSo‚Ä¶ what can we make of this?\nThere is one direct conclusion we can make: using Mish in the head of our Dynamic Unet boosts the mIOU by 5%. So it is absolutely worth trying and using with your projects."
  },
  {
    "objectID": "blog/CAMVID.html#where-do-we-go-from-here",
    "href": "blog/CAMVID.html#where-do-we-go-from-here",
    "title": "Generating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet",
    "section": "Where do we go from here?",
    "text": "Where do we go from here?\nA better dataset which is much more consistant is the CityScapes dataset. It‚Äôs for research only and you must upload your predictions on the test set to the website, it‚Äôs essentially a Kaggle competition for researchers, a format I believe works much better. Researchers compare both how they perform on the validation set and the test set. This is certainly an easier benchmark for folks to tackle with the fastai UNet, so hopefully one day someone will try a benchmark and see how it does!"
  },
  {
    "objectID": "blog/FeatureImportance-TestSet.html",
    "href": "blog/FeatureImportance-TestSet.html",
    "title": "Feature Importance and Gradable Test Sets in Fast.AI",
    "section": "",
    "text": "For all of my university research thus far, I have been utilizing the tabular library within Fast.AI to apply deep neural networks on tabular data (something not regularly done) with good results. As such, I have done a large amount of outside work and research into common practices with Tabular data. In this article I will be discussing Feature Importance as well as how to grade a test set using the Fast.AI library."
  },
  {
    "objectID": "blog/FeatureImportance-TestSet.html#what-is-it-and-why-should-it-matter",
    "href": "blog/FeatureImportance-TestSet.html#what-is-it-and-why-should-it-matter",
    "title": "Feature Importance and Gradable Test Sets in Fast.AI",
    "section": "What is it and why should it matter?",
    "text": "What is it and why should it matter?\nFeature importance in a very wide-grasp is figuring out what the most important factors in your data are in which the model can then perform the best with. In images, these can be visualized using ‚Äòheat-maps‚Äô, where we have a thermal image of an input and we can see where a particular model wanted to focus on. The Fast.AI library utilizes Grad-CAM to do this. Here is an example from the Lesson 6 notebook in the Practical Deep Learning for Coders course:\n\nIn the context of a tabular problem, where we deal with input variables, we can grade the relative importance of each variable we use and can figure out the best choice for our set. The technique this article will be discussing is called permutation importance."
  },
  {
    "objectID": "blog/FeatureImportance-TestSet.html#permutation-importance",
    "href": "blog/FeatureImportance-TestSet.html#permutation-importance",
    "title": "Feature Importance and Gradable Test Sets in Fast.AI",
    "section": "Permutation Importance:",
    "text": "Permutation Importance:\nPermutation importance, or Mean Decrease in Accuracy (MDA) is a method of measuring a variables importance in a model by first fully training a model with everything available, then taking each variable away one by one and seeing the relative change in accuracy or any metric. This should be done on a separate test set, as it has the least bias for our models.\nWhen we train a neural network in the library, one thing we can not do is simply wipe a column out from our inputs and see that change, as our models will always expect all variables to be present. As such there are two options available to us: fully train a new model for every single combination needed (which is extremely costly and time-intensive), or we can permutate a column and shuffle all of its values, with the hopes of breaking any link a particular column had to our data."
  },
  {
    "objectID": "blog/FeatureImportance-TestSet.html#doing-this-in-fast.ai",
    "href": "blog/FeatureImportance-TestSet.html#doing-this-in-fast.ai",
    "title": "Feature Importance and Gradable Test Sets in Fast.AI",
    "section": "Doing this in Fast.AI:",
    "text": "Doing this in Fast.AI:\nAs I mentioned above, we can permutate a column in Pandas very easily with the .sample function. This will randomly sample ‚Äòx‚Äô number of items from our dataframe. For us, since we want to shuffle the entire thing, it would look like this:\ndf[column] = df[column].sample(n=len(df), replace=True).reset_index(drop=True)\nHere df is our dataframe and column is what we want to shuffle. Now that we have this new shuffled column, we can make a new tabular databunch to pass into our model and grab some predictions.\nI‚Äôll make a small note of this here, the terminology here will be slightly confusing, this is due to I am grading a ‚Äòtest‚Äô set here. I‚Äôll explain why it‚Äôs this way later.\nThe start of our feature_importance function now looks something like this:\ndef permutation_imp(learn, cat_vars, cont_vars, dep_var, test):\n  dt = (TabularList.from_df(test, cat_names=cat_vars, cont_names=cont_vars,\n        procs=procs)\n       .split_none()\n       .label_from_df(cols=dep_var))\n  dt.valid = dt.train\n  dt = dt.databunch()\n\n  learn.data.valid_dl = dt.valid_dl\n  loss0 = float(learn.validate()[1])\n\n  fi=dict()\n\n  types = [cat_vars, cont_vars]\n  for j, t in enumerat(types):\n    for i, c in enumerate(t):\n      base = test.copy()\n      base[c] = base[c].sample(n=len(base), replace=True).reset_index(drop=True)\n      dt = (TabularList.from_df(test, cat_names=cat_vars, cont_names=cont_vars,\n        procs=procs)\n       .split_none()\n       .label_from_df(cols=dep_var))\n      dt.valid = dt.train\n      dt = dt.databunch()\n\n      learn.data.valid_dl = dt.valid.dl\n      fi[c] = float(learn.validate()[1])\n\nA few more notes on the above code, we are saving all of our new accuracies into a dictionary for us go through later, and the enumerate() loops allow us to go through and use every value in our types array of arrays.\n\n\nGreat, so now we can get our relative accuracies for shuffling a column right? We‚Äôre almost done right? Wrong. The above code actually will not quite work. The reason why is when we generate our databunch, our original cat_vars and cont_vars arrays will be overridden if there are any missing values in our dataset. So now we will have the possibility of _na variables, which we don‚Äôt want to shuffle as those are binary data representing if a value is missing.\nHow do we fix this? We can utilize Python‚Äôs copy library and the deepcopy function to make a new copy of our list that we can modify safely. On top of this, we need access to our data‚Äôs procs, so lets make a line to grab that from the training dataset before we make every TabularList:\ndef permutation_imp(learn, cat_vars, cont_vars, dep_var, test):\n  data = learn.data.train_ds.x\n  procs = data.procs\n  cat, cont = copy.deepcopy(cats), copy.deepcopy(conts)\n  dt = (TabularList.from_df(test, path='', cat_names=cat, cont_names=cont, procs=procs)\n  ...\n  ...\n  ...\n  fi = dict()\n  cat, cont = copy.deepcopy(cats, conts)\n  ...\n  ...\n  ...\n  for j, t in enumerate(types):\n    for i, c in enumerate(t):\n      base = test.copy()\n      base[c] = base[c].sample(n=len(base), replace=True).reset_index(drop=True)\n      cat, cont = copy.deepcopy(cats), copy.deepcopy(conts)\n      dt = (TabularList.from_df(test, path='', cat_names=cat, cont_names=cont, procs=procs)\n      ...\n      ...\n\nGreat! We‚Äôre almost there. All that‚Äôs left is giving us a pretty table that shows our changes in accuracy along with the variables name! We‚Äôll use a Pandas dataframe to show it, and we can look at the dataframe to see what variable‚Äôs results we are using:\nd = sorted(fi.items(), key = lambda kv: kv[1], reverse=False)\ndf = pd.DataFrame({'Variable': [l for l, v in d], 'Accuracy': [v for l, v in d]})\ndf['Type'] = ''\nfor x in range(len(df)):\n  if df['Variable'].iloc[x] in cats:\n    df['Type'].iloc[x] = 'categorical'\n  else:\n    df['Type'].iloc[x] = 'continuous'\n\nreturn df\nNow this will return a dataframe with our variable‚Äôs name, how much the accuracy was either lost or gained by shuffling it, and the type of variable it was. Here‚Äôs our new ‚Äòpermutation_importance‚Äô function in full:\nimport copy\n\ndef feature_importance(learn:Learner, cats:list, conts:list, dep_var:str, test:DataFrame):\n  data = learn.data.train_ds.x\n  procs = data.procs\n  cat, cont = copy.deepcopy(cats), copy.deepcopy(conts)\n  dt = (TabularList.from_df(test, path='', cat_names=cat, cont_names=cont, \n                            procs=procs)\n                           .split_none()\n                           .label_from_df(cols=dep_var))\n  dt.valid = dt.train\n  dt = dt.databunch()\n    \n  learn.data.valid_dl = dt.valid_dl\n  loss0 = float(learn.validate()[1])\n  \n  fi=dict()\n  cat, cont = copy.deepcopy(cats), copy.deepcopy(conts)\n  types = [cat, cont]\n  for j, t in enumerate(types):\n    for i, c in enumerate(t):\n      print(c)\n      base = test.copy()\n      base[c] = base[c].sample(n=len(base), replace=True).reset_index(drop=True)\n      cat, cont = copy.deepcopy(cats), copy.deepcopy(conts)\n      dt = (TabularList.from_df(base, path='', cat_names=cat, cont_names=cont, \n                            procs=procs)\n                           .split_none()\n                           .label_from_df(cols=dep_var))\n      dt.valid = dt.train\n      dt = dt.databunch()\n      \n      learn.data.valid_dl = dt.valid_dl\n      fi[c] = float(learn.validate()[1]) - loss0\n      \n  d = sorted(fi.items(), key =lambda kv: kv[1], reverse=True)\n  df = pd.DataFrame({'Variable': [l for l, v in d], 'Accuracy': [v for l, v in d]})\n  df['Type'] = ''\n  for x in range(len(df)):\n    if df['Variable'].iloc[x] in cats:\n      df['Type'].iloc[x] = 'categorical'\n    if df['Variable'].iloc[x] in conts:\n      df['Type'].iloc[x] = 'continuous'\n  return df \n\nWhy is this important? * First, we can understand how to get the best results from our models by dropping any values that were greater than zero, as we saw a positive impact by shuffling. * Second, now we can easily explain what our model is doing, and why we chose the features we did!"
  },
  {
    "objectID": "blog/intermediate.html",
    "href": "blog/intermediate.html",
    "title": "Zero to Hero with fastai - Intermediate",
    "section": "",
    "text": "A general overview of the major differences between the old fastai and the new"
  },
  {
    "objectID": "blog/intermediate.html#zero-to-hero",
    "href": "blog/intermediate.html#zero-to-hero",
    "title": "Zero to Hero with fastai - Intermediate",
    "section": "Zero to Hero",
    "text": "Zero to Hero\nThe ‚ÄúZero to Hero‚Äù series is a series of three articles geared towards getting anyone familair with the fastai library based upon their skill sets. The previous article is aimed towards those who have barely heard of ‚ÄúDeep Learning‚Äù and have zero experience with frameworks. This article article comes from a perspective of those who utilized the original fastai library in the past and understand the broad strokes of the library. Finally, the last article will briefly explain the advanced artifacts inside of fastai and how they all function. &gt; Note: These articles also presume you have read the previous to avoid redundancy, please read it before continuing so some context can be gathered here"
  },
  {
    "objectID": "blog/intermediate.html#who-am-i",
    "href": "blog/intermediate.html#who-am-i",
    "title": "Zero to Hero with fastai - Intermediate",
    "section": "Who am I",
    "text": "Who am I\nMy name is Zach Mueller, I‚Äôve extensively been involved and using fastai (and the newest version) for the better part of two years now. I‚Äôve designed my own course geared around the library from an implementation standpoint without getting too complex. At the time of writing this I‚Äôm still an Undergraduate at the University of West Florida majoring in Computer Science. I‚Äôm also heavily involved inside the fastai community, of which I would emplore you to join! My goal is to make fastai more approachable at all levels through examples and help further Jeremy‚Äôs dream in the process. My specific interests involve speeding up the framework for deployment, tabular neural networks, and providing overall usage guides to help further the community"
  },
  {
    "objectID": "blog/intermediate.html#what-will-we-cover-in-this-article",
    "href": "blog/intermediate.html#what-will-we-cover-in-this-article",
    "title": "Zero to Hero with fastai - Intermediate",
    "section": "What will we cover in this article?",
    "text": "What will we cover in this article?\nIn the second iteration of ‚ÄúZero to Hero‚Äù we will be going through the major differences between the two API‚Äôs. We will look more in detail at the high-level DataBlock API with a 1:1 code example to learn how to adjust your code from the old fastai. Afterwards we will look into the Mid-level API and transforms briefly to see how simple it can be to customize and adapt what you want into the framework through two seperate examples. Finally will then go into customizing test sets to include labelled and non-labelled data."
  },
  {
    "objectID": "blog/intermediate.html#installing-the-library",
    "href": "blog/intermediate.html#installing-the-library",
    "title": "Zero to Hero with fastai - Intermediate",
    "section": "Installing the library",
    "text": "Installing the library\nFirst let‚Äôs install fastai:\n!pip install fastai -qqq"
  },
  {
    "objectID": "blog/intermediate.html#whats-new",
    "href": "blog/intermediate.html#whats-new",
    "title": "Zero to Hero with fastai - Intermediate",
    "section": "What‚Äôs new?",
    "text": "What‚Äôs new?\nLet‚Äôs first look at two sets of code for a Tabular task, specifically Adult Sample &gt; Note: Some code cells may have # DO NOT RUN. Do not run these if you choose to open this notebook in Colaboratory as they reference the old codebase and will not work anymore\nAs per usual, we‚Äôll import the tabular library and use untar_data to grab the dataset:\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ADULT_SAMPLE)\nThen we will open the DataFrame in pandas:\ndf = pd.read_csv(path/'adult.csv')\ndf.head()\nIn both versions we still need to define our variables and procs, and the naming for each has not changed:\ndep_var = 'salary'\ncat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\ncont_names = ['age', 'fnlwgt', 'education-num']\nprocs = [FillMissing, Categorify, Normalize]\nHowever what did change was the API. Before our code would have looked like so:\n# DO NOT RUN\ndata = (TabularList.from_df(df, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n                           .split_by_idx(list(range(800,1000)))\n                           .label_from_df(cols=dep_var)\n                           .databunch())\nWhere we specify our API to have a TabularList, then split that list, label the list, and finally DataBunch it. This is gone, or at least simplified in the new version. Instead we have TabularPandas, a complete rework of the tabular API, which is different from the normal API. First we have special Splitter classes that we can call and use depending on our task. Since we are splitting by a list of indicies, it would make sense to utilize the IndexSplitter class. To utilize it we‚Äôll instantate the class with our list of indicies to split by, and then split our dataset via it‚Äôs indicies. To explore more of these splitters, see the documentation.\nsplits = IndexSplitter(list(range(800,1000)))(range_of(df)); splits\nThen we can pass everything to TabularPandas:\nto = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,\n                   y_names='salary', splits=splits)\n\nSomething very unique and nice about TabularPandas is we can actually use it in more than just fastai! To see how we can utilize it with Random Forests and XGBoost, see my course notebook where this is discussed\n\nAnd now we can build our DataLoaders:\ndls = to.dataloaders(bs=512)\nFrom here, the API remains the same. We have a tabular_learner and we can fit, fit_one_cycle, etc. One minor change is now to find the learning rate, it‚Äôs just lr_find:\nlearn = tabular_learner(dls, layers=[200,100], metrics=accuracy)\nlearn.lr_find()\nWhere it will also return the two suggested learning rates seen at the top of the graph."
  },
  {
    "objectID": "blog/intermediate.html#a-text-example",
    "href": "blog/intermediate.html#a-text-example",
    "title": "Zero to Hero with fastai - Intermediate",
    "section": "A Text Example",
    "text": "A Text Example\nThe next major upgrade is the Text API. This is a true example of the High-Level DataBlock API discussed in the previous article. It follows the same overall pattern we saw in TabularPandas, just a bit more split-up. Let‚Äôs take this example from my slides:\n\n\n\nimage.png\n\n\nWe‚Äôll run through this example step by step with a text example with IMDB_SAMPLE. Let‚Äôs load the library and grab our data:\nfrom fastai.text.all import *\npath_imdb = untar_data(URLs.IMDB_SAMPLE)\ndf_imdb = pd.read_csv(path_imdb/'texts.csv')\nIn the first version of fastai, to build our language model DataLoader it would look something like below:\n# DO NOT RUN\ndata = (TextList.from_csv(path, 'texts.csv', cols='text')\n       .split_by_rand_pct(0.1)\n       .label_for_lm()\n       .databunch(bs=8))\nNow let‚Äôs convert this to the new API following our pipeline description above\n\nDefine your input and output blocks:\n\nHere we have text as an input, so we simply say we have a TextBlock.from_df as have a DataFrame &gt; Note: text is a bit different in this regard as things get tokenized when generating the Dataset. As a result, we have .from_df and .from_folder, specifying where the data comes from:\nblocks = TextBlock.from_df(text_cols='text', res_col_name='text', is_lm=True)\nNow an important distinction here is when tokenized, we will get rid of text_cols in our DataFrame and it will be replaced with res_col_name. This is text by default. Finally we want to specify that it is a language model by passing is_lm=True.\n\nDefine our getters\n\nNow we need to tell fastai how to grab the data. Our text will be stored in a column named text, so we will use a ColReader to grab it:\nget_x = ColReader('text')\n\nSplit the Data\n\nWe‚Äôll use another splitter like we did for tabular, this time using RandomSplitter. When calling our DataBlock we won‚Äôt need to pass in the direct indicies, fastai will do this for us, so we can define it as below:\nsplitter = RandomSplitter(valid_pct=0.2, seed=42)\n\nLabel the Data\n\nWe have already done this by specifying is_lm to True back when we defined our blocks. When we examine a non-language model classification example next you will be able to understand the difference\n\nBuild the DataLoaders\n\nNow let‚Äôs build our DataBlock by passing in what we have:\ndblock = DataBlock(blocks=blocks,\n                   get_x=get_x,\n                   splitter=splitter)\nAnd we can build our DataLoaders:\ndls = dblock.dataloaders(df_imdb, bs=8)\nLet‚Äôs look at an example batch:\ndls.show_batch(max_n=2)\nNow if we wanted to train, the API still looks the same, where we call language_model_learner and pass in our data. We won‚Äôt train in this example though as that can take a bit with language models:\nlm_learn = language_model_learner(dls, arch=AWD_LSTM, metrics=accuracy)\nNow let‚Äôs move onto a text classification example. This only requires two major changes to what we had before in our DataBlock: the addition of another block and a get_y to tell fastai where our label is:\nblocks = (TextBlock.from_df(text_cols='text', res_col_name='text', is_lm=False), CategoryBlock())\nWe set is_lm to False (the default) and added a CategoryBlock telling fastai we will be dealing with a classification problem. Next we need a get_y to say where the label is. It‚Äôs still in that same DataFrame, so we can use another ColReader:\nget_y = ColReader('label')\nFinally, we‚Äôll make a new splitter that splits from a column, as our DataFrame has a is_valid option:\nsplitter = ColSplitter(col='is_valid')\nNow let‚Äôs remake our DataBlock:\nclas_dblock = DataBlock(blocks=blocks,\n                        get_x=get_x,\n                        get_y=get_y,\n                        splitter=splitter)\nAnd make some new DataLoaders:\ndls = clas_dblock.dataloaders(df_imdb, bs=8)\nAnd that‚Äôs it for the text example! Now you‚Äôve seen the basic building blocks and how it all works. For the final example we‚Äôll walk through the PETS dataset as we did during the previous article, and recreate it with the API"
  },
  {
    "objectID": "blog/intermediate.html#pets",
    "href": "blog/intermediate.html#pets",
    "title": "Zero to Hero with fastai - Intermediate",
    "section": "Pets",
    "text": "Pets\nfrom fastai.vision.all import *\nLet‚Äôs first grab our data:\npath = untar_data(URLs.PETS)\nWe‚Äôll define our blocks again. This time, since we have an image problem we‚Äôll use an ImageBlock and re-use CategoryBlock:\nblocks = (ImageBlock(cls=PILImage), CategoryBlock())\n\nNote that we can define sub-classes for blocks to use. If we were doing a black and white image problem (such as MNIST), we could define our ImageBlock as ImageBlock(cls=PILImageBW)\n\nNext we want our getters. This is actually just as simple as get_image_files. Why? Let‚Äôs look:\nimgs = get_image_files(path/'images')\nimgs[0]\nHere we have a list of our images, so this is all we actually need since both our x and y are there.\nNext we want to split the data. We did a random 80/20 split in the first article, so we will repeat this here using RandomSplitter:\nsplitter = RandomSplitter(valid_pct=0.2, seed=42)\nFinally we need our labeller, which is a RegexLabeller:\nget_y=RegexLabeller(pat = r'/([^/]+)_\\d+.*')\nNow before we continue we need some item and batch transforms to augment our data:\nitem_tfms = RandomResizedCrop(460, min_scale=0.75, ratio=(1.,1.))\nbatch_tfms = [*aug_transforms(size=224, max_warp=0), Normalize.from_stats(*imagenet_stats)]\nAnd finally we can work this into our DataBlock:\ndblock = DataBlock(blocks=blocks,\n                   get_items=get_image_files,\n                   splitter=splitter,\n                   get_y=get_y,\n                   item_tfms=item_tfms,\n                   batch_tfms=batch_tfms)\nLet‚Äôs build our data:\ndls = dblock.dataloaders(path/'images', bs=64)\nAnd view some data just to be sure:\ndls.show_batch()\nWe have now seen three major examples of the API from a DataLoader perspective. Along with this article I invite you to read my other articles related to the DataBlock API:\n\nfastai and the New DataBlock API\nThe Idea of a Transform\nLooking at fastai‚Äôs test_dl\n\nAs they cover a few more specifics in regards to the API."
  },
  {
    "objectID": "blog/intermediate.html#test-sets",
    "href": "blog/intermediate.html#test-sets",
    "title": "Zero to Hero with fastai - Intermediate",
    "section": "Test Sets",
    "text": "Test Sets\nNow finally I mentioned the addition of labelled and non-labelled test sets. Originally back in the old fastai version when you did add_test for a test set and wanted it labelled you had to do a weird workaround. However this is no longer the case. With fastai‚Äôs test_dl method we can pass with_labels=True and it will attempt to label our data if it is labelled the same format as it were for training.\n\nNote: tabular problems will always assume with_labels to be True if the y is present in the DataFrame\n\nNow let‚Äôs first use the defaults for test_dl on some data:\ndl = dls.test_dl(imgs[:10], with_labels=False)\nWe can look at a batch:\ndl.show_batch()\nAnd we can just see blank images! Now if we change this:\ndl = dls.test_dl(imgs[:10], with_labels=True)\ndl.show_batch()\nWe have our labels again! This is fantastically nice as you can then just pass this DataLoader into learn.validate by doing learn.validate(dl=dl) and there will be no complaints!"
  },
  {
    "objectID": "blog/intermediate.html#minor-changes-and-closing-thoughts",
    "href": "blog/intermediate.html#minor-changes-and-closing-thoughts",
    "title": "Zero to Hero with fastai - Intermediate",
    "section": "Minor Changes and Closing Thoughts",
    "text": "Minor Changes and Closing Thoughts\nFinally, let‚Äôs cover some minor naming changes.\n\nPassing callbacks to our Learner and during any fit are now called cbs rather than callbacks\nCallbacks have more events in which you are able to adjust and their naming is slightly different\nMetrics should inherit AccuMetric, but loss functions do not need this\n\nEverything is is a major API change or difference altogether.\nThank you so much for reading, and I implore you to check out the new library! It‚Äôs been carefully crafted over the last year and a half (since the previous part 2) and has really turned into something special. These first two articles I wanted out the day of release, so part 3 will take me a few more days. Thanks again for reading and have fun exploring!"
  },
  {
    "objectID": "blog/gradient_accumulation.html",
    "href": "blog/gradient_accumulation.html",
    "title": "PyTorch, Gradient Accumulation, and the dreaded drop in speed",
    "section": "",
    "text": "Recently I was helping someone at work debug some distributed code as they were looking to find ways to speed it up. Immediately I noticed something odd, gradient accumulation.\nThat in of itself is not odd. But when it comes to distributed compute with Pytorch, if you are not careful you can see immense slowdowns in your code.\nWhat follows below is an exploratory analysis I performed using Hugging Face Accelerate, PyTorch Distributed, and three machines to test what and by how much is the optimal and correct setup for gradient accumulation on multiple GPUs.\n\n\nFirst let‚Äôs discuss setup.\nFor these experiments, I used the following:\n\nPython: 3.9.13\nPyTorch: v1.12.1+cu113\nAccelerate: v0.16.0\nTransformers: v4.26.1\nCompute:\n\nTwo single-GPU T4 nodes from GCP that can communicate to each other\nOne node with two T4 GPUs\n\nScript-specific parameters:\n\nBatch size per GPU: 16\nGradient accumulation steps: 4\nTotal observed batch size (1624): 128\nMixed precision: fp16\n\nScripts: available here\n\n\n\n\nLet‚Äôs talk about why gradient accumulation is different on multiple GPUs. On a single GPU, everything happens on that device, you can accumulate, compute, and update the gradients all exceedingly quickly. However when multiple GPUs get involved (both on a single network and on a single machine), each time the backward pass is performed all GPUs communicate with each other. The gradients are updated based on the average between each model on each GPU, and all the weights are synchronized to be this new result based on the average.\nAs you can imagine, for every instance you need to have all your GPUs communicate there will be a time loss. Even if they are in the same machine!\nThis time loss can be deadly to your programs as you run them because it can lead to even a 2x slowdown!\nSo, what‚Äôs the cure?\nIn PyTorch distributed training, the model is wrapped in a DistributedDataParallel class. This module is what stores the model and understands how to update and process these weight changes, and communicate between all the GPUs you are utilizing during training to do so. This update, as mentioned earlier, happens on backward(), but begins on the forward pass.\nAs a result, the DistributedDataParallel class has a function called no_sync. Essentially this tells PyTorch while this block of code is running, do not synchronize with the other GPUs.\nTo make this work, this wrapper needs to be around both the forward and backward pass, such that:\nnet = MyModel()\nnet = DistributedDataParallel(net,...)\nwith net.no_sync():\n    pred = net(input)\n    loss = loss_func(pred)\n    pred.backward()\nTo synchronize again, remove the no_sync wrapper for a batch and processes will synchronize again.\nTranslated, this is what gradient accumulation looks like properly in native PyTorch:\nfor step, (x,y) in enumerate(dataloader):\n    if step % gradient_accumulation_steps != 0:\n        with model.no_sync():\n            outputs = model(x)\n            loss = loss_func(outputs, y)\n            loss = loss / gradient_accumulation_steps\n            accelerator.backward(loss)\n    else:\n        outputs = model(x)\n        loss = loss_func(outputs, y)\n        loss = loss / gradient_accumulation_steps\n        accelerator.backward(loss)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\nBut just how important is this?\nCan I just wrap around .backward() with the no_sync?\nI ran a few experiments to figure exactly that out.\n\n\n\nEach experiment ran through 29 total batches, using bert-base-cased as the model and the mrpc dataset. Each attempt was then ran 5 times and the average was taken.\nI‚Äôll highlight each individual result below, as well as their code changes.\n\n\nThe baseline consists of nothing special. It calls .backward at every step, and if we are finished accumulating then the optimizer and scheduler are zero‚Äôd and stepped.\nfor step, (x,y) in enumerate(train_dataloader):\n    outputs = model(x)\n    loss = loss_func(outputs, y)\n    loss = loss / gradient_accumulation_steps\n    accelerator.backward(loss)\n    if step % gradient_accumulation_steps == 0:\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\nThe Accelerator here is simply used to handle the standard DDP processes, and nothing more.\nThis baseline finished at:\n\nNote: Times are in Seconds per Batch\n\n\n\n\n\nMulti Node\nSingle Node\n\n\n\n\nRun 1\n1.95\n0.52\n\n\nRun 2\n2.11\n0.5\n\n\nRun 3\n1.94\n0.5\n\n\nAverage\n2¬±0.01s\n0.50¬±0.01s\n\n\n\nOverall 2 seconds per batch on multi-node, and 0.5 seconds per batch on a single node. That‚Äôs a 4x slowdown when comparing single to multi-node. That is not efficient at all!\nSo, let‚Äôs try using this fancy no_sync thing\n\n\n\nFor no_sync to work correctly, it needs to be wrapped around both the backward pass and forward pass. Otherwise, processes will still be synchronized during .backward().\nHere is the bad example of what not to do, and its results:\n    for step, batch in enumerate(train_dataloader):\n        batch.to(accelerator.device)\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss = loss / gradient_accumulation_steps\n        if step % gradient_accumulation_steps != 0:\n            with model.no_sync():\n                accelerator.backward(loss)\n        else:\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\nNote: Times are in Seconds per Batch\n\n\n\n\n\nMulti Node\nSingle Node\n\n\n\n\nRun 1\n2.08\n0.52\n\n\nRun 2\n2.09\n0.5\n\n\nRun 3\n2.23\n0.5\n\n\nAverage\n2.13¬±0.08s\n0.50¬±0.01s\n\n\n\nAs you can see, negligible different because it‚Äôs not actually doing any non-synchronization! Everything is still being synced at the same time, and there‚Äôs potential some amount of extra communication is being added on top of this considering on average it was .13s slower.\n\n\n\nThe correct way to use no_sync, as mentioned earlier, is to wrap around both the forward and backward pass. This ensures that only when we break out of the no_sync will the gradients fully be synchronized properly.\nThe snippet and results are below:\nfor step, (x,y) in enumerate(train_dataloader):\n    if step % gradient_accumulation_steps != 0:\n        with model.no_sync():\n            outputs = model(x)\n            loss = loss_function(outputs, y)\n            loss = loss / gradient_accumulation_steps\n            accelerator.backward(loss)\n    else:\n        outputs = model(**batch)\n        loss = loss_function(outputs, y)\n        loss = loss / gradient_accumulation_steps\n        accelerator.backward(loss)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\nNote: Times are in Seconds per Batch\n\n\n\n\n\nMulti Node\nSingle Node\n\n\n\n\nRun 1\n0.84\n0.4\n\n\nRun 2\n1.04\n0.43\n\n\nRun 3\n0.86\n0.41\n\n\nAverage\n0.91¬±0.11s\n0.41¬±0.015s\n\n\n\nYou can see that not only did we get a 2x speedup on the multi-node setup, but there was also a 25% speedup on the single node!\nReducing the amount of communication between all of your GPUs when training in a distributed process is paramount to training fast and efficiently.\nThe last script I will show is how Hugging Face Accelerate can do this automatically for you, using the accumulate wrapper:\n\n\n\nSnippet:\nfor step, (x,y) in enumerate(train_dataloader):\n    with accelerator.accumulate(model):\n        outputs = model(x)\n        loss = loss_function(outputs, y)\n        accelerator.backward(loss)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\nTimings:\n\nNote: Times are in Seconds per Batch\n\n\n\n\n\nMulti Node\nSingle Node\n\n\n\n\nRun 1\n0.84\n0.4\n\n\nRun 2\n1.04\n0.43\n\n\nRun 3\n0.86\n0.41\n\n\nAverage\n0.91¬±0.11s\n0.41¬±0.015s\n\n\n\nYou can see that we get roughly the same times as the no_sync example showed earlier, however Accelerate let‚Äôs us remove all of the if/else logic that was required entirely!\nThis helpful piece of magic not only lets you reduce lines of code, but it also ensures that you can never see the slowdowns presented here.\n\n\n\n\nWhat I would like for you to take away from this brief discussion is:\n\nFirst, you should be very careful when writing distributed code, and try to minimize the number of times all your processes need to synchronize. This is one of the largest places a slowdown can occur, and it‚Äôs not even limited by network!\nUnderstand that even if something works the same on a single GPU, there may be behavioral changes and tweaks to have the same code working efficiently on other distributed systems. Accelerate helps with this by ensuring that the same code can be used across any distributed platform with minimal overhead on the user, however in general it is also a good idea to be familiar with just what needs to be changed and how\n\nIf you liked this article, please be sure to check out my Twitter and if you are interested be sure to check out Accelerate, a library I work on: Accelerate."
  },
  {
    "objectID": "blog/gradient_accumulation.html#setup",
    "href": "blog/gradient_accumulation.html#setup",
    "title": "PyTorch, Gradient Accumulation, and the dreaded drop in speed",
    "section": "",
    "text": "First let‚Äôs discuss setup.\nFor these experiments, I used the following:\n\nPython: 3.9.13\nPyTorch: v1.12.1+cu113\nAccelerate: v0.16.0\nTransformers: v4.26.1\nCompute:\n\nTwo single-GPU T4 nodes from GCP that can communicate to each other\nOne node with two T4 GPUs\n\nScript-specific parameters:\n\nBatch size per GPU: 16\nGradient accumulation steps: 4\nTotal observed batch size (1624): 128\nMixed precision: fp16\n\nScripts: available here"
  },
  {
    "objectID": "blog/gradient_accumulation.html#gradient-accumulation-is-special",
    "href": "blog/gradient_accumulation.html#gradient-accumulation-is-special",
    "title": "PyTorch, Gradient Accumulation, and the dreaded drop in speed",
    "section": "",
    "text": "Let‚Äôs talk about why gradient accumulation is different on multiple GPUs. On a single GPU, everything happens on that device, you can accumulate, compute, and update the gradients all exceedingly quickly. However when multiple GPUs get involved (both on a single network and on a single machine), each time the backward pass is performed all GPUs communicate with each other. The gradients are updated based on the average between each model on each GPU, and all the weights are synchronized to be this new result based on the average.\nAs you can imagine, for every instance you need to have all your GPUs communicate there will be a time loss. Even if they are in the same machine!\nThis time loss can be deadly to your programs as you run them because it can lead to even a 2x slowdown!\nSo, what‚Äôs the cure?\nIn PyTorch distributed training, the model is wrapped in a DistributedDataParallel class. This module is what stores the model and understands how to update and process these weight changes, and communicate between all the GPUs you are utilizing during training to do so. This update, as mentioned earlier, happens on backward(), but begins on the forward pass.\nAs a result, the DistributedDataParallel class has a function called no_sync. Essentially this tells PyTorch while this block of code is running, do not synchronize with the other GPUs.\nTo make this work, this wrapper needs to be around both the forward and backward pass, such that:\nnet = MyModel()\nnet = DistributedDataParallel(net,...)\nwith net.no_sync():\n    pred = net(input)\n    loss = loss_func(pred)\n    pred.backward()\nTo synchronize again, remove the no_sync wrapper for a batch and processes will synchronize again.\nTranslated, this is what gradient accumulation looks like properly in native PyTorch:\nfor step, (x,y) in enumerate(dataloader):\n    if step % gradient_accumulation_steps != 0:\n        with model.no_sync():\n            outputs = model(x)\n            loss = loss_func(outputs, y)\n            loss = loss / gradient_accumulation_steps\n            accelerator.backward(loss)\n    else:\n        outputs = model(x)\n        loss = loss_func(outputs, y)\n        loss = loss / gradient_accumulation_steps\n        accelerator.backward(loss)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\nBut just how important is this?\nCan I just wrap around .backward() with the no_sync?\nI ran a few experiments to figure exactly that out."
  },
  {
    "objectID": "blog/gradient_accumulation.html#the-experiments",
    "href": "blog/gradient_accumulation.html#the-experiments",
    "title": "PyTorch, Gradient Accumulation, and the dreaded drop in speed",
    "section": "",
    "text": "Each experiment ran through 29 total batches, using bert-base-cased as the model and the mrpc dataset. Each attempt was then ran 5 times and the average was taken.\nI‚Äôll highlight each individual result below, as well as their code changes.\n\n\nThe baseline consists of nothing special. It calls .backward at every step, and if we are finished accumulating then the optimizer and scheduler are zero‚Äôd and stepped.\nfor step, (x,y) in enumerate(train_dataloader):\n    outputs = model(x)\n    loss = loss_func(outputs, y)\n    loss = loss / gradient_accumulation_steps\n    accelerator.backward(loss)\n    if step % gradient_accumulation_steps == 0:\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\nThe Accelerator here is simply used to handle the standard DDP processes, and nothing more.\nThis baseline finished at:\n\nNote: Times are in Seconds per Batch\n\n\n\n\n\nMulti Node\nSingle Node\n\n\n\n\nRun 1\n1.95\n0.52\n\n\nRun 2\n2.11\n0.5\n\n\nRun 3\n1.94\n0.5\n\n\nAverage\n2¬±0.01s\n0.50¬±0.01s\n\n\n\nOverall 2 seconds per batch on multi-node, and 0.5 seconds per batch on a single node. That‚Äôs a 4x slowdown when comparing single to multi-node. That is not efficient at all!\nSo, let‚Äôs try using this fancy no_sync thing\n\n\n\nFor no_sync to work correctly, it needs to be wrapped around both the backward pass and forward pass. Otherwise, processes will still be synchronized during .backward().\nHere is the bad example of what not to do, and its results:\n    for step, batch in enumerate(train_dataloader):\n        batch.to(accelerator.device)\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss = loss / gradient_accumulation_steps\n        if step % gradient_accumulation_steps != 0:\n            with model.no_sync():\n                accelerator.backward(loss)\n        else:\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\nNote: Times are in Seconds per Batch\n\n\n\n\n\nMulti Node\nSingle Node\n\n\n\n\nRun 1\n2.08\n0.52\n\n\nRun 2\n2.09\n0.5\n\n\nRun 3\n2.23\n0.5\n\n\nAverage\n2.13¬±0.08s\n0.50¬±0.01s\n\n\n\nAs you can see, negligible different because it‚Äôs not actually doing any non-synchronization! Everything is still being synced at the same time, and there‚Äôs potential some amount of extra communication is being added on top of this considering on average it was .13s slower.\n\n\n\nThe correct way to use no_sync, as mentioned earlier, is to wrap around both the forward and backward pass. This ensures that only when we break out of the no_sync will the gradients fully be synchronized properly.\nThe snippet and results are below:\nfor step, (x,y) in enumerate(train_dataloader):\n    if step % gradient_accumulation_steps != 0:\n        with model.no_sync():\n            outputs = model(x)\n            loss = loss_function(outputs, y)\n            loss = loss / gradient_accumulation_steps\n            accelerator.backward(loss)\n    else:\n        outputs = model(**batch)\n        loss = loss_function(outputs, y)\n        loss = loss / gradient_accumulation_steps\n        accelerator.backward(loss)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n\nNote: Times are in Seconds per Batch\n\n\n\n\n\nMulti Node\nSingle Node\n\n\n\n\nRun 1\n0.84\n0.4\n\n\nRun 2\n1.04\n0.43\n\n\nRun 3\n0.86\n0.41\n\n\nAverage\n0.91¬±0.11s\n0.41¬±0.015s\n\n\n\nYou can see that not only did we get a 2x speedup on the multi-node setup, but there was also a 25% speedup on the single node!\nReducing the amount of communication between all of your GPUs when training in a distributed process is paramount to training fast and efficiently.\nThe last script I will show is how Hugging Face Accelerate can do this automatically for you, using the accumulate wrapper:\n\n\n\nSnippet:\nfor step, (x,y) in enumerate(train_dataloader):\n    with accelerator.accumulate(model):\n        outputs = model(x)\n        loss = loss_function(outputs, y)\n        accelerator.backward(loss)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\nTimings:\n\nNote: Times are in Seconds per Batch\n\n\n\n\n\nMulti Node\nSingle Node\n\n\n\n\nRun 1\n0.84\n0.4\n\n\nRun 2\n1.04\n0.43\n\n\nRun 3\n0.86\n0.41\n\n\nAverage\n0.91¬±0.11s\n0.41¬±0.015s\n\n\n\nYou can see that we get roughly the same times as the no_sync example showed earlier, however Accelerate let‚Äôs us remove all of the if/else logic that was required entirely!\nThis helpful piece of magic not only lets you reduce lines of code, but it also ensures that you can never see the slowdowns presented here."
  },
  {
    "objectID": "blog/gradient_accumulation.html#article-takeaways",
    "href": "blog/gradient_accumulation.html#article-takeaways",
    "title": "PyTorch, Gradient Accumulation, and the dreaded drop in speed",
    "section": "",
    "text": "What I would like for you to take away from this brief discussion is:\n\nFirst, you should be very careful when writing distributed code, and try to minimize the number of times all your processes need to synchronize. This is one of the largest places a slowdown can occur, and it‚Äôs not even limited by network!\nUnderstand that even if something works the same on a single GPU, there may be behavioral changes and tweaks to have the same code working efficiently on other distributed systems. Accelerate helps with this by ensuring that the same code can be used across any distributed platform with minimal overhead on the user, however in general it is also a good idea to be familiar with just what needs to be changed and how\n\nIf you liked this article, please be sure to check out my Twitter and if you are interested be sure to check out Accelerate, a library I work on: Accelerate."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "My Blogs",
    "section": "",
    "text": "Introducing nbquarto: A framework for Processing Jupyter Notebooks\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPyTorch, Gradient Accumulation, and the dreaded drop in speed\n\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHow to be happy, or ways to try your best\n\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nDear Me of 2022\n\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nAnnouncing Walk with fastai, the missing pieces for success\n\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2022\n\n\n\n\n\n\n  \n\n\n\n\nDecorators\n\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2022\n\n\n\n\n\n\n  \n\n\n\n\nInference in PyTorch, what do the wrappers mean? What‚Äôs best?\n\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2022\n\n\n\n\n\n\n  \n\n\n\n\nNeuro-Diversity in Tech, The Slippery Slope to Burnout\n\n\n\n\n\nMy lifetime journey with ADHD\n\n\n\n\n\n\nMay 6, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHacking the Enum\n\n\n\n\n\n\n\n\n\n\n\n\nMar 25, 2022\n\n\n\n\n\n\n  \n\n\n\n\nDocker for Data Science, Efficient Image Instancing without System Issues\n\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2021\n\n\n\n\n\n\n  \n\n\n\n\nHuggingFace Course Notes, Chapter 1 (And Zero), Part 1\n\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2021\n\n\n\n\n\n\n  \n\n\n\n\nPytorch to fastai, Bridging the Gap\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2021\n\n\n\n\n\n\n  \n\n\n\n\nGenerating Comparitive Baselines for CAMVID with fastai‚Äôs Dynamic Unet\n\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2020\n\n\n\n\n\n\n  \n\n\n\n\nZero to Hero with fastai - Intermediate\n\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\nZero to Hero with fastai - Beginner\n\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\nSpeeding up fastai Tabular with NumPy\n\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\nThe Idea of a Transform\n\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\nfastai and the New DataBlock API\n\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\nNew Year, New Adventures\n\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2020\n\n\n\n\n\n\n  \n\n\n\n\nCapstone Project - Revisiting IMDB\n\n\n\n\n\n\n\n\n\n\n\n\nAug 23, 2019\n\n\n\n\n\n\n  \n\n\n\n\nClass Confusion, Analyzing Fastai Model Behaviors\n\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2019\n\n\n\n\n\n\n  \n\n\n\n\nFeature Importance and Gradable Test Sets in Fast.AI\n\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2019\n\n\n\n\n\n\n  \n\n\n\n\nSuspecto - Analyzing News Articles with Natural Language Processing to Score Credibility\n\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2019\n\n\n\n\n\n\n  \n\n\n\n\nSummer Smackdown - Week 1\n\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2019\n\n\n\n\n\n\n  \n\n\n\n\nSummer Smackdown - An Introduction\n\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/beginner.html",
    "href": "blog/beginner.html",
    "title": "Zero to Hero with fastai - Beginner",
    "section": "",
    "text": "An introduction to the world of Deep Learning, what the fastai library is, and the problems it attempts to solve"
  },
  {
    "objectID": "blog/beginner.html#zero-to-hero",
    "href": "blog/beginner.html#zero-to-hero",
    "title": "Zero to Hero with fastai - Beginner",
    "section": "Zero to Hero",
    "text": "Zero to Hero\nThe ‚ÄúZero to Hero‚Äù series is a collection of three seperate articles geared towards getting anyone familair with the fastai library based upon their skill sets. This article is geared towards those who have barely heard of ‚ÄúDeep Learning‚Äù and have zero experience with frameworks. The intermediate article comes from a perspective of those who utilized the original fastai library in the past and understand the broad strokes of the library. Finally, the last article will briefly explain the advanced artifacts inside of fastai and how they all function."
  },
  {
    "objectID": "blog/beginner.html#who-am-i",
    "href": "blog/beginner.html#who-am-i",
    "title": "Zero to Hero with fastai - Beginner",
    "section": "Who am I",
    "text": "Who am I\nMy name is Zach Mueller, I‚Äôve extensively been involved and using fastai (and the newest version) for the better part of two years now. I‚Äôve designed my own course geared around the library from an implementation standpoint without getting too complex. At the time of writing this I‚Äôm still an Undergraduate at the University of West Florida majoring in Computer Science. I‚Äôm also heavily involved inside the fastai community, of which I would emplore you to join! My goal is to make fastai more approachable at all levels through examples and help further Jeremy‚Äôs dream in the process. My specific interests involve speeding up the framework for deployment, tabular neural networks, and providing overall usage guides to help further the community"
  },
  {
    "objectID": "blog/beginner.html#what-will-we-cover-in-this-article",
    "href": "blog/beginner.html#what-will-we-cover-in-this-article",
    "title": "Zero to Hero with fastai - Beginner",
    "section": "What will we cover in this article?",
    "text": "What will we cover in this article?\nIn broad strokes, this article will cover what exactly a ‚ÄúJupyter Notebook‚Äù is and why it is popular among the Machine Learning community. Next we will divulge into fastai, what the library aims to do, and why it can be considered a ‚Äúlibrary for all.‚Äù Afterwards we will close by showing examples of what a ‚Äúmodel‚Äù is and how to utilize the framework at a high level to complete your Deep Learning tasks."
  },
  {
    "objectID": "blog/beginner.html#what-is-a-jupyter-notebook",
    "href": "blog/beginner.html#what-is-a-jupyter-notebook",
    "title": "Zero to Hero with fastai - Beginner",
    "section": "What is a ‚ÄúJupyter Notebook‚Äù?",
    "text": "What is a ‚ÄúJupyter Notebook‚Äù?\nWhy, you‚Äôre reading one now! A Jupyter Notebook is a type of workflow that involves active coding. In a typical software development setting, normally everything would be put into various files and run from a console (such as .cpp, .py, etc). This does not provide much interactability to the user, nor anyone reading the code over.\nJupyter provides a middle-ground for this. Each ‚Äúcell‚Äù in a notebook can be run independantly of others, such as a quick example below where we do a basic addition:\n\nTo run a Jupyter cell, you can press ‚Äúshift + enter‚Äù\n\no = 1+2; print(o)\nJupyter uses pretty print to make outputs display readable, so they may differ from your regular console outputs. This in turn allows us to break up code when writing documentation directly inside the contained notebook and not rely on just code comments. As mentioned earlier, this blog was written from a Jupyter Notebook! The platform is evolving as the needs grow, and as such the library leans into this. If you are curious to how, read the documentation for nbdev and the fastai article as well.\nFor absolute beginners, Google Colaboratory is a great free platform to get familiar with Jupyter Notebooks and how the environment operates. For this series we will be running our code inside of Google Colab with the GPU enabled, which helps speed up training time. &gt; Note: Google Colaboratory‚Äôs environment is not standard Jupyter, so some things may differ but the overall appearance and operation remains the same."
  },
  {
    "objectID": "blog/beginner.html#what-is-this-fastai-library-and-deep-learning-what-is-the-goal",
    "href": "blog/beginner.html#what-is-this-fastai-library-and-deep-learning-what-is-the-goal",
    "title": "Zero to Hero with fastai - Beginner",
    "section": "What is this fastai library, and Deep Learning? What is the goal?",
    "text": "What is this fastai library, and Deep Learning? What is the goal?\nfastai is an open-source library designed to make State-of-the-Art Machine Learning and Deep Learning approachable for everyone. Spearheaded by Jeremy Howard, this library has seen many iterations, with each looking radically different than the last. The newest version seeks to solve many of the headaches most frameworks have, where there is too much code and not enough readability to those who may not have the coding background. This is the goal of fastai: make machine learning approachable by anyone regardless of their backgrounds to help further progress. Along with this, the layered API makes it suitable for researchers that want to customize absolutely everythign while ensuring strong-performing baselines.\nThese problems come in a variety of shapes and sizes, but can absolutely touch every aspect of the world. Something as simple as identifying cats versus dogs to helping self-driving vehicles operate safer and more effectively. All of which, while may not be able to solved by fastai directly without some work, Deep Learning can help provide the solution.\nFinally, in an attempt to mitigate this, fast.ai has provided numerous resources for you to peruse and learn from thanks to the hard efforts of Jeremy, Rachel Thomas, and Sylvain Gugger. Their new course, the fourth edition of Practical Deep Learning for Coders, is available here, and their newly released book is available off Amazon and available for free with fastbook. Finally, I have released my own sub-course based on fastai from an application-only perspective available here."
  },
  {
    "objectID": "blog/beginner.html#installing-the-library-and-a-general-overview",
    "href": "blog/beginner.html#installing-the-library-and-a-general-overview",
    "title": "Zero to Hero with fastai - Beginner",
    "section": "Installing the Library and a General Overview:",
    "text": "Installing the Library and a General Overview:\nIn Python (what these notebooks run on), we can install specific programming packages with pip. We‚Äôll do so with fastai below:\n!pip install fastai -qqq\nWe include the three q‚Äôs to silence the installation and keep our notebook pretty clean. If your code outputs ever get too cumbersome, you can always clear their outputs.\nNow let‚Äôs talk about an interesting coding practice. The fastai library revolves around importing sections of the library with from fastai.module.all import *. This can be extremely uncomfortable (or just wrong) to some, however this library is built upon utilizing every import and making it available. In general fastai has four submodules geared towards specific tasks: * vision - Image-related problems * tabular - Structured data-related problems * text - Text related problems * collab - Collaborative filtering-related tasks\nAs you can see, it‚Äôs a very readable library. We would call semantically based on what task is being performed. For our example, we will classify between species of dogs and cats based on their pictures. Given we are using images, let‚Äôs import the vision module:\nfrom fastai.vision.all import *\nThe first step is to gather our data. Since this is a pre-made dataset, fastai can download the .tar file using untar_data. The path itself is stored inside URLs.PETS, and calling this function will return where our data was stored:\npath = untar_data(URLs.PETS)"
  },
  {
    "objectID": "blog/beginner.html#pre-processing-our-data-and-the-high-level-api",
    "href": "blog/beginner.html#pre-processing-our-data-and-the-high-level-api",
    "title": "Zero to Hero with fastai - Beginner",
    "section": "Pre-Processing our Data and the High-Level API",
    "text": "Pre-Processing our Data and the High-Level API\nWhen training Machine Learning models, we need to gather our data into something that can be grouped into mini sets or ‚Äúbatches‚Äù, and apply some form of adjustments to our data, or augmentation. This in turn lets us feed our model data efficiently and can provide unique challenges in our data that may not have been present before. Such augmentations could be flipping the image, rotating it, adjusting the exposure, etc. fastai has available one-liners to allow our data to be processed. Let‚Äôs walk through our PETs example more.\nFirst, we want to check where and how the data is stored:\npath.ls()\nIn this particular instance, our images are stored in the images folder. Now let‚Äôs pull the filenames and take a look:\nimgs = get_image_files(path/'images'); imgs[0]\nWe can see that our label is inside of our filename. Using a technique called regex, we can extract it. fastai comes equipped with a RegexLabeller to do so. &gt; There are a suit of other labellers you can explore in the documentation as well as other examples including how to build your own.\nFirst we‚Äôll need a regex pattern capable of extracting our filename. This is provided below for our example:\npat = r'(.+)_\\d+.jpg$'\nNext let‚Äôs define a few of those ‚Äúaugmentations‚Äù for our data. We‚Äôll want to ensure that before stacking multiple images into batches (so they can be run efficently through our models) they are the same size. There are many resize transforms available and for our case we will use RandomResizedCrop which will randomly resize the image before cropping it to the determined size. Our example will crop our image to a 460 pixel by 460 pixel image:\nitem_tfms = [RandomResizedCrop(460)]\nYou will notice that I named this item_tfms. To seperate what is applied individually on an input-by-input bases versus into batches, they are seperated by item and batch tfms.\nNext our batch transforms will apply some basic random agumentation before finally normalizing all of our data. This example will utilize something called transfer learning. This involves taking another model which has been trained initially on some dataset and utilizing it for our own dataset. This allows for us to train models faster to our datasets. To do so we need to utilize the original model‚Äôs training statistics (the mean and standard deviation) and we then normalize our input data based on these values. Our pre-trained model used the ImageNet dataset, so this will be reflected here:\nbatch_tfms = [*aug_transforms(size=224), Normalize.from_stats(*imagenet_stats)]\nFinally we need to define how many images get fed into our model at one time. This is called our ‚Äúbatch size‚Äù. Our example will feed 64:\nbs = 64\nNow let‚Äôs bring it all together into DataLoaders. These contain our transform information and apply them to our data as we want to feed it to our Machine Learning models. We will feed it a relative path to our data, a list of images to use, the pattern to extract our labels, our transforms, and finally our batch size.\ndls = ImageDataLoaders.from_name_re(path, imgs, pat, item_tfms=item_tfms, batch_tfms=batch_tfms,\n                                    bs=bs)\nUnder the surface here our data was automatically split with 80% going to the training dataset and 20% going to the validation set. We can see how many classes there are in our problem as well as their names by looking inside dls.c and dls.vocab:\ndls.vocab\ndls.c\nWe can view a batch of our data with dls.show_batch\ndls.show_batch()\nAs well as specify which DataLoader (the training or validation) we want to see:\ndls.train.show_batch()"
  },
  {
    "objectID": "blog/beginner.html#training-your-model",
    "href": "blog/beginner.html#training-your-model",
    "title": "Zero to Hero with fastai - Beginner",
    "section": "Training Your Model",
    "text": "Training Your Model\nNext we will need to make a model to train on our data. This example will use the ResNet34 architecture. To do so, we will call cnn_learner (or convolutional-neural-network) to generate a Learner ready to train.\nWe will pass in our DataLoaders we made a moment ago, an architecture to use, as well as any metrics we would like to use. Metrics provide a human-readable understanding of the results in our model. We‚Äôll use the error rate in this example:\nlearn = cnn_learner(dls, resnet34, pretrained=True, metrics=error_rate)\nNow all that‚Äôs needed is to fit our model to our data. This can be done quickly and effectively with fastai‚Äôs fine_tune method for transfer learning. We‚Äôll fit for six iterations through our data (or epochs):\nlearn.fine_tune(5)\n\nNote: if each epoch takes more than a minute or two, you do not have the GPU enabled on your platform and are relying on the CPU\n\nGreat! We have only a 8% error rate at identifying 37 different types of cats and dogs, and we did so in less than 15 lines of code with the defaults! Not bad! If you want to take this further I invite you to learn about the DataBlock API in the next article or through any of the other fastai-related resources mentioned earlier.\nLastly we can take a look at the results of our model by calling, you guessed it, learn.show_results()! Let‚Äôs take a peek:\nlearn.show_results()"
  },
  {
    "objectID": "blog/beginner.html#now-what-can-i-do",
    "href": "blog/beginner.html#now-what-can-i-do",
    "title": "Zero to Hero with fastai - Beginner",
    "section": "Now what can I do?",
    "text": "Now what can I do?\nWe have a trained model, and we need to utilize it in production. The first initial step is to export our model. This will package everything important to us in a production-level environment. Specifically this includes how our data was made (but not the data itself!), the transforms applied to them, and the model itself.\nWe‚Äôll do so below:\nlearn.export(fname='export.pkl')\nTo load our models back in we call load_learner. We can then further map these to the CPU (by default) or GPU. We‚Äôll map to the GPU since we are still in our GPU instance:\nlearn = load_learner(fname=path/'export.pkl', cpu=False)"
  },
  {
    "objectID": "blog/beginner.html#gathering-predictions",
    "href": "blog/beginner.html#gathering-predictions",
    "title": "Zero to Hero with fastai - Beginner",
    "section": "Gathering Predictions",
    "text": "Gathering Predictions\nfastai has two methods to do this, predict, and get_preds. We‚Äôll focus on the prior but still mention the latter.\n\nPredict\nPredict is aimed at predicting one item. It will apply all the transforms done to our validation set only and then feed it to the model. Let‚Äôs grab one of our filenames we trained on:\ntest_im = imgs[0]; test_im\nWe simply pass this path into predict and it will tell us the relative class, the class index in the vocabulary, as well as the percentages:\nlearn.predict(test_im)\nIf we are dealing with a slew of data at once, we can convert them into more of those batches again by calling learn.dls.test_dl. This will make a new validation DataLoader we can pass to get_preds to get predictions off of. We‚Äôll make one based on the first five images:\nfnames_test = imgs[:5]\ndl = learn.dls.test_dl(fnames_test)\nWe can see it‚Äôs just like our DataLoaders from earlier by calling show_batch:\ndl.show_batch()\nAnd finally we can grab predictions by passing our dl to get_preds:\npreds = learn.get_preds(dl=dl, with_decoded=True)\npreds[2]\nYou‚Äôll notice it‚Äôs a little different from predict, instead we get the raw probabilities and the class indicies inside of vocab:\nlearn.dls.vocab[18]\nAnd that‚Äôs it! Again if you wish to learn more, there is a second part to this series, Jeremy and Sylvains book, their new course, as well as my own course. Best of luck and enjoy the library!"
  }
]